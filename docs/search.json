[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Environmental Metagenome Analysis in R Lab Fall 2025",
    "section": "",
    "text": "“Metagnomes! They’re always on hand to help with mining sequence from those complicated microbiomes” - redpenblackpen by Jason Mcdermott\n\n\nThis is a rendering of files from the course repository for Biology 478/678 Metagenome Analysis in R Lab taught at the University of Massachusetts Amherst by Prof. Jeffrey Blanchard."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule Fall 2025",
    "section": "",
    "text": "Joint Genome Institute Mural - artist Nigel Sussman - https://jgi.doe.gov/science-stories/jgi25\n\n\n\nSept 3 (Wed) - Course Introduction & Lab 1: Introduction to Unity, R and RStudio\nSept 8 (Mon) - Discussion of research project possibilities\nSept 10 (Wed) - Lab 2: Coding in R with generative AI\nSept 15 (Mon) - Project background research - Adding references to Quarto documents using Zotero\nSept 17 (Wed) - Lab 3: Data Transformation with dyplr\nSept 22 (Mon) - Discussion of research project possibilities\nSept 24 (Wed) - Lab 4: Coding Basics, Style and Data Read/Write\nSept 29 (Mon) - Discussion of the NEON data set and projects\nOct 1 (Wed) - Lab 5 : Working with NEON Metagenome Assembled Genomes (MAGs/bins)\nOct 6 (Mon) - Lab S2 : Adding References to Your Report with Zotero\nOct 8 (Wed) - Lab 6: ggplotting with NEON MAG data\nOct 13 (Mon) Holiday – Indigenous People’s Day\nOct 15 (Wed) - Lab 7 : Data Tidying, Transformation and Visualization with COVID-19 reporting data\nOct 20 (Mon) - Dr. Jennifer DeBruyn - All Ooze Considered\nOct 22 (Wed) - Lab 8: Metagenome brainstorming with AI\nOct 27 (Mon) - Lab S3 : Connecting a Github repo site with a new RStudio project\nOct 29 (Wed) - Lab 9 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization\nNov 3 (Mon) - Lab S4 : Working on the command line in Terminal with Shell\nNov 5 (Wed) - Lab 10 : Detecting Patterns in Regular Expression\nNov 10 (Mon) - Lab S5 : Running bioinformatics software interactively and in batches\nNov 12 (Wed) - Lab 11 : Joining Data Frames\nNov 17 (Mon) - Catch up\nNov 19 (Wed) - Lab 12 : Accessing NEON Metadata for Metagenomics\nNov 24 (Mon) - Lab S6 : AI assisted analysis of the NEON MAG taxonomy, soil chemistry and location data - Part I\nNov 26 (Wed) Holiday - Thanksgiving\nDec 1 (Mon) - Lab S7 : AI assisted analysis of the NEON MAG taxonomy, soil chemistry and location data - Part II\nDec 3 (Wed) - Lab 13 :\nDec 8 (Mon) - Lab S8 :"
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "CURE Project Specific Methods",
    "section": "",
    "text": "phyloNEON\nAccess_NEON_Data_for_Metagenomics\nVisualizing NEON samples within a plot"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Project Space",
    "section": "",
    "text": "“One Health is a collaborative, multisectoral, and transdisciplinary approach — working at the local, regional, national, and global levels — with the goal of achieving optimal health outcomes recognizing the interconnection between people, animals, plants, and their shared environment.” -Center for Disease Control (CDC) One Health. In addition to the CDC One Health, the One Health perspective is supported by the One Health Commission (OHC), One Health Initiative, One Health Platform, CDC One Health Office, the Food and Agriculture Organization of the United Nations (FAO), the United Nations Environment Programme (UNEP), the World Organisation for Animal Health (WOAH, founded as OIE), and the World Health Organization (WHO) – One Health Joint Plan of Action, 2022–2026. “One Health issues include emerging, re-emerging, and endemic zoonotic diseases, neglected tropical diseases, vector-borne diseases, antimicrobial resistance, food safety and food security, environmental contamination, climate change and other health threats shared by people, animals, and the environment.” - Center for Disease Control (CDC) One Health\n\n\n\nOne Health - https://en.wikipedia.org/wiki/One_Health\n\n\n\n\n\nOur environmental data will come from the National Ecological Observatory Network (NEON) funded through the National Science Foundation. NEON’s mission is to “To collect and freely share critical ecological data, samples, and infrastructure with researchers and the public to advance understanding of ecological processes and inform the sustainable management of U.S. ecosystems.” NEON’s data are an important part of the One Health framework.\n\n\n\nNEON Field Sites - https://www.neonscience.org/field-sites/about-field-sites\n\n\nThe NEON Harvard Forest and Quabbin Watershed sites are of particular interest to us in Massachusetts. The Quabbin Reservoir is the primary water supply for Boston, 40 towns in the Greater Boston area and several surrounding towns. There is a rich history of the river valley. The name Quabbin, meaning meeting of many waters, is after Native American chief Nani-Quaben. Artifacts show that people (ancestors of the Nipmucs) were living in the Swift River Valley as far back as 12,000 years - Ref. Here is a timeline of events including the removal of the towns of Dana, Prescott, Enfield and Greenwich. Today the Massachusetts Department of Conservation and Recreation (DCR) manages the forests surrounding the watershed which provide a living green bio-filter. The watershed “catches the rain, stores it, and releases it slowly, soaking up nutrients, keeping erosion to a minimum, and yielding a consistent supply of clean water.”\n\n\n\nNEON Harvard Forest and Quabbin sites\n\n\nHere are some of the NEON data portals we may work with\n\nNEON Soils\nNEON Soil Microbes\nNEON Aquatic Microbes\nNEON Pathogens\nNEON Biorepository\n\n\n\n\nOur metagenomic data comes from a collaboration between NEON and the Joint Genome Institute at the Lawrence Berkeley National Laboratory and supported by the US Department of Energy. We can browse the data at the Integrated Microbial Genome & Metagenome (IMG/M) system which supports the annotation, analysis, and distribution of microbial genome and microbiome datasets sequenced at JGI.\nJGI has done all lot of data processing for us, but there is much still to do.\n\n\n\nDOE JGI Metagenome Workflow - https://journals.asm.org/doi/10.1128/msystems.00804-20"
  },
  {
    "objectID": "projects.html#one-health",
    "href": "projects.html#one-health",
    "title": "Project Space",
    "section": "",
    "text": "For our course projects we will work with the One Health framework. “One Health is a collaborative, multisectoral, and transdisciplinary approach — working at the local, regional, national, and global levels — with the goal of achieving optimal health outcomes recognizing the interconnection between people, animals, plants, and their shared environment.” -Center for Disease Control (CDC) One Health. In addition to the CDC One Health, the One Health perspective is supported by the One Health Commission (OHC), One Health Initiative, One Health Platform, CDC One Health Office, the Food and Agriculture Organization of the United Nations (FAO), the United Nations Environment Programme (UNEP), the World Organisation for Animal Health (WOAH, founded as OIE), and the World Health Organization (WHO) – One Health Joint Plan of Action, 2022–2026. “One Health issues include emerging, re-emerging, and endemic zoonotic diseases, neglected tropical diseases, vector-borne diseases, antimicrobial resistance, food safety and food security, environmental contamination, climate change and other health threats shared by people, animals, and the environment.” - Center for Disease Control (CDC) One Health\n\n\n\nOne Health - https://en.wikipedia.org/wiki/One_Health"
  },
  {
    "objectID": "projects.html#the-national-science-foudation-nsf-national-ecological-observatory-network-neon",
    "href": "projects.html#the-national-science-foudation-nsf-national-ecological-observatory-network-neon",
    "title": "Project Space",
    "section": "The National Science Foudation (NSF) National Ecological Observatory Network (NEON)",
    "text": "The National Science Foudation (NSF) National Ecological Observatory Network (NEON)\nOur environmental data will come from the National Ecological Observatory Network (NEON) funded by the National Science Foundation."
  },
  {
    "objectID": "projects.html#the-department-of-energy-doe-joint-genome-institute",
    "href": "projects.html#the-department-of-energy-doe-joint-genome-institute",
    "title": "Project Space",
    "section": "The Department of Energy (DOE) Joint Genome Institute",
    "text": "The Department of Energy (DOE) Joint Genome Institute\n\nOverview\nOur metagenomic data comes from a collaboration between NEON and the Joint Genome Institute at the Lawrence Berkeley National Laboratory and supported by the US Department of Energy. We can browse the data at the Integrated Microbial Genome & Metagenome (IMG/M) system which supports the annotation, analysis, and distribution of microbial genome and microbiome datasets sequenced at JGI.\n\n\nOur metagenome data\nWe will work with the Gs0166454 and Gs0161344 study sets. JGI has done all lot of data processing for us, but there is much still to do.\n\n\n\nDOE JGI Metagenome Workflow - https://journals.asm.org/doi/10.1128/msystems.00804-20"
  },
  {
    "objectID": "projects.html#the-national-ecological-observatory-network-neon",
    "href": "projects.html#the-national-ecological-observatory-network-neon",
    "title": "Project Space",
    "section": "The National Ecological Observatory Network (NEON)",
    "text": "The National Ecological Observatory Network (NEON)\n\nOverview\nOur environmental data will come from the National Ecological Observatory Network (NEON) funded through the National Science Foundation. NEON’s mission is to “To collect and freely share critical ecological data, samples, and infrastructure with researchers and the public to advance understanding of ecological processes and inform the sustainable management of U.S. ecosystems.” NEON’s data are an important part of the One Health framework.\n\n\n\nNEON Field Sites - https://www.neonscience.org/field-sites/about-field-sites\n\n\n\n\nNEON Harvard Forest and Quabbin Watershed sites\nThe NEON Harvard Forest and Quabbin Watershed sites are of particular interest to us in Massachusetts. The Quabbin Reservoir is the primary water supply for Boston, 40 towns in the Greater Boston area and several surrounding towns. There is a rich history of the river valley. The name Quabbin, meaning meeting of many waters, is after Native American chief Nani-Quaben. Artifacts show that people (ancestors of the Nipmucs) were living in the Swift River Valley as far back as 12,000 years - Ref. Here is a timeline of events including the removal of the towns of Dana, Prescott, Enfield and Greenwich. Today the Massachusetts Department of Conservation and Recreation (DCR) manages the forests surrounding the watershed which provide a living green bio-filter. The watershed “catches the rain, stores it, and releases it slowly, soaking up nutrients, keeping erosion to a minimum, and yielding a consistent supply of clean water.”\n\n\n\nNEON Harvard Forest and Quabbin sites\n\n\n\n\nNEON Data Portals\n\nNEON Soils\nNEON Soil Microbes\nNEON Aquatic Microbes\nNEON Pathogens\nNEON Biorepository"
  },
  {
    "objectID": "labs.html",
    "href": "labs.html",
    "title": "Labs",
    "section": "",
    "text": "Group Vibes\n\n\nLong Labs\n\nLab 1 : Overview & Getting Started\nLab 2 : Learning R with the Help of AI tools Starting with Graphing using ggplot\nLab 3 : Data Transformation with dplyr\nLab 4 : Coding Basics, Style and Data Read/Write\nLab 5 : Working with NEON Metagenome Assembled Genomes (MAGs / bins)\nLab 6 : ggplotting with NEON MAG data\nLab 7 : Data Tidying, Transformation and Visualization with COVID-19 reporting data\nLab 8: Catch up, What Will viruses Do Next?, The First AI Genomes and metagenome Brainstorming\nLab 9 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization\nLab 10 : Detecting Patters with Regular Expressions\nLab 11 : Joining Data Frames\nLab 12 : Accessing NEON Metadata for Metagenomics\nLab 13 : Creating a detailed report with explanations from the NEON MAG data\n\nShort Labs\nSharing and Publishing\n\nLab S1 : Quarto\nLab S2 : Adding References to Your Report with Zotero\nLab S3 : Connecting a Github repo site with a RStudio project\n\nUnity\n\nLab S4 : Working on the command line in Terminal with Shell\nLab S5 - Running bioinformatics software interactively and in batches\n\nAI-driven\n\nLab S6 : AI assisted analysis of the NEON MAG taxonomy, soil chemistry and spatial location data - I\nLab S7 : AI assisted analysis of the NEON MAG taxonomy, soil chemistry and spatial location data - II"
  },
  {
    "objectID": "report.html",
    "href": "report.html",
    "title": "Pathogen Genome Analysis Fall 2025",
    "section": "",
    "text": "Report\nManuscript style with Title Abstract Intro Methods Results Discussion Future Directions\nPublic communication Poems"
  },
  {
    "objectID": "projects.html#the-joint-genome-institute",
    "href": "projects.html#the-joint-genome-institute",
    "title": "Project Space",
    "section": "The Joint Genome Institute",
    "text": "The Joint Genome Institute\n\nOverview\nOur metagenomic data comes from a collaboration between NEON and the Joint Genome Institute at the Lawrence Berkeley National Laboratory and supported by the US Department of Energy. We can browse the data at the Integrated Microbial Genome & Metagenome (IMG/M) system which supports the annotation, analysis, and distribution of microbial genome and microbiome datasets sequenced at JGI.\n\n\nOur metagenome data\nWe will work with the Gs0166454 and Gs0161344 study sets. JGI has done all lot of data processing for us, but there is much still to do.\n\n\n\nDOE JGI Metagenome Workflow - https://journals.asm.org/doi/10.1128/msystems.00804-20"
  },
  {
    "objectID": "manuscripts.html",
    "href": "manuscripts.html",
    "title": "Manuscripts",
    "section": "",
    "text": "Should labs be in Manuscript form too?\nQuarto documentation on manuscripts https://quarto.org/docs/manuscripts/\nSee Quarto section on RStudio IDE for spell checking https://quarto.org/docs/visual-editor/content.html\nReproducible publishing with Quarto Dr. Mine Çetinkaya-Rundel Dr. Charlotte Wickham https://mine-cetinkaya-rundel.github.io/quarto-jsm24/4-manuscripts/4-manuscripts.html#/title-slide https://mine.quarto.pub/quarto-manuscripts-rmed/#/title-slide\nCut the tyranny of copy-and-paste with these coding tools https://www.nature.com/articles/d41586-022-00563-z\nNotebooks now! https://data.agu.org/notebooks-now/\nDecent example with references - https://jperkel.github.io/computed_quarto_manuscript/\nIntroducing eLife’s first computationally reproducible article https://elifesciences.org/labs/ad58f08d/introducing-elife-s-first-computationally-reproducible-article\nPioneering ‘live-code’ article allows scientists to play with each other’s results https://www.nature.com/articles/d41586-019-00724-7"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "A Zotero Workflow for R - https://www.anthonyschmidt.co/post/2021-10-25-a-zotero-workflow-for-r/"
  },
  {
    "objectID": "phyloNEON.html",
    "href": "phyloNEON.html",
    "title": "phyloNEON",
    "section": "",
    "text": "This is from https://github.com/NEONScience/phyloNEON/blob/main/README.md and https://github.com/NEONScience/phyloNEON/blob/main/docs/metagenomic/README.md\nA set of tools in R and Python to run phylogenetic and taxonomic analyses on NEON and related data"
  },
  {
    "objectID": "phyloNEON.html#installation",
    "href": "phyloNEON.html#installation",
    "title": "phyloNEON",
    "section": "Installation",
    "text": "Installation\nTo install phyloNEON, you will need the devtools package.\nlibrary(devtools)\n\ninstall_github(\"NEONScience/phyloNEON/phyloNEON\")"
  },
  {
    "objectID": "phyloNEON.html#accessing-and-using-neon-genetic-data",
    "href": "phyloNEON.html#accessing-and-using-neon-genetic-data",
    "title": "phyloNEON",
    "section": "Accessing and using NEON genetic data",
    "text": "Accessing and using NEON genetic data\nNEON offers several data products that include genetic data. This repository is being developed to include tools and guidelines to help users of NEON data to better utilize the genetic data.\n\nNEON metagenomic data\nDNA is extracted from NEON soil and aquatic samples and sequenced with a shotgun sequence library prep. Through collaborations with the Joint Genome Institute (JGI) and the National Microbime Collaborative Network (NMDC), most of the metagenomic sequencing data are available on the data portals of these organizations. Connections to these external data sources are being built into NEON data releases. The phyloNEON package also offers some tools and guidelines to help the user find and analyze NEON metagenomic data on the JGI/NMDC data portals.\nThis page on the repo (in docs/metagenomic/README.md) will help you get started"
  },
  {
    "objectID": "phyloNEON.html#accessing-neon-samples-on-the-jgi-img-data-portal",
    "href": "phyloNEON.html#accessing-neon-samples-on-the-jgi-img-data-portal",
    "title": "phyloNEON",
    "section": "Accessing NEON samples on the JGI IMG data portal",
    "text": "Accessing NEON samples on the JGI IMG data portal\n\nNEON metagenome database\nA table (neon.metaDB) has been added to the phyloNEON package that contains over 1,800 NEON metagenome samples that are on the JGI IMG data portal. This includes legacy data as well as all samples that are part of the JGI CSP award, which covers deep sequencing and analysis by JGI of all NEON metagenome samples collected in 2023 and 2024. Included in the table are several fields with JGI metadata and statistics for each sample, such as Sequencing Method, GenomeSize, GeneCount, and number of bins (metaBATbinCount). Also included are some NEON variables such as siteID and collectDate, as well as multiple environmental terms assigned to each sample according to ENVO specifications (e.g. Ecosystem Category, Ecosystem Type, Specific Ecosystem). The table also has reference codes for the Genome Online Database (GOLD), including GOLD Analysis Project ID and GOLD Study ID; and the taxon OID (imgGenomeID) that allows accessing the sample on the JGI IMG data portal.\nThis table is available when you load the package phyloNEON.\n\n\nR code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(phyloNEON)\nlibrary(DT)\nlibrary(viridis)\n\n\nSave file to have version used\n\n\nR code\nwrite_csv(neon.metaDB, \"data/NEON_metadata/neon.metaDB_20250701.csv\")\n\n\nTo view the table neon.metaDB (Note set eval = FALSE or do not include in your R code or you will get an error when rendering)\n\n\nR code\nView(neon.metaDB)\n\n\nTo view the structure of the neon.metaDB\n\n\nR code\nstr(neon.metaDB)\n\n\ntibble [1,834 × 32] (S3: tbl_df/tbl/data.frame)\n $ dnaSampleID               : chr [1:1834] \"ONAQ_044-M-20190619-COMP-DNA1\" \"PUUM_031-O-20210104-COMP-DNA1\" \"TECR.20230821.EPILITHON.8.DNA-DNA1\" \"KONZ_003-M-20170710-COMP-DNA1\" ...\n $ imgGenomeID               : num [1:1834] 3.3e+09 3.3e+09 3.3e+09 3.3e+09 3.3e+09 ...\n $ jgiProjectID              : num [1:1834] NA NA 1506438 0 1500369 ...\n $ ITS Proposal ID           : num [1:1834] NA NA 509938 NA 509938 ...\n $ Sequencing Status         : chr [1:1834] \"Permanent Draft\" \"Permanent Draft\" \"Permanent Draft\" \"Permanent Draft\" ...\n $ Study Name                : chr [1:1834] \"Terrestrial soil microbial communities from various locations - NEON\" \"Terrestrial soil microbial communities from various locations - NEON\" \"Soil and water microbial communities from various NEON Field Sites across the United States\" \"Terrestrial soil microbial communities from various locations - NEON\" ...\n $ GenomeName                : chr [1:1834] \"Terrestrial soil microbial communities from Onaqui, Utah, USA - ONAQ_044-M-20190619-COMP-DNA1\" \"Terrestrial soil microbial communities from Puu Makaala Natural Area Reserve, Hawaii, USA - PUUM_031-O-20210104-COMP-DNA1\" \"Freshwater microbial communities from Teakettle 2 Creek NEON Field Site, Sierra National Forest, CA, USA - TECR\"| __truncated__ \"Terrestrial soil microbial communities from Konza Prairie Biological Station, Prairie Peninsula, KS, USA - KONZ\"| __truncated__ ...\n $ Sequencing Center         : chr [1:1834] \"Battelle Memorial Institute\" \"Battelle Memorial Institute\" \"DOE Joint Genome Institute  (JGI)\" \"Battelle Memorial Institute\" ...\n $ GOLD Analysis Project ID  : chr [1:1834] \"Ga0620072\" \"Ga0619546\" \"Ga0672972\" \"Ga0428256\" ...\n $ GOLD Analysis Project Type: chr [1:1834] \"Metagenome Analysis\" \"Metagenome Analysis\" \"Metagenome Analysis\" \"Metagenome Analysis\" ...\n $ GOLD Sequencing Project ID: chr [1:1834] \"Gp0766640\" \"Gp0766114\" \"Gp0812633\" \"Gp0476824\" ...\n $ GOLD Study ID             : chr [1:1834] \"Gs0144570\" \"Gs0144570\" \"Gs0166454\" \"Gs0144570\" ...\n $ Funding Program           : chr [1:1834] NA NA \"CSP\" NA ...\n $ Sequencing Method         : chr [1:1834] \"Illumina NextSeq 550\" \"Illumina NextSeq 550\" \"Illumina NovaSeq X 10B\" \"Illumina NextSeq 550\" ...\n $ Sequencing Quality        : chr [1:1834] \"Level 1: Standard Draft\" \"Level 1: Standard Draft\" \"Level 1: Standard Draft\" \"Level 1: Standard Draft\" ...\n $ siteID                    : chr [1:1834] \"ONAQ\" \"PUUM\" \"TECR\" \"KONZ\" ...\n $ collectDate               : chr [1:1834] \"20190619\" \"20210104\" \"20230821\" \"20170710\" ...\n $ Ecosystem                 : chr [1:1834] \"Environmental\" \"Environmental\" \"Environmental\" \"Environmental\" ...\n $ Ecosystem Category        : chr [1:1834] \"Terrestrial\" \"Terrestrial\" \"Aquatic\" \"Terrestrial\" ...\n $ Ecosystem Subtype         : chr [1:1834] \"Unclassified\" \"Forest\" \"Creek\" \"Unclassified\" ...\n $ Ecosystem Type            : chr [1:1834] \"Soil\" \"Soil\" \"Freshwater\" \"Soil\" ...\n $ Specific Ecosystem        : chr [1:1834] \"Unclassified\" \"Unclassified\" \"Unclassified\" \"Unclassified\" ...\n $ GenomeSize                : num [1:1834] 3.57e+06 8.86e+04 8.33e+08 2.23e+07 7.62e+08 ...\n $ GeneCount                 : num [1:1834] 11955 313 1296700 63840 1113040 ...\n $ ScaffoldCount             : num [1:1834] 10633 277 671686 56485 522571 ...\n $ metaBATbinCount           : num [1:1834] 0 0 11 0 23 5 0 7 0 0 ...\n $ eukCCbinCount             : num [1:1834] 0 0 2 0 2 0 0 0 0 0 ...\n $ estNumberGenomes          : num [1:1834] 0 0 147 0 151 215 0 132 0 0 ...\n $ avgGenomeSize             : num [1:1834] 0 0 5663975 0 5046004 ...\n $ numberFilteredReads       : num [1:1834] 0.00 0.00 1.87e+08 0.00 3.15e+08 ...\n $ numberMappedReads         : num [1:1834] 0.00 0.00 1.03e+08 0.00 2.78e+08 ...\n $ pctAssembledReads         : num [1:1834] 0 0 55.4 0 88.2 ...\n\n\nConvert the collectDate from character to date format\n\n\nR code\nneon.metaDB.my &lt;- neon.metaDB\nneon.metaDB.my$collectDate &lt;- as.numeric(neon.metaDB.my$collectDate)\nneon.metaDB.my$collectDate &lt;- ymd(neon.metaDB.my$collectDate)\nstr(neon.metaDB.my$collectDate)\n\n\n Date[1:1834], format: \"2019-06-19\" \"2021-01-04\" \"2023-08-21\" \"2017-07-10\" \"2023-07-12\" ...\n\n\n\n\nTable of mean genome size per year\n\n\nR code\nneon.metaDB.my |&gt; \n  filter(`GOLD Analysis Project Type` != \"Combined Assembly\") |&gt; \n  group_by(year = lubridate::year(collectDate)) |&gt; \n  summarize(mean_GenomeSize = mean(GenomeSize))\n\n\n# A tibble: 9 × 2\n   year mean_GenomeSize\n  &lt;dbl&gt;           &lt;dbl&gt;\n1  2013        8635723.\n2  2014        7670580.\n3  2016       16242281.\n4  2017       17172060.\n5  2018       14100181.\n6  2019       16735397.\n7  2020       37402350.\n8  2021     1334660078.\n9  2023     1310246892.\n\n\n\n\nTable HARV mean genome size per year\n\n\nR code\nneon.metaDB.my |&gt; \n  filter(siteID == \"HARV\") |&gt; \n  group_by(year = lubridate::year(collectDate)) |&gt; \n  summarize(mean_GenomeSize = mean(GenomeSize))\n\n\n# A tibble: 6 × 2\n   year mean_GenomeSize\n  &lt;dbl&gt;           &lt;dbl&gt;\n1  2013        4721159.\n2  2016       17096867.\n3  2017       29942423.\n4  2019       13990976.\n5  2020       30567005.\n6  2023     2236533461.\n\n\n\n\nPlot of genome size per year\n\n\nR code\nneon.metaDB.my |&gt; \n  ggplot(aes(x=collectDate, y = GenomeSize)) +\n  geom_col(colour = \"maroon\", fill = \"maroon\") +\n  coord_flip()\n\n\n\n\n\n\n\n\n\nTo reformat dnasampleID column for terrestrial samples (This does not work for the aquatic samples)\n\n\nR code\nneon.metaDB.my.soil &lt;- neon.metaDB.my |&gt; \n  filter(`Ecosystem Category` == \"Terrestrial\") |&gt; \n  filter(`GOLD Analysis Project Type` != \"Combined Assembly\") |&gt; \n  \n  separate(`dnaSampleID`, c(\"dnaSampleID.site\",\"dnaSampleID.sub\"), \"_\", remove=FALSE) |&gt; \n  \n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-COMP\", \"_COMP\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-GEN\", \"_GEN\") |&gt;\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.sub\",\"dnaSampleID.type\"), \"_\") |&gt; \n  \n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-M\", \"_M\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-O\", \"_O\") |&gt;\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.plot\",\"dnaSampleID.sub\"), \"_\") |&gt; \n  \n  mutate_at(\"dnaSampleID.sub\", str_replace, \"M-\", \"M_\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"O-\", \"O_\") |&gt;\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.layer\",\"dnaSampleID.sub\"), \"_\") |&gt; \n\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-201\", \"201\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-202\", \"202\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"201\", \"_201\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"202\", \"_202\") |&gt;\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.subplot\",\"dnaSampleID.date\"), \"_\") |&gt; \n\n  unite(plotID, c(dnaSampleID.site, dnaSampleID.plot), sep='_', remove=FALSE)\n\nneon.metaDB.my.soil$dnaSampleID.data &lt;- as.numeric(neon.metaDB.my.soil$dnaSampleID.date)\nneon.metaDB.my.soil$dnaSampleID.date &lt;- ymd(neon.metaDB.my.soil$dnaSampleID.date)\n\n\nTo reformat dnasampleID column for aquatic samples\n\n\nR code\nneon.metaDB.my.aquatic &lt;- neon.metaDB.my |&gt; \n  filter(`Ecosystem Category` == \"Aquatic\") |&gt; \n  filter(`GOLD Analysis Project Type` != \"Combined Assembly\") |&gt; \n  \n  mutate(dnaSampleID.sub = dnaSampleID) |&gt; \n  mutate_at(\"dnaSampleID.sub\", str_replace, \".202\", \"_202\") |&gt;\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.site\",\"dnaSampleID.sub\"), \"_\") |&gt; \n  separate(`dnaSampleID.site`, c(\"dnaSampleID.site\",\"dnaSampleID.code\"), \"\\\\.\") |&gt; \n  \n  mutate_at(\"dnaSampleID.sub\", str_replace, \".DNA\", \"_DNA\") |&gt;\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.sub\",\"dnaSampleID.type\"), \"_\") |&gt; \n  \n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.data\",\"dnaSampleID.niche\", \"dnaSampleID.num\"), \"\\\\.\") |&gt; \n\n  unite(dnaSampleID.niche, c(dnaSampleID.code, dnaSampleID.niche)) |&gt; \n  mutate_at(\"dnaSampleID.niche\", str_replace, \"NA_\", \"\") |&gt;\n  mutate_at(\"dnaSampleID.niche\", str_replace, \"_NA\", \"\")\n\n\n\n\nHARV metagenomes by year and plot\n\n\nR code\ndatatable(\nneon.metaDB.my.soil |&gt; \n  filter(siteID == \"HARV\") |&gt; \n  group_by(Year = lubridate::year(collectDate), dnaSampleID.plot) |&gt; \n  count() |&gt; \n  pivot_wider(names_from = dnaSampleID.plot, values_from = n) |&gt; \n  mutate_all(funs(replace_na(.,0)))\n)\n\n\n\n\n\n\n\n\nWREF metagenomes by year and plot\n\n\nR code\ndatatable(\nneon.metaDB.my.soil |&gt; \n  filter(siteID == \"WREF\") |&gt; \n  group_by(Year = lubridate::year(collectDate), dnaSampleID.plot) |&gt; \n  count() |&gt; \n  pivot_wider(names_from = dnaSampleID.plot, values_from = n) |&gt; \n  mutate_all(funs(replace_na(.,0)))\n)\n\n\n\n\n\n\n\n\nPlot of HARV samples per plot per year\n\n\nR code\nneon.metaDB.my.soil |&gt; \n  filter(siteID == \"HARV\") |&gt; \n  group_by(Year = lubridate::year(collectDate), dnaSampleID.plot) |&gt; \n  count() |&gt; \n  pivot_wider(names_from = dnaSampleID.plot, values_from = n) |&gt; \n  mutate_all(funs(replace_na(.,0))) |&gt; \n  pivot_longer(!Year, names_to = \"plot\", values_to = \"metagenomes\") |&gt; \n  ggplot(aes(x=Year, y = plot)) +\n  geom_tile(aes(fill = metagenomes)) +\n  scale_fill_viridis(discrete=FALSE, direction = -1) +\n  scale_x_continuous(breaks = seq(2013, 2023, by = 1)) \n\n\n\n\n\n\n\n\n\n\n\nMissing years at HARV\nAll sites are missing data from 2021 and 2022. That should be in IMG soon. What about 2018?\n\nCollectDate\n\n\nR code\nneon.metaDB.my.soil |&gt; \n  group_by(Year = lubridate::year(collectDate)) |&gt; \n  count() \n\n\n# A tibble: 9 × 2\n# Groups:   Year [9]\n   Year     n\n  &lt;dbl&gt; &lt;int&gt;\n1  2013    63\n2  2014   106\n3  2016   229\n4  2017   326\n5  2018    45\n6  2019   231\n7  2020   185\n8  2021   117\n9  2023   303\n\n\n\n\ndnaSampleID.date\n\n\nR code\nneon.metaDB.my.soil |&gt; \n  group_by(Year = lubridate::year(dnaSampleID.date)) |&gt; \n  count() \n\n\n# A tibble: 9 × 2\n# Groups:   Year [9]\n   Year     n\n  &lt;dbl&gt; &lt;int&gt;\n1  2013    63\n2  2014   106\n3  2016   229\n4  2017   326\n5  2018    45\n6  2019   231\n7  2020   185\n8  2021   117\n9  2023   303\n\n\n\n\n\nPlot of samples per plot per year at all sites\n\n\nR code\nneon.metaDB.my.soil |&gt; \n  group_by(siteID, Year = lubridate::year(collectDate), dnaSampleID.plot) |&gt; \n  count() |&gt; \n  pivot_wider(names_from = Year, values_from = n) |&gt; \n  mutate_all(funs(replace_na(.,0))) |&gt; \n  pivot_longer(!c(siteID, dnaSampleID.plot), names_to = \"Year\", values_to = \"metagenomes\") |&gt; \n  ggplot(aes(x=Year, y = dnaSampleID.plot)) +\n  geom_tile(aes(fill = metagenomes)) +\n  scale_fill_viridis(discrete=FALSE, direction = -1) +\n # scale_x_continuous(breaks = seq(2013, 2023, by = 1)) +\n  facet_wrap(~siteID, scales =\"free_y\", ncol = 3) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))"
  },
  {
    "objectID": "project_opps.html",
    "href": "project_opps.html",
    "title": "Project Opportunities",
    "section": "",
    "text": "We need to randomly pull 1/3 reads from each individual core then combine and assemble\n\n\nhttps://www.neonscience.org/field-sites/harv\n\nHARV_001-O-20230705-COMP-DNA1\nHARV_002-O-20230706-COMP-DNA1\nHARV_005-O-20230710-COMP-DNA1\nHARV_013-O-20230704-COMP-DNA1\nHARV_021-O-20230706-COMP-DNA1\nHARV_033-O-20230703-COMP-DNA1\nHARV_035-O-20230704-COMP-DNA1\n\n\n\n\nhttps://www.neonscience.org/field-sites/sjer\n\nSJER_001-M-20230227-COMP-DNA1\nSJER_002-M-20230228-COMP-DNA1\nSJER_005-M-20230306-COMP-DNA1\nSJER_025-M-20230302-COMP-DNA1\nSJER_045-M-20230227-COMP-DNA1\nSJER_046-M-20230227-COMP-DNA1\n\n\n\n\n\nPossibility of combined assembly and mapping\n\n\nhttps://www.neonscience.org/field-sites/hopb\n\nFreshwater microbial communities from Lower Hop Brook NEON Field Site, New Salem, MA, USA - HOPB.SS.20230117.DNA-DNA1\nFreshwater microbial communities from Lower Hop Brook NEON Field Site, New Salem, MA, USA - HOPB.SS.20230221.DNA-DNA1\nFreshwater microbial communities from Lower Hop Brook NEON Field Site, New Salem, MA, USA - HOPB.SS.20230419.DNA-DNA1\nFreshwater microbial communities from Lower Hop Brook NEON Field Site, New Salem, MA, USA - HOPB.SS.20230620.DNA-DNA1\nFreshwater microbial communities from Hop Brook NEON Field Site, New Salem, MA, USA - HOPB.SS.20230719.DNA-DNA1 (version 2)\nFreshwater microbial communities from Lower Hop Brook NEON Field Site, New Salem, MA, USA - HOPB.SS.20230815.DNA-DNA1\nFreshwater microbial communities from Lower Hop Brook NEON Field Site, New Salem, MA, USA - HOPB.SS.20231017.DNA-DNA1\n\n\n\n\nhttps://www.neonscience.org/field-sites/cari\n\nFreshwater microbial communities from Caribou Creek NEON Field Site, Chatanika, Alaska, USA - CARI.SS.20230110.DNA-DNA1\nFreshwater microbial communities from Caribou Creek NEON Field Site, Chatanika, Alaska, USA - CARI.SS.20230314.DNA-DNA1e\nFreshwater microbial communities from Caribou Creek NEON Field Site, Chatanika, Alaska, USA - CARI.SS.20230502.DNA-DNA1\nFreshwater microbial communities from Caribou Creek NEON Field Site, Chatanika, Alaska, USA - CARI.SS.20230731.DNA-DNA1\nFreshwater microbial communities from Caribou Creek NEON Field Site, Chatanika, Alaska, USA - CARI.SS.20230905.DNA-DNA1\nFreshwater microbial communities from Caribou Creek NEON Field Site, Chatanika, Alaska, USA - CARI.SS.20231031.DNA-DNA1"
  },
  {
    "objectID": "project_opps.html#projects",
    "href": "project_opps.html#projects",
    "title": "Project Opportunities",
    "section": "",
    "text": "We need to randomly pull 1/3 reads from each individual core then combine and assemble\n\n\nhttps://www.neonscience.org/field-sites/harv\n\nHARV_001-O-20230705-COMP-DNA1\nHARV_002-O-20230706-COMP-DNA1\nHARV_005-O-20230710-COMP-DNA1\nHARV_013-O-20230704-COMP-DNA1\nHARV_021-O-20230706-COMP-DNA1\nHARV_033-O-20230703-COMP-DNA1\nHARV_035-O-20230704-COMP-DNA1\n\n\n\n\nhttps://www.neonscience.org/field-sites/sjer\n\nSJER_001-M-20230227-COMP-DNA1\nSJER_002-M-20230228-COMP-DNA1\nSJER_005-M-20230306-COMP-DNA1\nSJER_025-M-20230302-COMP-DNA1\nSJER_045-M-20230227-COMP-DNA1\nSJER_046-M-20230227-COMP-DNA1\n\n\n\n\n\nPossibility of combined assembly and mapping\n\n\nhttps://www.neonscience.org/field-sites/hopb\n\nFreshwater microbial communities from Lower Hop Brook NEON Field Site, New Salem, MA, USA - HOPB.SS.20230117.DNA-DNA1\nFreshwater microbial communities from Lower Hop Brook NEON Field Site, New Salem, MA, USA - HOPB.SS.20230221.DNA-DNA1\nFreshwater microbial communities from Lower Hop Brook NEON Field Site, New Salem, MA, USA - HOPB.SS.20230419.DNA-DNA1\nFreshwater microbial communities from Lower Hop Brook NEON Field Site, New Salem, MA, USA - HOPB.SS.20230620.DNA-DNA1\nFreshwater microbial communities from Hop Brook NEON Field Site, New Salem, MA, USA - HOPB.SS.20230719.DNA-DNA1 (version 2)\nFreshwater microbial communities from Lower Hop Brook NEON Field Site, New Salem, MA, USA - HOPB.SS.20230815.DNA-DNA1\nFreshwater microbial communities from Lower Hop Brook NEON Field Site, New Salem, MA, USA - HOPB.SS.20231017.DNA-DNA1\n\n\n\n\nhttps://www.neonscience.org/field-sites/cari\n\nFreshwater microbial communities from Caribou Creek NEON Field Site, Chatanika, Alaska, USA - CARI.SS.20230110.DNA-DNA1\nFreshwater microbial communities from Caribou Creek NEON Field Site, Chatanika, Alaska, USA - CARI.SS.20230314.DNA-DNA1e\nFreshwater microbial communities from Caribou Creek NEON Field Site, Chatanika, Alaska, USA - CARI.SS.20230502.DNA-DNA1\nFreshwater microbial communities from Caribou Creek NEON Field Site, Chatanika, Alaska, USA - CARI.SS.20230731.DNA-DNA1\nFreshwater microbial communities from Caribou Creek NEON Field Site, Chatanika, Alaska, USA - CARI.SS.20230905.DNA-DNA1\nFreshwater microbial communities from Caribou Creek NEON Field Site, Chatanika, Alaska, USA - CARI.SS.20231031.DNA-DNA1"
  },
  {
    "objectID": "projects.html#overview",
    "href": "projects.html#overview",
    "title": "Project Space",
    "section": "",
    "text": "“One Health is a collaborative, multisectoral, and transdisciplinary approach — working at the local, regional, national, and global levels — with the goal of achieving optimal health outcomes recognizing the interconnection between people, animals, plants, and their shared environment.” -Center for Disease Control (CDC) One Health. In addition to the CDC One Health, the One Health perspective is supported by the One Health Commission (OHC), One Health Initiative, One Health Platform, CDC One Health Office, the Food and Agriculture Organization of the United Nations (FAO), the United Nations Environment Programme (UNEP), the World Organisation for Animal Health (WOAH, founded as OIE), and the World Health Organization (WHO) – One Health Joint Plan of Action, 2022–2026. “One Health issues include emerging, re-emerging, and endemic zoonotic diseases, neglected tropical diseases, vector-borne diseases, antimicrobial resistance, food safety and food security, environmental contamination, climate change and other health threats shared by people, animals, and the environment.” - Center for Disease Control (CDC) One Health\n\n\n\nOne Health - https://en.wikipedia.org/wiki/One_Health\n\n\n\n\n\nOur environmental data will come from the National Ecological Observatory Network (NEON) funded through the National Science Foundation. NEON’s mission is to “To collect and freely share critical ecological data, samples, and infrastructure with researchers and the public to advance understanding of ecological processes and inform the sustainable management of U.S. ecosystems.” NEON’s data are an important part of the One Health framework.\n\n\n\nNEON Field Sites - https://www.neonscience.org/field-sites/about-field-sites\n\n\nThe NEON Harvard Forest and Quabbin Watershed sites are of particular interest to us in Massachusetts. The Quabbin Reservoir is the primary water supply for Boston, 40 towns in the Greater Boston area and several surrounding towns. There is a rich history of the river valley. The name Quabbin, meaning meeting of many waters, is after Native American chief Nani-Quaben. Artifacts show that people (ancestors of the Nipmucs) were living in the Swift River Valley as far back as 12,000 years - Ref. Here is a timeline of events including the removal of the towns of Dana, Prescott, Enfield and Greenwich. Today the Massachusetts Department of Conservation and Recreation (DCR) manages the forests surrounding the watershed which provide a living green bio-filter. The watershed “catches the rain, stores it, and releases it slowly, soaking up nutrients, keeping erosion to a minimum, and yielding a consistent supply of clean water.”\n\n\n\nNEON Harvard Forest and Quabbin sites\n\n\nHere are some of the NEON data portals we may work with\n\nNEON Soils\nNEON Soil Microbes\nNEON Aquatic Microbes\nNEON Pathogens\nNEON Biorepository\n\n\n\n\nOur metagenomic data comes from a collaboration between NEON and the Joint Genome Institute at the Lawrence Berkeley National Laboratory and supported by the US Department of Energy. We can browse the data at the Integrated Microbial Genome & Metagenome (IMG/M) system which supports the annotation, analysis, and distribution of microbial genome and microbiome datasets sequenced at JGI.\nJGI has done all lot of data processing for us, but there is much still to do.\n\n\n\nDOE JGI Metagenome Workflow - https://journals.asm.org/doi/10.1128/msystems.00804-20"
  },
  {
    "objectID": "projects.html#our-project-space",
    "href": "projects.html#our-project-space",
    "title": "Project Space",
    "section": "Our Project Space",
    "text": "Our Project Space\nNEON has produced metagenomic data as data product since 2014. The Joint Genome Institute has recently annotated version all reads prior to 20222 Gs0144570. However, these metagenomes have a low sequencing depth and therefore are difficult to use for assembling bacterial genomes.\nLast year JGI and NEON collaborated to produce metagenomes which are ~10-fold deeper which allow for better assemble of reads into gene length and greater fragments. We will work with the NEON 2023 Gs0166454 study set. Although some of the data for 2024 is available, JGI is still processing the data and producing annotations (see above illustration)."
  },
  {
    "objectID": "projects.html#projects",
    "href": "projects.html#projects",
    "title": "Project Space",
    "section": "Projects",
    "text": "Projects\n\nSeparate Core vs Combined Core Metagenomes\nWe need to randomly pull 1/3 reads from each individual core then combine and assemble\n\nHarvard Forest & Quabbin (HARV), Worcester County, MA\nhttps://www.neonscience.org/field-sites/harv\n\nHARV_001-O-20230705-COMP-DNA1\nHARV_002-O-20230706-COMP-DNA1\nHARV_005-O-20230710-COMP-DNA1\nHARV_013-O-20230704-COMP-DNA1\nHARV_021-O-20230706-COMP-DNA1\nHARV_033-O-20230703-COMP-DNA1\nHARV_035-O-20230704-COMP-DNA1\n\n\n\nSan Joaquin Experimental Range (SJER), Madera County, CA\nhttps://www.neonscience.org/field-sites/sjer\n\nSJER_001-M-20230227-COMP-DNA1\nSJER_002-M-20230228-COMP-DNA1\nSJER_005-M-20230306-COMP-DNA1\nSJER_025-M-20230302-COMP-DNA1\nSJER_045-M-20230227-COMP-DNA1\nSJER_046-M-20230227-COMP-DNA1\n\n\n\n\nAquatic time series\nPossibility of combined assembly and mapping\n\nHop Brook, New Salem, MA (Quabbin)\nhttps://www.neonscience.org/field-sites/hopb\n\nFreshwater microbial communities from Lower Hop Brook NEON Field Site, New Salem, MA, USA - HOPB.SS.20230117.DNA-DNA1\nFreshwater microbial communities from Lower Hop Brook NEON Field Site, New Salem, MA, USA - HOPB.SS.20230221.DNA-DNA1\nFreshwater microbial communities from Lower Hop Brook NEON Field Site, New Salem, MA, USA - HOPB.SS.20230419.DNA-DNA1\nFreshwater microbial communities from Lower Hop Brook NEON Field Site, New Salem, MA, USA - HOPB.SS.20230620.DNA-DNA1\nFreshwater microbial communities from Hop Brook NEON Field Site, New Salem, MA, USA - HOPB.SS.20230719.DNA-DNA1 (version 2)\nFreshwater microbial communities from Lower Hop Brook NEON Field Site, New Salem, MA, USA - HOPB.SS.20230815.DNA-DNA1\nFreshwater microbial communities from Lower Hop Brook NEON Field Site, New Salem, MA, USA - HOPB.SS.20231017.DNA-DNA1\n\n\n\nCaribou Creek, Chatanika, Alaska\nhttps://www.neonscience.org/field-sites/cari\n\nFreshwater microbial communities from Caribou Creek NEON Field Site, Chatanika, Alaska, USA - CARI.SS.20230110.DNA-DNA1\nFreshwater microbial communities from Caribou Creek NEON Field Site, Chatanika, Alaska, USA - CARI.SS.20230314.DNA-DNA1\nFreshwater microbial communities from Caribou Creek NEON Field Site, Chatanika, Alaska, USA - CARI.SS.20230502.DNA-DNA1\nFreshwater microbial communities from Caribou Creek NEON Field Site, Chatanika, Alaska, USA - CARI.SS.20230731.DNA-DNA1\nFreshwater microbial communities from Caribou Creek NEON Field Site, Chatanika, Alaska, USA - CARI.SS.20230905.DNA-DNA1\nFreshwater microbial communities from Caribou Creek NEON Field Site, Chatanika, Alaska, USA - CARI.SS.20231031.DNA-DNA1"
  },
  {
    "objectID": "Access_NEON_Data_for_Metagenomics.html",
    "href": "Access_NEON_Data_for_Metagenomics.html",
    "title": "Access NEON Data for Metagenomics",
    "section": "",
    "text": "From Access NEON Data for Metagenomics See * Update on the changing NEON microbial data * Soil microbe metagenome sequences\nInstall packages\n\n\nR code\ninstall.packages(\"neonUtilities\")\ninstall.packages(\"neonOS\")\n\n\n\n\nR code\nlibrary(neonUtilities)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(viridis)\n\n\n\n\nR code\nsoilTrialSites = c(\"BONA\",\"DEJU\",\"HEAL\",\"TOOL\",\"BARR\")\nsoilTrialSites = c(\"HARV\")\n\n\nsoilChem &lt;- loadByProduct(\n  dpID='DP1.10086.001',\n  startdate = \"2017-01\",\n  enddate = \"2019-12\",\n  check.size = FALSE,\n  site = soilTrialSites,\n  package='expanded')\n\n\n\n\nR code\nView(soilChem$sls_metagenomicsPooling)\n\n\n\n\nR code\nmetaGdata &lt;- loadByProduct(dpID = 'DP1.10107.001',\n\n                          check.size = FALSE,\n\n                          package = 'expanded') \n\n\n\n\nR code\nmetaGdata_mms_metagenomeSequencing &lt;- metaGdata$mms_metagenomeSequencing\nwrite_csv(metaGdata_mms_metagenomeSequencing, \"data/NEON_metadata/metaGdata_mms_metagenomeSequencing.csv\")\n\n\nThis has HARV data collected up until 2022. The 2021 and 2022 data are not in the phyloNEON data.\n\n\nR code\nmetaGdata_mms_metagenomeSequencing_HARV &lt;- metaGdata$mms_metagenomeSequencing |&gt; \nwrite_csv(metaGdata_mms_metagenomeSequencing_HARV, \"data/NEON_metadata/metaGdata_mms_metagenomeSequencing_HARV.csv\")\n\n\n\nRead in saved file (all above in eval = FALSE)\n\n\nR code\nmetaGdata_mms_metagenomeSequencing &lt;- read_csv(\"data/NEON_metadata/metaGdata_mms_metagenomeSequencing.csv\")\n\n\n\n\nCreate data frame from dnaSampleID\n\n\nR code\nmetaGdata_dnaSampleID &lt;- metaGdata_mms_metagenomeSequencing |&gt; \n  select(dnaSampleID)\n\n# Read in 2023 and 2024 data that is not in the metagenome data product yet\n\nneon_ay23_jgi_samples &lt;- read_csv(\"data/NEON_metadata/neon_ay23_jgi_samples_soil.csv\")\nneon_ay24_jgi_samples &lt;- read_csv(\"data/NEON_metadata/neon_ay24_jgi_samples_soil.csv\")\n\nneon_dnaSampleID &lt;- rbind(metaGdata_dnaSampleID, neon_ay23_jgi_samples, neon_ay24_jgi_samples)\n\n\n\nFull table\n\n\nR code\nneon_dnaSampleID_split &lt;- neon_dnaSampleID |&gt; \n\n  separate(`dnaSampleID`, c(\"dnaSampleID.site\",\"dnaSampleID.sub\"), \"_\", remove=FALSE) |&gt; \n  \n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-comp\", \"_COMP\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-COMP\", \"_COMP\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-GEN\", \"_GEN\") |&gt;\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.sub\",\"dnaSampleID.type\"), \"_\") |&gt; \n  \n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-M\", \"_M\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-O\", \"_O\") |&gt;\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.plot\",\"dnaSampleID.sub\"), \"_\") |&gt; \n  \n  mutate_at(\"dnaSampleID.sub\", str_replace, \"M-\", \"M_\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"O-\", \"O_\") |&gt;\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.layer\",\"dnaSampleID.sub\"), \"_\") |&gt; \n\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-201\", \"201\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-202\", \"202\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"2013\", \"_2013\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"2014\", \"_2014\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"2015\", \"_2015\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"2016\", \"_2016\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"2017\", \"_2017\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"2018\", \"_2018\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"2019\", \"_2019\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"2020\", \"_2020\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"2021\", \"_2021\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"2022\", \"_2022\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"2023\", \"_2023\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"2024\", \"_2024\") |&gt;\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.subplot\",\"dnaSampleID.date\"), \"_\") |&gt; \n\n  unite(plotID, c(dnaSampleID.site, dnaSampleID.plot), sep='_', remove=FALSE)\n\nneon_dnaSampleID_split$dnaSampleID.date &lt;- as.numeric(neon_dnaSampleID_split$dnaSampleID.date)\nneon_dnaSampleID_split$dnaSampleID.date &lt;- ymd(neon_dnaSampleID_split$dnaSampleID.date)\n\n\n\n\nFull table\n\n\nR code\nmetaGdata_mms_metagenomeSequencing &lt;- metaGdata_mms_metagenomeSequencing |&gt; \n\n  separate(`dnaSampleID`, c(\"dnaSampleID.site\",\"dnaSampleID.sub\"), \"_\", remove=FALSE) |&gt; \n  \n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-comp\", \"_COMP\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-COMP\", \"_COMP\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-GEN\", \"_GEN\") |&gt;\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.sub\",\"dnaSampleID.type\"), \"_\") |&gt; \n  \n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-M\", \"_M\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-O\", \"_O\") |&gt;\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.plot\",\"dnaSampleID.sub\"), \"_\") |&gt; \n  \n  mutate_at(\"dnaSampleID.sub\", str_replace, \"M-\", \"M_\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"O-\", \"O_\") |&gt;\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.layer\",\"dnaSampleID.sub\"), \"_\") |&gt; \n\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-201\", \"201\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-202\", \"202\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"201\", \"_201\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"202\", \"_202\") |&gt;\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.subplot\",\"dnaSampleID.date\"), \"_\") |&gt; \n\n  unite(plotID, c(dnaSampleID.site, dnaSampleID.plot), sep='_', remove=FALSE)\n\nmetaGdata_mms_metagenomeSequencing$dnaSampleID.data &lt;- as.numeric(metaGdata_mms_metagenomeSequencing$dnaSampleID.date)\nmetaGdata_mms_metagenomeSequencing$dnaSampleID.date &lt;- ymd(metaGdata_mms_metagenomeSequencing$dnaSampleID.date)\n\n\n\n\n\nPlot of HARV samples per plot per year\n\n\nR code\nneon_dnaSampleID_split |&gt; \n  filter(dnaSampleID.site == \"HARV\") |&gt; \n  group_by(Year = lubridate::year(dnaSampleID.date), dnaSampleID.plot) |&gt; \n  count() |&gt; \n  pivot_wider(names_from = dnaSampleID.plot, values_from = n) |&gt; \n  mutate_all(funs(replace_na(.,0))) |&gt; \n  pivot_longer(!Year, names_to = \"plot\", values_to = \"metagenomes\") |&gt; \n  ggplot(aes(x=Year, y = plot)) +\n  geom_tile(aes(fill = metagenomes)) +\n  scale_fill_viridis(discrete=FALSE, direction = -1) +\n  scale_x_continuous(breaks = seq(2013, 2024, by = 1)) \n\n\n\n\n\n\n\n\n\n\n\nPlot of samples per plot per year at all sites\n\n\nR code\nneon_dnaSampleID_split |&gt; \n  group_by(dnaSampleID.site, Year = lubridate::year(dnaSampleID.date), dnaSampleID.plot) |&gt; \n  count() |&gt; \n  pivot_wider(names_from = Year, values_from = n) |&gt; \n  mutate_all(funs(replace_na(.,0))) |&gt; \n  pivot_longer(!c(dnaSampleID.site, dnaSampleID.plot), names_to = \"Year\", values_to = \"metagenomes\") |&gt; \n  ggplot(aes(x=Year, y = dnaSampleID.plot)) +\n  geom_tile(aes(fill = metagenomes)) +\n  scale_fill_viridis(discrete=FALSE, direction = -1) +\n  facet_wrap(~dnaSampleID.site, scales =\"free_y\", ncol = 3) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) \n\n\n\n\n\n\n\n\n\n\n\nTable of mean sampleFilteredReadNumber\nAbout 3-5x increase in reads in 2021 and 2022 About 10-20x increase in 2023 over 2021 and 2022\n\n\nR code\nmetaGdata_mms_metagenomeSequencing |&gt; \n  filter(dnaSampleID.site == \"HARV\") |&gt; \n  group_by(Year = lubridate::year(collectDate)) |&gt; \n  summarize(mean_sampleFilteredReadNumber = mean(sampleFilteredReadNumber))\n\n\n# A tibble: 8 × 2\n   Year mean_sampleFilteredReadNumber\n  &lt;dbl&gt;                         &lt;dbl&gt;\n1  2013                           NA \n2  2016                      6643162.\n3  2017                      5038437 \n4  2018                      2633512.\n5  2019                      4560748.\n6  2020                      3468523.\n7  2021                     16005210.\n8  2022                     19339618.\n\n\n\n\nTable of mean sampleTotalReadNumber\nAbout 3-5x increase in reads in 2021 and 2022 About 10-20x increase in 2023 over 2021 and 2022\n\n\nR code\nmetaGdata_mms_metagenomeSequencing |&gt; \n  filter(dnaSampleID.site == \"HARV\") |&gt; \n  group_by(Year = lubridate::year(collectDate)) |&gt; \n  summarize(mean_sampleTotalReadNumber = mean(sampleTotalReadNumber))\n\n\n# A tibble: 8 × 2\n   Year mean_sampleTotalReadNumber\n  &lt;dbl&gt;                      &lt;dbl&gt;\n1  2013                   9270681.\n2  2016                  10802077.\n3  2017                  11456008.\n4  2018                   6664285.\n5  2019                   5442366.\n6  2020                   4674903 \n7  2021                  27270286.\n8  2022                  31374636."
  },
  {
    "objectID": "lab_AI_R.html",
    "href": "lab_AI_R.html",
    "title": "Learning R with the help of AI tools",
    "section": "",
    "text": "AI won’t take your job, but someone using who knows how to use AI might. Think of AI as a force multiplier. You have to learn to code first before you can use AI to help you. Y Combinator reports that over 25% of its startups are now relying on AI for 95% of their code base, while Google recently reported that about 25% of its new code is AI-generated.\nMicrosoft designed Copilot to work off of the latest version of OpenAI’s GPT model, GPT-4,\n\ngithub co-pilot - https://github.com/copilot\ngithub education - https://github.com/education\nRStudio github copilot - https://docs.posit.co/ide/user/ide/guide/tools/copilot.html\nMicrosoft Introduction to GitHub Copilot - https://learn.microsoft.com/en-us/training/modules/introduction-to-github-copilot/\nIntroduction to GitHub CoPilot videos - https://learn.microsoft.com/en-us/shows/introduction-to-github-copilot/\nSet up co-pilot for learning - https://docs.github.com/en/get-started/learning-to-code/setting-up-copilot-for-learning-to-code\nHow to write better prompts for GitHub Copilot - https://github.blog/developer-skills/github/how-to-write-better-prompts-for-github-copilot/\nAI Assisted Coding in RStudio - https://research-it.manchester.ac.uk/news/2024/11/29/ai-assisted-coding-in-rstudio/\nRTutor AI - https://rtutor.ai/\nSyntha AI - https://syntha.ai/code-generators/r\nR Code Generator - https://codingfleet.com/code-generator/r/\nLearning the tidyverse with the help of AI tools - https://www.tidyverse.org/blog/2025/04/learn-tidyverse-ai/\nIntroducing vitals, a toolkit for evaluating LLM products in R - https://www.tidyverse.org/blog/2025/06/vitals-0-1-0/\nUsing AI with R - https://rfortherestofus.com/courses/ai\nBoost Your R Skills with AI - https://artscience.ai/boost-your-r-skills-with-ai/\nBeginner’s Tutorial for the OpenAI API in R - https://tilburg.ai/2024/03/tutorial-openai-api-in-r/\nCodex - https://chatgpt.com/codex\n\nAI Assisted Coding in RStudio - https://research-it.manchester.ac.uk/news/2024/11/29/ai-assisted-coding-in-rstudio/ Integrating OpenAI’s ChatGPT into RStudio is now possible with “Chattr”, “GPT Studio” and “GitHub Copilot”. These new tools will help you find the right functions and commands and to quickly generate code snippets to save you time.\n8 ChatGPT packages for R - https://www.infoworld.com/article/2338386/8-chatgpt-tools-for-r-programming.html\n\nVibe coding https://www.geeksforgeeks.org/techtips/what-is-vibe-coding/\nWhat is vibe coding, exactly? - https://www.technologyreview.com/2025/04/16/1115135/what-is-vibe-coding-exactly/\nWhat Is Vibe Coding? Definition, Tools, Pros, and Cons - https://www.datacamp.com/blog/vibe-coding\nYou can use GitHub and Git to collaborate on work. https://docs.github.com/en/get-started/start-your-journey/about-github-and-git\nUsing co-pilot as your tutor - https://docs.github.com/en/get-started/learning-to-code/setting-up-copilot-for-learning-to-code\nchattr - https://mlverse.github.io/chattr/\n\nBioinformatics and AI * A data-intelligence-intensive bioinformatics copilot system for large-scale omics research and scientific insights - https://academic.oup.com/bib/article/26/4/bbaf312/8196318?login=true * Bioinformatics AI: Driving Future Biological Breakthroughs - https://biologyinsights.com/bioinformatics-ai-driving-future-biological-breakthroughs/"
  },
  {
    "objectID": "methods/phyloNEON.html",
    "href": "methods/phyloNEON.html",
    "title": "phyloNEON",
    "section": "",
    "text": "This is from https://github.com/NEONScience/phyloNEON/blob/main/README.md and https://github.com/NEONScience/phyloNEON/blob/main/docs/metagenomic/README.md\nA set of tools in R and Python to run phylogenetic and taxonomic analyses on NEON and related data"
  },
  {
    "objectID": "methods/phyloNEON.html#installation",
    "href": "methods/phyloNEON.html#installation",
    "title": "phyloNEON",
    "section": "Installation",
    "text": "Installation\nTo install phyloNEON, you will need the devtools package.\nlibrary(devtools)\n\ninstall_github(\"NEONScience/phyloNEON/phyloNEON\")"
  },
  {
    "objectID": "methods/phyloNEON.html#accessing-and-using-neon-genetic-data",
    "href": "methods/phyloNEON.html#accessing-and-using-neon-genetic-data",
    "title": "phyloNEON",
    "section": "Accessing and using NEON genetic data",
    "text": "Accessing and using NEON genetic data\nNEON offers several data products that include genetic data. This repository is being developed to include tools and guidelines to help users of NEON data to better utilize the genetic data.\n\nNEON metagenomic data\nDNA is extracted from NEON soil and aquatic samples and sequenced with a shotgun sequence library prep. Through collaborations with the Joint Genome Institute (JGI) and the National Microbime Collaborative Network (NMDC), most of the metagenomic sequencing data are available on the data portals of these organizations. Connections to these external data sources are being built into NEON data releases. The phyloNEON package also offers some tools and guidelines to help the user find and analyze NEON metagenomic data on the JGI/NMDC data portals.\nThis page on the repo (in docs/metagenomic/README.md) will help you get started"
  },
  {
    "objectID": "methods/phyloNEON.html#accessing-neon-samples-on-the-jgi-img-data-portal",
    "href": "methods/phyloNEON.html#accessing-neon-samples-on-the-jgi-img-data-portal",
    "title": "phyloNEON",
    "section": "Accessing NEON samples on the JGI IMG data portal",
    "text": "Accessing NEON samples on the JGI IMG data portal\n\nNEON metagenome database\nA table (neon.metaDB) has been added to the phyloNEON package that contains over 1,800 NEON metagenome samples that are on the JGI IMG data portal. This includes legacy data as well as all samples that are part of the JGI CSP award, which covers deep sequencing and analysis by JGI of all NEON metagenome samples collected in 2023 and 2024. Included in the table are several fields with JGI metadata and statistics for each sample, such as Sequencing Method, GenomeSize, GeneCount, and number of bins (metaBATbinCount). Also included are some NEON variables such as siteID and collectDate, as well as multiple environmental terms assigned to each sample according to ENVO specifications (e.g. Ecosystem Category, Ecosystem Type, Specific Ecosystem). The table also has reference codes for the Genome Online Database (GOLD), including GOLD Analysis Project ID and GOLD Study ID; and the taxon OID (imgGenomeID) that allows accessing the sample on the JGI IMG data portal.\nThis table is available when you load the package phyloNEON.\n\n\nR code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(phyloNEON)\nlibrary(DT)\nlibrary(viridis)\n\n\nSave file to have version used\n\n\nR code\nwrite_csv(neon.metaDB, \"../data/NEON_metadata/neon.metaDB_20250701.csv\")\n\n\n\n\nR code\nneon.metaDB &lt;- read_csv(\"../data/NEON_metadata/neon.metaDB_20250701.csv\")\n\n\nTo view the table neon.metaDB (Note set eval = FALSE or do not include in your R code or you will get an error when rendering)\n\n\nR code\nView(neon.metaDB)\n\n\nTo view the structure of the neon.metaDB\n\n\nR code\nstr(neon.metaDB)\n\n\ntibble [1,834 × 32] (S3: tbl_df/tbl/data.frame)\n $ dnaSampleID               : chr [1:1834] \"ONAQ_044-M-20190619-COMP-DNA1\" \"PUUM_031-O-20210104-COMP-DNA1\" \"TECR.20230821.EPILITHON.8.DNA-DNA1\" \"KONZ_003-M-20170710-COMP-DNA1\" ...\n $ imgGenomeID               : num [1:1834] 3.3e+09 3.3e+09 3.3e+09 3.3e+09 3.3e+09 ...\n $ jgiProjectID              : num [1:1834] NA NA 1506438 0 1500369 ...\n $ ITS Proposal ID           : num [1:1834] NA NA 509938 NA 509938 ...\n $ Sequencing Status         : chr [1:1834] \"Permanent Draft\" \"Permanent Draft\" \"Permanent Draft\" \"Permanent Draft\" ...\n $ Study Name                : chr [1:1834] \"Terrestrial soil microbial communities from various locations - NEON\" \"Terrestrial soil microbial communities from various locations - NEON\" \"Soil and water microbial communities from various NEON Field Sites across the United States\" \"Terrestrial soil microbial communities from various locations - NEON\" ...\n $ GenomeName                : chr [1:1834] \"Terrestrial soil microbial communities from Onaqui, Utah, USA - ONAQ_044-M-20190619-COMP-DNA1\" \"Terrestrial soil microbial communities from Puu Makaala Natural Area Reserve, Hawaii, USA - PUUM_031-O-20210104-COMP-DNA1\" \"Freshwater microbial communities from Teakettle 2 Creek NEON Field Site, Sierra National Forest, CA, USA - TECR\"| __truncated__ \"Terrestrial soil microbial communities from Konza Prairie Biological Station, Prairie Peninsula, KS, USA - KONZ\"| __truncated__ ...\n $ Sequencing Center         : chr [1:1834] \"Battelle Memorial Institute\" \"Battelle Memorial Institute\" \"DOE Joint Genome Institute  (JGI)\" \"Battelle Memorial Institute\" ...\n $ GOLD Analysis Project ID  : chr [1:1834] \"Ga0620072\" \"Ga0619546\" \"Ga0672972\" \"Ga0428256\" ...\n $ GOLD Analysis Project Type: chr [1:1834] \"Metagenome Analysis\" \"Metagenome Analysis\" \"Metagenome Analysis\" \"Metagenome Analysis\" ...\n $ GOLD Sequencing Project ID: chr [1:1834] \"Gp0766640\" \"Gp0766114\" \"Gp0812633\" \"Gp0476824\" ...\n $ GOLD Study ID             : chr [1:1834] \"Gs0144570\" \"Gs0144570\" \"Gs0166454\" \"Gs0144570\" ...\n $ Funding Program           : chr [1:1834] NA NA \"CSP\" NA ...\n $ Sequencing Method         : chr [1:1834] \"Illumina NextSeq 550\" \"Illumina NextSeq 550\" \"Illumina NovaSeq X 10B\" \"Illumina NextSeq 550\" ...\n $ Sequencing Quality        : chr [1:1834] \"Level 1: Standard Draft\" \"Level 1: Standard Draft\" \"Level 1: Standard Draft\" \"Level 1: Standard Draft\" ...\n $ siteID                    : chr [1:1834] \"ONAQ\" \"PUUM\" \"TECR\" \"KONZ\" ...\n $ collectDate               : chr [1:1834] \"20190619\" \"20210104\" \"20230821\" \"20170710\" ...\n $ Ecosystem                 : chr [1:1834] \"Environmental\" \"Environmental\" \"Environmental\" \"Environmental\" ...\n $ Ecosystem Category        : chr [1:1834] \"Terrestrial\" \"Terrestrial\" \"Aquatic\" \"Terrestrial\" ...\n $ Ecosystem Subtype         : chr [1:1834] \"Unclassified\" \"Forest\" \"Creek\" \"Unclassified\" ...\n $ Ecosystem Type            : chr [1:1834] \"Soil\" \"Soil\" \"Freshwater\" \"Soil\" ...\n $ Specific Ecosystem        : chr [1:1834] \"Unclassified\" \"Unclassified\" \"Unclassified\" \"Unclassified\" ...\n $ GenomeSize                : num [1:1834] 3.57e+06 8.86e+04 8.33e+08 2.23e+07 7.62e+08 ...\n $ GeneCount                 : num [1:1834] 11955 313 1296700 63840 1113040 ...\n $ ScaffoldCount             : num [1:1834] 10633 277 671686 56485 522571 ...\n $ metaBATbinCount           : num [1:1834] 0 0 11 0 23 5 0 7 0 0 ...\n $ eukCCbinCount             : num [1:1834] 0 0 2 0 2 0 0 0 0 0 ...\n $ estNumberGenomes          : num [1:1834] 0 0 147 0 151 215 0 132 0 0 ...\n $ avgGenomeSize             : num [1:1834] 0 0 5663975 0 5046004 ...\n $ numberFilteredReads       : num [1:1834] 0.00 0.00 1.87e+08 0.00 3.15e+08 ...\n $ numberMappedReads         : num [1:1834] 0.00 0.00 1.03e+08 0.00 2.78e+08 ...\n $ pctAssembledReads         : num [1:1834] 0 0 55.4 0 88.2 ...\n\n\nConvert the collectDate from character to date format\n\n\nR code\nneon.metaDB.my &lt;- neon.metaDB\nneon.metaDB.my$collectDate &lt;- as.numeric(neon.metaDB.my$collectDate)\nneon.metaDB.my$collectDate &lt;- ymd(neon.metaDB.my$collectDate)\nstr(neon.metaDB.my$collectDate)\n\n\n Date[1:1834], format: \"2019-06-19\" \"2021-01-04\" \"2023-08-21\" \"2017-07-10\" \"2023-07-12\" ...\n\n\n\n\nTable of mean genome size per year\n\n\nR code\nneon.metaDB.my |&gt; \n  filter(`GOLD Analysis Project Type` != \"Combined Assembly\") |&gt; \n  group_by(year = lubridate::year(collectDate)) |&gt; \n  summarize(mean_GenomeSize = mean(GenomeSize))\n\n\n# A tibble: 9 × 2\n   year mean_GenomeSize\n  &lt;dbl&gt;           &lt;dbl&gt;\n1  2013        8635723.\n2  2014        7670580.\n3  2016       16242281.\n4  2017       17172060.\n5  2018       14100181.\n6  2019       16735397.\n7  2020       37402350.\n8  2021     1334660078.\n9  2023     1310246892.\n\n\n\n\nTable HARV mean genome size per year\n\n\nR code\nneon.metaDB.my |&gt; \n  filter(siteID == \"HARV\") |&gt; \n  group_by(year = lubridate::year(collectDate)) |&gt; \n  summarize(mean_GenomeSize = mean(GenomeSize))\n\n\n# A tibble: 6 × 2\n   year mean_GenomeSize\n  &lt;dbl&gt;           &lt;dbl&gt;\n1  2013        4721159.\n2  2016       17096867.\n3  2017       29942423.\n4  2019       13990976.\n5  2020       30567005.\n6  2023     2236533461.\n\n\n\n\nPlot of genome size per year\n\n\nR code\nneon.metaDB.my |&gt; \n  ggplot(aes(x=collectDate, y = GenomeSize)) +\n  geom_col(colour = \"maroon\", fill = \"maroon\") +\n  coord_flip()\n\n\n\n\n\n\n\n\n\nTo reformat dnasampleID column for terrestrial samples (This does not work for the aquatic samples)\n\n\nR code\nneon.metaDB.my.soil &lt;- neon.metaDB.my |&gt; \n  filter(`Ecosystem Category` == \"Terrestrial\") |&gt; \n  filter(`GOLD Analysis Project Type` != \"Combined Assembly\") |&gt; \n  \n  separate(`dnaSampleID`, c(\"dnaSampleID.site\",\"dnaSampleID.sub\"), \"_\", remove=FALSE) |&gt; \n  \n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-COMP\", \"_COMP\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-GEN\", \"_GEN\") |&gt;\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.sub\",\"dnaSampleID.type\"), \"_\") |&gt; \n  \n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-M\", \"_M\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-O\", \"_O\") |&gt;\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.plot\",\"dnaSampleID.sub\"), \"_\") |&gt; \n  \n  mutate_at(\"dnaSampleID.sub\", str_replace, \"M-\", \"M_\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"O-\", \"O_\") |&gt;\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.layer\",\"dnaSampleID.sub\"), \"_\") |&gt; \n\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-201\", \"201\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-202\", \"202\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"201\", \"_201\") |&gt;\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"202\", \"_202\") |&gt;\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.subplot\",\"dnaSampleID.date\"), \"_\") |&gt; \n\n  unite(plotID, c(dnaSampleID.site, dnaSampleID.plot), sep='_', remove=FALSE)\n\nneon.metaDB.my.soil$dnaSampleID.data &lt;- as.numeric(neon.metaDB.my.soil$dnaSampleID.date)\nneon.metaDB.my.soil$dnaSampleID.date &lt;- ymd(neon.metaDB.my.soil$dnaSampleID.date)\n\n\nTo reformat dnasampleID column for aquatic samples\n\n\nR code\nneon.metaDB.my.aquatic &lt;- neon.metaDB.my |&gt; \n  filter(`Ecosystem Category` == \"Aquatic\") |&gt; \n  filter(`GOLD Analysis Project Type` != \"Combined Assembly\") |&gt; \n  \n  mutate(dnaSampleID.sub = dnaSampleID) |&gt; \n  mutate_at(\"dnaSampleID.sub\", str_replace, \".202\", \"_202\") |&gt;\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.site\",\"dnaSampleID.sub\"), \"_\") |&gt; \n  separate(`dnaSampleID.site`, c(\"dnaSampleID.site\",\"dnaSampleID.code\"), \"\\\\.\") |&gt; \n  \n  mutate_at(\"dnaSampleID.sub\", str_replace, \".DNA\", \"_DNA\") |&gt;\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.sub\",\"dnaSampleID.type\"), \"_\") |&gt; \n  \n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.data\",\"dnaSampleID.niche\", \"dnaSampleID.num\"), \"\\\\.\") |&gt; \n\n  unite(dnaSampleID.niche, c(dnaSampleID.code, dnaSampleID.niche)) |&gt; \n  mutate_at(\"dnaSampleID.niche\", str_replace, \"NA_\", \"\") |&gt;\n  mutate_at(\"dnaSampleID.niche\", str_replace, \"_NA\", \"\")\n\n\n\n\nHARV metagenomes by year and plot\n\n\nR code\ndatatable(\nneon.metaDB.my.soil |&gt; \n  filter(siteID == \"HARV\") |&gt; \n  group_by(Year = lubridate::year(collectDate), dnaSampleID.plot) |&gt; \n  count() |&gt; \n  pivot_wider(names_from = dnaSampleID.plot, values_from = n) |&gt; \n  mutate_all(funs(replace_na(.,0)))\n)\n\n\n\n\n\n\n\n\nWREF metagenomes by year and plot\n\n\nR code\ndatatable(\nneon.metaDB.my.soil |&gt; \n  filter(siteID == \"WREF\") |&gt; \n  group_by(Year = lubridate::year(collectDate), dnaSampleID.plot) |&gt; \n  count() |&gt; \n  pivot_wider(names_from = dnaSampleID.plot, values_from = n) |&gt; \n  mutate_all(funs(replace_na(.,0)))\n)\n\n\n\n\n\n\n\n\nPlot of HARV samples per plot per year\n\n\nR code\nneon.metaDB.my.soil |&gt; \n  filter(siteID == \"HARV\") |&gt; \n  group_by(Year = lubridate::year(collectDate), dnaSampleID.plot) |&gt; \n  count() |&gt; \n  pivot_wider(names_from = dnaSampleID.plot, values_from = n) |&gt; \n  mutate_all(funs(replace_na(.,0))) |&gt; \n  pivot_longer(!Year, names_to = \"plot\", values_to = \"metagenomes\") |&gt; \n  ggplot(aes(x=Year, y = plot)) +\n  geom_tile(aes(fill = metagenomes)) +\n  scale_fill_viridis(discrete=FALSE, direction = -1) +\n  scale_x_continuous(breaks = seq(2013, 2023, by = 1)) \n\n\n\n\n\n\n\n\n\n\n\nMissing years at HARV\nAll sites are missing data from 2021 and 2022. That should be in IMG soon. What about 2018?\n\nCollectDate\n\n\nR code\nneon.metaDB.my.soil |&gt; \n  group_by(Year = lubridate::year(collectDate)) |&gt; \n  count() \n\n\n# A tibble: 9 × 2\n# Groups:   Year [9]\n   Year     n\n  &lt;dbl&gt; &lt;int&gt;\n1  2013    63\n2  2014   106\n3  2016   229\n4  2017   326\n5  2018    45\n6  2019   231\n7  2020   185\n8  2021   117\n9  2023   303\n\n\n\n\ndnaSampleID.date\n\n\nR code\nneon.metaDB.my.soil |&gt; \n  group_by(Year = lubridate::year(dnaSampleID.date)) |&gt; \n  count() \n\n\n# A tibble: 9 × 2\n# Groups:   Year [9]\n   Year     n\n  &lt;dbl&gt; &lt;int&gt;\n1  2013    63\n2  2014   106\n3  2016   229\n4  2017   326\n5  2018    45\n6  2019   231\n7  2020   185\n8  2021   117\n9  2023   303\n\n\n\n\n\nPlot of samples per plot per year at all sites\n\n\nR code\nneon.metaDB.my.soil |&gt; \n  group_by(siteID, Year = lubridate::year(collectDate), dnaSampleID.plot) |&gt; \n  count() |&gt; \n  pivot_wider(names_from = Year, values_from = n) |&gt; \n  mutate_all(funs(replace_na(.,0))) |&gt; \n  pivot_longer(!c(siteID, dnaSampleID.plot), names_to = \"Year\", values_to = \"metagenomes\") |&gt; \n  ggplot(aes(x=Year, y = dnaSampleID.plot)) +\n  geom_tile(aes(fill = metagenomes)) +\n  scale_fill_viridis(discrete=FALSE, direction = -1) +\n # scale_x_continuous(breaks = seq(2013, 2023, by = 1)) +\n  facet_wrap(~siteID, scales =\"free_y\", ncol = 3) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))"
  },
  {
    "objectID": "methods/Access_NEON_Data_for_Metagenomics.html",
    "href": "methods/Access_NEON_Data_for_Metagenomics.html",
    "title": "Access NEON Data for Metagenomics",
    "section": "",
    "text": "From Access NEON Data for Metagenomics See * Update on the changing NEON microbial data * Soil microbe metagenome sequences\nInstall packages\n\n\nR code\ninstall.packages(\"neonUtilities\")\ninstall.packages(\"neonOS\")\n\n\n\n\nR code\nlibrary(neonUtilities)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(viridis)\n\n\n\n\nR code\nsoilTrialSites = c(\"BONA\",\"DEJU\",\"HEAL\",\"TOOL\",\"BARR\")\nsoilTrialSites = c(\"HARV\")\n\n\nsoilChem <- loadByProduct(\n  dpID='DP1.10086.001',\n  startdate = \"2017-01\",\n  enddate = \"2019-12\",\n  check.size = FALSE,\n  site = soilTrialSites,\n  package='expanded')\n\n\n\n\nR code\nView(soilChem$sls_metagenomicsPooling)\n\n\n\n\nR code\nmetaGdata <- loadByProduct(dpID = 'DP1.10107.001',\n\n                          check.size = FALSE,\n\n                          package = 'expanded') \n\n\n\n\nR code\nmetaGdata_mms_metagenomeSequencing <- metaGdata$mms_metagenomeSequencing\nwrite_csv(metaGdata_mms_metagenomeSequencing, \"../data/NEON_metadata/metaGdata_mms_metagenomeSequencing.csv\")\n\n\nThis has HARV data collected up until 2022. The 2021 and 2022 data are not in the phyloNEON data.\n\n\nR code\nmetaGdata_mms_metagenomeSequencing_HARV <- metaGdata$mms_metagenomeSequencing |> \nwrite_csv(metaGdata_mms_metagenomeSequencing_HARV, \"../data/NEON_metadata/metaGdata_mms_metagenomeSequencing_HARV.csv\")\n\n\n\nRead in saved file (all above in eval = FALSE)\n\n\nR code\nmetaGdata_mms_metagenomeSequencing <- read_csv(\"../data/NEON_metadata/metaGdata_mms_metagenomeSequencing.csv\")\n\n\n\n\nCreate data frame from dnaSampleID\n\n\nR code\nmetaGdata_dnaSampleID <- metaGdata_mms_metagenomeSequencing |> \n  select(dnaSampleID)\n\n# Read in 2023 and 2024 data that is not in the metagenome data product yet\n\nneon_ay23_jgi_samples <- read_csv(\"../data/NEON_metadata/neon_ay23_jgi_samples_soil.csv\")\nneon_ay24_jgi_samples <- read_csv(\"../data/NEON_metadata/neon_ay24_jgi_samples_soil.csv\")\n\nneon_dnaSampleID <- rbind(metaGdata_dnaSampleID, neon_ay23_jgi_samples, neon_ay24_jgi_samples)\n\n\n\nFull table\n\n\nR code\nneon_dnaSampleID_split <- neon_dnaSampleID |> \n\n  separate(`dnaSampleID`, c(\"dnaSampleID.site\",\"dnaSampleID.sub\"), \"_\", remove=FALSE) |> \n  \n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-comp\", \"_COMP\") |>\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-COMP\", \"_COMP\") |>\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-GEN\", \"_GEN\") |>\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.sub\",\"dnaSampleID.type\"), \"_\") |> \n  \n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-M\", \"_M\") |>\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-O\", \"_O\") |>\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.plot\",\"dnaSampleID.sub\"), \"_\") |> \n  \n  mutate_at(\"dnaSampleID.sub\", str_replace, \"M-\", \"M_\") |>\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"O-\", \"O_\") |>\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.layer\",\"dnaSampleID.sub\"), \"_\") |> \n\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-201\", \"201\") |>\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-202\", \"202\") |>\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"2013\", \"_2013\") |>\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"2014\", \"_2014\") |>\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"2015\", \"_2015\") |>\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"2016\", \"_2016\") |>\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"2017\", \"_2017\") |>\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"2018\", \"_2018\") |>\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"2019\", \"_2019\") |>\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"2020\", \"_2020\") |>\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"2021\", \"_2021\") |>\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"2022\", \"_2022\") |>\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"2023\", \"_2023\") |>\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"2024\", \"_2024\") |>\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.subplot\",\"dnaSampleID.date\"), \"_\") |> \n\n  unite(plotID, c(dnaSampleID.site, dnaSampleID.plot), sep='_', remove=FALSE)\n\nneon_dnaSampleID_split$dnaSampleID.date <- as.numeric(neon_dnaSampleID_split$dnaSampleID.date)\nneon_dnaSampleID_split$dnaSampleID.date <- ymd(neon_dnaSampleID_split$dnaSampleID.date)\n\n\n\n\nFull table\n\n\nR code\nmetaGdata_mms_metagenomeSequencing <- metaGdata_mms_metagenomeSequencing |> \n\n  separate(`dnaSampleID`, c(\"dnaSampleID.site\",\"dnaSampleID.sub\"), \"_\", remove=FALSE) |> \n  \n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-comp\", \"_COMP\") |>\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-COMP\", \"_COMP\") |>\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-GEN\", \"_GEN\") |>\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.sub\",\"dnaSampleID.type\"), \"_\") |> \n  \n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-M\", \"_M\") |>\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-O\", \"_O\") |>\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.plot\",\"dnaSampleID.sub\"), \"_\") |> \n  \n  mutate_at(\"dnaSampleID.sub\", str_replace, \"M-\", \"M_\") |>\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"O-\", \"O_\") |>\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.layer\",\"dnaSampleID.sub\"), \"_\") |> \n\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-201\", \"201\") |>\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"-202\", \"202\") |>\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"201\", \"_201\") |>\n  mutate_at(\"dnaSampleID.sub\", str_replace, \"202\", \"_202\") |>\n  separate(`dnaSampleID.sub`, c(\"dnaSampleID.subplot\",\"dnaSampleID.date\"), \"_\") |> \n\n  unite(plotID, c(dnaSampleID.site, dnaSampleID.plot), sep='_', remove=FALSE)\n\nmetaGdata_mms_metagenomeSequencing$dnaSampleID.data <- as.numeric(metaGdata_mms_metagenomeSequencing$dnaSampleID.date)\nmetaGdata_mms_metagenomeSequencing$dnaSampleID.date <- ymd(metaGdata_mms_metagenomeSequencing$dnaSampleID.date)\n\n\n\n\n\nPlot of HARV samples per plot per year\n\n\nR code\nneon_dnaSampleID_split |> \n  filter(dnaSampleID.site == \"HARV\") |> \n  group_by(Year = lubridate::year(dnaSampleID.date), dnaSampleID.plot) |> \n  count() |> \n  pivot_wider(names_from = dnaSampleID.plot, values_from = n) |> \n  mutate_all(funs(replace_na(.,0))) |> \n  pivot_longer(!Year, names_to = \"plot\", values_to = \"metagenomes\") |> \n  ggplot(aes(x=Year, y = plot)) +\n  geom_tile(aes(fill = metagenomes)) +\n  scale_fill_viridis(discrete=FALSE, direction = -1) +\n  scale_x_continuous(breaks = seq(2013, 2024, by = 1)) \n\n\n\n\n\n\n\nPlot of samples per plot per year at all sites\n\n\nR code\nneon_dnaSampleID_split |> \n  group_by(dnaSampleID.site, Year = lubridate::year(dnaSampleID.date), dnaSampleID.plot) |> \n  count() |> \n  pivot_wider(names_from = Year, values_from = n) |> \n  mutate_all(funs(replace_na(.,0))) |> \n  pivot_longer(!c(dnaSampleID.site, dnaSampleID.plot), names_to = \"Year\", values_to = \"metagenomes\") |> \n  ggplot(aes(x=Year, y = dnaSampleID.plot)) +\n  geom_tile(aes(fill = metagenomes)) +\n  scale_fill_viridis(discrete=FALSE, direction = -1) +\n  facet_wrap(~dnaSampleID.site, scales =\"free_y\", ncol = 3) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) \n\n\n\n\n\n\n\nTable of mean sampleFilteredReadNumber\nAbout 3-5x increase in reads in 2021 and 2022 About 10-20x increase in 2023 over 2021 and 2022\n\n\nR code\nmetaGdata_mms_metagenomeSequencing |> \n  filter(dnaSampleID.site == \"HARV\") |> \n  group_by(Year = lubridate::year(collectDate)) |> \n  summarize(mean_sampleFilteredReadNumber = mean(sampleFilteredReadNumber))\n\n\n# A tibble: 8 × 2\n   Year mean_sampleFilteredReadNumber\n  <dbl>                         <dbl>\n1  2013                           NA \n2  2016                      6643162.\n3  2017                      5038437 \n4  2018                      2633512.\n5  2019                      4560748.\n6  2020                      3468523.\n7  2021                     16005210.\n8  2022                     19339618.\n\n\n\n\nTable of mean sampleTotalReadNumber\nAbout 3-5x increase in reads in 2021 and 2022 About 10-20x increase in 2023 over 2021 and 2022\n\n\nR code\nmetaGdata_mms_metagenomeSequencing |> \n  filter(dnaSampleID.site == \"HARV\") |> \n  group_by(Year = lubridate::year(collectDate)) |> \n  summarize(mean_sampleTotalReadNumber = mean(sampleTotalReadNumber))\n\n\n# A tibble: 8 × 2\n   Year mean_sampleTotalReadNumber\n  <dbl>                      <dbl>\n1  2013                   9270681.\n2  2016                  10802077.\n3  2017                  11456008.\n4  2018                   6664285.\n5  2019                   5442366.\n6  2020                   4674903 \n7  2021                  27270286.\n8  2022                  31374636."
  },
  {
    "objectID": "labs/lab1_overview.html",
    "href": "labs/lab1_overview.html",
    "title": "Lab 1 - Overview & Getting Started",
    "section": "",
    "text": "What is bioinformatics?\nWhat is reproducible research?\nWhy learn bioinformatics and data science skills?\nHigh Performance Computing (Unity)\nOverview of the R statistical programming language\n\n\n\n\nIn recent years, the field of genomic analysis and bioinformatics has sifted towards requiring some knowledge of R, Python/Perl/C and the use of high performance computers (often requiring some fundamental Unix skills) available at national computing centers for working with large data sets. While there are many great software packages available for particular computational problems in evolutionary biology, many software programs do not have a user interface (e.g. drop down menus and such) and are run in command line mode. The lab sessions in this course have been designed to give students an introduction to working with R and packages used for genome and metagenome analyses. We are using recently release National Ecological Observatory Network data to design a course-based Undergraduate Research Experience (CURE). The first 4 weeks we will discuss to project space, discuss research ideas and formulate testable hypothesis or discovery driven approaches, design the experimental approaches. For a preview today I will give an overview of the project space.\n\n\n\nBioinformatics is the field of science in which biology, computer science, statistics and information technology merge into a single discipline. There are three important sub-disciplines within bioinformatics:\n\nThe development of new algorithms and statistics with which to assess relationships among members of large data sets.\nThe development and implementation of tools that enable efficient access and management of different types of information.\nThe analysis and interpretation of various types of data including nucleotide and amino acid sequences, protein domains, and protein structures.\n\n\n\n\nis a term coined in response to the high demand of techniques and resources for handling the explosion of molecular data.\nis a buzzword to describe a growing field.\nbenefits from the physicists, chemists and mathematicians crossing over into biology.\nis a collection of tools.\nis way of thinking about a problem!\n\n\n\n\nIn order to make new algorithms and data sources available to biologists someone needs to write applications that include these algorithms and create new databases. Often this is first done by academic research groups. Later redone by private companies when market is large and profitable enough. There is a large gap between what is done by research groups and companies. Sometimes this is filled by large government funded projects, but not usually in time for most researchers. This is why bioinformatics and programming skills have become very valuable.\n\n\n\n\nThe field of data science has grown tremendously over the last decade and the two programming languages, R and Python, used in analyzing genomic data are the most popular languages for data science. This made it easy to transfer bioinformatics skills to diverse fields.\nHere a few links that I will go over in lab:\n\nWhat is Data Science? \n\nWhat is Data Science? 8 Skills That Will Get You Hired\n\nOpen Science is Kinder Science\n\nData Carpentry\n\nBuilding a local community of practice in scientific programming for life scientists\n\n\n\n\nR is the largest and most comprehensive public domain statistical computing environment. The core R package is enhanced by several hundred user-supplied add-on packages, including many for gene expression analysis, in the  Comprehensive R Archive Network (CRAN). Omegahat Project for Statistical Computing.  BioConductor is an open source and open development software project for the analysis and comprehension of genomic data and is based primarily on the R programming language. R and Bioconductor are free, Open Source and available for Windows, MacOS and a wide variety of UNIX platforms.\n\n\nReproducibility is the hallmark of science, which is based on empirical observations coupled with explanatory models. While reproducibility encompasses the full science lifecycle, and includes issues such as methodological consistency and treatment of bias, in this course we will focus on computational reproducibility: the ability to document data, analyses, and models sufficiently for other researchers to be able to understand and ideally re-execute the computations that lead to scientific results and conclusions. With current publishing practices, this can be difficult because data are typically unavailable, the method sections of papers do not detail the computational approaches used, and analyses and models are often conducted in graphical programs, or, when scripted analyses are employed, the code is not available. In this course we will learn how to write code that is integrated into reproducible reports.\n\n\n\nMany introductory and advance tutorials have been developed for R. Here are a few\n\nThe offical R manuals\nCRAN’s Introduction to R\nR for Data Science by Garrett Grolemund and Hadley Wickham\nR Graphics Cookbook by Winston Chang\nData Carpentries Genomic Workshop Sessions\nData Analysis and Visualization in R for Ecologists\n\nThere are also many workshops and online R courses that you could take to follow up what you learn in this class.\n\n\n\n\nGitHub has become a popular way to manage, share and view code for open source projects. The tutorials created for this course will be written in Quarto and posted on GitHub. Thus, you will be able to continue to see course materials after the end of the semester. You will use and make a GitHub web site for your research project.\n\n\n\nYou all are part of the first generation of generative AI users. The saying goes “AI won’t take your job, but someone using who knows how to use AI might.” Think of AI as a force multiplier. You have to learn to code and clearly state your problems before AI can to help you. This class will fully use generative AI in hopes that it will challenge us to think more creatively about problems and not stress out about syntax. Think first…ask questions…code…solve problem! We will use the UMass version of Microsoft Copilot Chat and Copilot integrated into RStudio. We will embrace “Vibe coding” (see What is Vibe Coding, Exactly?) and ride the waves.\n\n\n\nIn this course we will learn to write basic unix commands, run bioinformatics software from the command line and allocate computer resources for submitting large jobs. UMass has modern High Performance Computing system, Unity, and excellent staff members to help get you going and trouble shoot issues. Working on HPCs has become much easier with the advent of web interfaces that look and work much like software running on your computer.\n\n\n\nYou can do all of the R-based labs on your own computer. Follow these directions by the makers of RStudio, Posit. You will need to download the lab files from Unity or the course GitHub site."
  },
  {
    "objectID": "labs/lab_AI_R.html",
    "href": "labs/lab_AI_R.html",
    "title": "Learning R with the help of AI tools",
    "section": "",
    "text": "AI won’t take your job, but someone using who knows how to use AI might. Think of AI as a force multiplier. You have to learn to code first before you can use AI to help you. Google recently reported that about 25% of its new code is AI-generated.\nMicrosoft designed Copilot to work off of the latest version of OpenAI’s GPT model, GPT-4,\n\ngithub co-pilot - https://github.com/copilot\ngithub education - https://github.com/education\nRStudio github copilot - https://docs.posit.co/ide/user/ide/guide/tools/copilot.html\nMicrosoft Introduction to GitHub Copilot - https://learn.microsoft.com/en-us/training/modules/introduction-to-github-copilot/\nIntroduction to GitHub CoPilot videos - https://learn.microsoft.com/en-us/shows/introduction-to-github-copilot/\nSet up co-pilot for learning - https://docs.github.com/en/get-started/learning-to-code/setting-up-copilot-for-learning-to-code\nHow to write better prompts for GitHub Copilot - https://github.blog/developer-skills/github/how-to-write-better-prompts-for-github-copilot/\nAI Assisted Coding in RStudio - https://research-it.manchester.ac.uk/news/2024/11/29/ai-assisted-coding-in-rstudio/\nRTutor AI - https://rtutor.ai/\nSyntha AI - https://syntha.ai/code-generators/r\nR Code Generator - https://codingfleet.com/code-generator/r/\nLearning the tidyverse with the help of AI tools - https://www.tidyverse.org/blog/2025/04/learn-tidyverse-ai/\nIntroducing vitals, a toolkit for evaluating LLM products in R - https://www.tidyverse.org/blog/2025/06/vitals-0-1-0/\nUsing AI with R - https://rfortherestofus.com/courses/ai\nBoost Your R Skills with AI - https://artscience.ai/boost-your-r-skills-with-ai/\nBeginner’s Tutorial for the OpenAI API in R - https://tilburg.ai/2024/03/tutorial-openai-api-in-r/\nCodex - https://chatgpt.com/codex\n\nAI Assisted Coding in RStudio - https://research-it.manchester.ac.uk/news/2024/11/29/ai-assisted-coding-in-rstudio/ Integrating OpenAI’s ChatGPT into RStudio is now possible with “Chattr”, “GPT Studio” and “GitHub Copilot”. These new tools will help you find the right functions and commands and to quickly generate code snippets to save you time.\n8 ChatGPT packages for R - https://www.infoworld.com/article/2338386/8-chatgpt-tools-for-r-programming.html\n\nVibe coding https://www.geeksforgeeks.org/techtips/what-is-vibe-coding/\nWhat is vibe coding, exactly? - https://www.technologyreview.com/2025/04/16/1115135/what-is-vibe-coding-exactly/\nWhat Is Vibe Coding? Definition, Tools, Pros, and Cons - https://www.datacamp.com/blog/vibe-coding\nYou can use GitHub and Git to collaborate on work. https://docs.github.com/en/get-started/start-your-journey/about-github-and-git\nUsing co-pilot as your tutor - https://docs.github.com/en/get-started/learning-to-code/setting-up-copilot-for-learning-to-code\nchattr - https://mlverse.github.io/chattr/\n\nBioinformatics and AI * A data-intelligence-intensive bioinformatics copilot system for large-scale omics research and scientific insights - https://academic.oup.com/bib/article/26/4/bbaf312/8196318?login=true * Bioinformatics AI: Driving Future Biological Breakthroughs - https://biologyinsights.com/bioinformatics-ai-driving-future-biological-breakthroughs/"
  },
  {
    "objectID": "labs/lab1_rstudio.html",
    "href": "labs/lab1_rstudio.html",
    "title": "Lab 1 : Introduction to R and Reproducible Research",
    "section": "",
    "text": "The RStudio Integrated Development Environment\nThe Quarto scientific publishing system\nWorking in R coding chunks in Quarto\nReading error messages\n\n\n\nThe most popular way to write R programs and to interactively run code and create graphs is using the RStudio Integrated Devopement Environment (IDE). It is open source software that is available for free. There are other ways to write and run R code, such as using text editors, VS Code editors, Neovim or Jupyter Notebooks, but we will focus on RStudio in this class.\n\n\nI run R and RStudio on my computer. You can too. Most of the basics R labs for the class you should be able to do from your laptop.\n\nInstall the latest release (2025-06-13, Great Square Root) R-4.5.1 of R from CRAN and follow the installation instructions. If you have an older verion of R on your computer please update to this release as we can’t guarantee the labs will work on older versions.\nInstall R Studio, a nice graphical interface for working with R.\nOpen RStudio and install tidyverse under Tools &gt; Install Packages. You will need to install other packages as well for this and future labs."
  },
  {
    "objectID": "labs/lab1_rstudio.html#learning-objectives",
    "href": "labs/lab1_rstudio.html#learning-objectives",
    "title": "Lab 1 : Introduction to R and Reproducible Research",
    "section": "",
    "text": "The RStudio Integrated Development Environment\nThe Quarto scientific publishing system\nWorking in R coding chunks in Quarto\nReading error messages\n\n\n\nThe most popular way to write R programs and to interactively run code and create graphs is using the RStudio Integrated Devopement Environment (IDE). It is open source software that is available for free. There are other ways to write and run R code, such as using text editors, VS Code editors, Neovim or Jupyter Notebooks, but we will focus on RStudio in this class.\n\n\nI run R and RStudio on my computer. You can too. Most of the basics R labs for the class you should be able to do from your laptop.\n\nInstall the latest release (2025-06-13, Great Square Root) R-4.5.1 of R from CRAN and follow the installation instructions. If you have an older verion of R on your computer please update to this release as we can’t guarantee the labs will work on older versions.\nInstall R Studio, a nice graphical interface for working with R.\nOpen RStudio and install tidyverse under Tools &gt; Install Packages. You will need to install other packages as well for this and future labs."
  },
  {
    "objectID": "labs/lab1_rstudio.html#overview",
    "href": "labs/lab1_rstudio.html#overview",
    "title": "Lab 1 : Introduction to R and Reproducible Research",
    "section": "Overview",
    "text": "Overview\nIn recent years, the field of genomic analysis has sifted towards requiring some knowledge of R, Python/Perl/C and the use of high performance computers (often requiring some fundamental Unix skills) available at national computing centers for working with large data sets. While there are many great software packages available for particular computational problems in evolutionary biology, many software programs do not have a user interface (e.g. drop down menus and such) and are run in command line mode. The lab sessions in this course have been designed to give students an introduction to working with R and packages used for Human Genome Analysis.\nThe lab course is divided into 3 parts\n\nIntroduction to R and the tidyverse\nGene Expression Analysis\nAnalysis of SNPs and your genetic data"
  },
  {
    "objectID": "labs/lab1_rstudio.html#reproducible-research",
    "href": "labs/lab1_rstudio.html#reproducible-research",
    "title": "Lab 1 : Introduction to R and Reproducible Research",
    "section": "Reproducible Research",
    "text": "Reproducible Research\nReproducibility is the hallmark of science, which is based on empirical observations coupled with explanatory models. While reproducibility encompasses the full science lifecycle, and includes issues such as methodological consistency and treatment of bias, in this course we will focus on computational reproducibility: the ability to document data, analyses, and models sufficiently for other researchers to be able to understand and ideally re-execute the computations that lead to scientific results and conclusions. With current publishing practices, this can be difficult because data are typically unavailable, the method sections of papers do not detail the computational approaches used, and analyses and models are often conducted in graphical programs, or, when scripted analyses are employed, the code is not available. In this course we will learn how to write code that is integrated into reproducible reports."
  },
  {
    "objectID": "labs/lab1_rstudio.html#data-science",
    "href": "labs/lab1_rstudio.html#data-science",
    "title": "Lab 1 : Introduction to R and Reproducible Research",
    "section": "Data Science",
    "text": "Data Science\nHere a few links that I will go over in lab:\n\nWhat is Data Science? \n\nWhat is Data Science? 8 Skills That Will Get You Hired\n\nOpen Science is Kinder Science\n\nData Carpentry\n\nBuilding a local community of practice in scientific programming for life scientists"
  },
  {
    "objectID": "labs/lab1_rstudio.html#r",
    "href": "labs/lab1_rstudio.html#r",
    "title": "Lab 1 : Introduction to R and Reproducible Research",
    "section": "R",
    "text": "R\nR is the largest and most comprehensive public domain statistical computing environment. The core R package is enhanced by several hundred user-supplied add-on packages, including many for gene expression analysis, in the  Comprehensive R Archive Network (CRAN). Omegahat Project for Statistical Computing.  BioConductor is an open source and open development software project for the analysis and comprehension of genomic data and is based primarily on the R programming language. R and Bioconductor are free, Open Source and available for Windows, MacOS and a wide variety of UNIX platforms.\n\nThe RStudio Integrated Devopement Environment (IDE)\nThe most popular way to write R programs and to interactively run code and create graphs is using the RStudio Integrated Devopement Environment (IDE). It is open source software that is available for free. There are other ways to write and run R code, such as using text editors, VS Code editors, Neovim or Jupyter Notebooks, but we will focus on RStudio in this class.\n\n\nR manuals, help and tutorials\nMany introductory and advance tutorials have been developed for R. Here are a few\n\nThe offical R manuals\nCRAN’s Introduction to R\nR for Data Science by Garrett Grolemund and Hadley Wickham\nR Graphics Cookbook by Winston Chang\nData Carpentries Genomic Workshop Sessions\nData Analysis and Visualization in R for Ecologists\n\nThere are also many workshops and online R courses that you could take to follow up what you learn in this class."
  },
  {
    "objectID": "labs/lab1_rstudio.html#on-the-computer",
    "href": "labs/lab1_rstudio.html#on-the-computer",
    "title": "Lab 1 : Introduction to R and Reproducible Research",
    "section": "On the Computer",
    "text": "On the Computer\n\nGetting started on Unity\nUnity https://unity.rc.umass.edu/ is the UMass High Performance Computing cluster. We will be running bioinformatics software and using R and RStudio from the Unity HPC.\n\n\nWorking in RStudio\nThe default R studio appearance includes 4 windows.\n\nThe R script(s) and data view (upper left window).\nConsole (bottom left window).\nWorkspace and history (upper right window).\nFiles, plots, packages and help (botton right window).\n\n\n\n\nRStudio Screenshot\n\n\n\nThe R script(s) and data view window (upper left window)\nIn this window you can type directly into a file, run code and save the file for reuse. In this class we will work with Quarto files (discussed below).\n\n\nConsole Window (bottom left window)\nThe console is where you can type R commands and see output.\nType\n3 + 3\nTo better document and save your code write it in the Quarto documents rather than the console. On occasion we will use the console to access documentation and for other purposes.\n\n\nEnvironment and History tabs (upper right window)\nThe Environment tab shows all the active objects. If you have a data frame loaded, then click on the object will enable you to view the table. The History tab shows a list of commands used so far.\n\n\nFiles, Plots, Packages and Help (bottom right window)\nThere are data sets that come with the R package and used in tutorials. If you run the following command you will see a graph of related to the cars data set in the Plots window\n\n\n\nQuarto\nThe Quarto is a scientific publishing system. In this class we will use one of it’s simplest features, producing a report with the code and resulting output (graphs, tables, statistical analysis). Quarto can also be used to produce slides, web sites, scientific manuscripts and books. For example, and all the labs for this course and my research laboratory website were made using Quarto. Quarto wraps together many previous packages used for publishing with R.\nTo use Quarto with R, the rmarkdown R package is installed. There are some differences between a Quarto and R Markdown document, but overall they are very similar.\n\nProducing Lab Reports with Quarto\nIn RStudio select File &gt; New File &gt; Quarto Document. Add a title (e.g. Lab 1) and your name then create the document. Notice the your file says untitled with an asterisk. Save your file (e.g. lab1). This will automatically add the .qmd extension to your file (lab1.qmd). ALWAYS SAVE YOUR FILE BEFORE YOU START WORKING AND OFTEN WHILE WORKING.\nThe top section of the document delineated by the --- is called the YAML block. In this template it contains your the title, your name, the output type (html) and the editor preference (visual).You can also work with your file directly with the source code by clicking the source icon.\nThe following lines of code in your YAML block with generate a table of contents (toc) as shown at the top of this lab. The line with embed-resources creates a stand alone html file.\n---\ntitle: \"Lab 1\"\nauthor: \"Jeff Blanchard\"\nformat:\n  html:\n    toc: true\n    toc_float: true\n    embed-resources: true\neditor: visual\n---\nThe text with the white background is in rmarkdown. The icons in the same section as the visual icon you can easily made the text in bold or in italics, change the text from normal to a header, create bulleted or numbered lists, insert html links, add images, insert tables and more.\nThe text with the gray background is in R code chunks. Click on the green play icon in the top right corner of the code chunk to run the code.\nClick on the Render icon. This will run the code, show the output and create a html file that is automatically saved to your directory (look for the lab1.html file) and will automatically open this file in your browser.\nCreate new code chunk by clicking on the green +C icon to the right of the Render icon. In the code chunk type plot(cars). Then click the run the code to see a graph of the cars data set that comes preloaded into R.\n\n\nR code\nplot(cars)"
  },
  {
    "objectID": "labs/lab1_rstudio.html#writing-r-code",
    "href": "labs/lab1_rstudio.html#writing-r-code",
    "title": "Lab 1 : Introduction to R and Reproducible Research",
    "section": "Writing R code",
    "text": "Writing R code\n\nAssignment statements\nAll R statements where you create objects are called assignment statements and the form “object_name &lt;- value”\n\n\nR code\nx &lt;- 3\n\n\nSimply typing x will give the value of x\n\n\nR code\nx\n\n\n[1] 3\n\n\nYou will make lots of assignments and &lt;- is a pain to type. Instead, use RStudio’s keyboard shortcut: Alt + - (the minus sign). Notice that RStudio automagically surrounds &lt;- with spaces, which is a good code formatting practice. An equals sign = will work in place of &lt;-, but it will cause confusion later so keep to the convention of using &lt;- to make assignments\n\n\nObject Names\nObject names must start with a letter, and can only contain letters, numbers, underscores and periods. You want your object names to be descriptive, so you’ll need a convention for multiple words. I recommend snake_case where you separate lowercase words with an underscore. Note that R is case sensitive, e.g., object names gene, GENE, Gene are all different.\n\n\nR code\ngenome_size &lt;- 3100000000\n\n\nImportant note: since there are many built-in functions in R, make sure that the new object names you assign are not already used by the system. A simple way of checking this is to type in the name you want to use. If the system returns an error message telling you that such object is not found, it is safe to use the name.\n\n\nCharacters\nA character object is used to represent string values in R. It is defined by double quotes ““.\n\n\nR code\nDNA &lt;- \"ATGAAA\"\nDNA\n\n\n[1] \"ATGAAA\"\n\n\n\n\nVectors\nA vector is a sequence of data elements of the same basic type. data elements in a vector are officially called components. Assignment operator (&lt;-) stores the value (object) on the right side of (&lt;-) expression in the left side. Once assigned, the object can be used just as an ordinary component of the computation. The c function concanenates the components into a vector.\n\n\nR code\nrandom_numbers &lt;- c(1,10,100)    \nrandom_numbers\n\n\n[1]   1  10 100\n\n\nNow you can do scalar computations on a vector\n\n\nR code\nrandom_numbers * 2\n\n\n[1]   2  20 200\n\n\nor use sum, sort, min, max, length and many other operations. For example\n\n\nR code\nsort(random_numbers)\n\n\n[1]   1  10 100\n\n\nYou can also do vector arithmatic\n\n\nR code\nrandom_numbers &lt;- c(1,10,100) \ny&lt;- c(1,2,3) \nrandom_numbers * y\n\n\n[1]   1  20 300\n\n\nVectors can also be made of characters\n\n\nR code\ncodons&lt;- c(\"AUG\", \"UAU\", \"UGA\") \ncodons\n\n\n[1] \"AUG\" \"UAU\" \"UGA\""
  },
  {
    "objectID": "labs/lab1_rstudio.html#exercises",
    "href": "labs/lab1_rstudio.html#exercises",
    "title": "Lab 1 : Introduction to R and Reproducible Research",
    "section": "Exercises",
    "text": "Exercises\nYour lab report must have each exercise labeled with a header (e.g. ## Exercise 1) so that each one appears in the table of contents.\nYou will need to first export (download) the Lab1_yourname.html file to your computer, then upload the file to Canvas. In the bottom right corner click on the wheel icon then select Export.\n\nThe main goal for today’s lab is to create the lab report so we need a few exercises to fill it out\n\nExercise 1\nFor x = 2 and y = 15, compute the sum and difference of x and y\n\n\nExercise 2\nCreate a vector of the values 22, 62, 148, 43 and 129. Multiple the vector by 5.\n\n\nExercise 3\nCreate a vector of the nucleotides A, T, C and G. Remember to put a “” around each letter. Arrange the nucleotides alphabetically using the sort function sort(vector_name)"
  },
  {
    "objectID": "labs/lab1_overview.html#learning-objectives",
    "href": "labs/lab1_overview.html#learning-objectives",
    "title": "Lab 1 - Overview & Getting Started",
    "section": "",
    "text": "What is bioinformatics?\nWhat is reproducible research?\nWhy learn bioinformatics and data science skills?\nHigh Performance Computing (Unity)\nOverview of the R statistical programming language"
  },
  {
    "objectID": "labs/lab1_overview.html#overview",
    "href": "labs/lab1_overview.html#overview",
    "title": "Lab 1 - Overview",
    "section": "Overview",
    "text": "Overview\nIn recent years, the field of genomic analysis and bioinformatics has sifted towards requiring some knowledge of R, Python/Perl/C and the use of high performance computers (often requiring some fundamental Unix skills) available at national computing centers for working with large data sets. While there are many great software packages available for particular computational problems in evolutionary biology, many software programs do not have a user interface (e.g. drop down menus and such) and are run in command line mode. The lab sessions in this course have been designed to give students an introduction to working with R and packages used for genome and metagenome analyses.\nThe lab course is divided into 3 parts\n\nIntroduction to R and the tidyverse\nGene Expression Analysis\nAnalysis of SNPs and your genetic data"
  },
  {
    "objectID": "labs/lab1_overview.html#reproducible-research",
    "href": "labs/lab1_overview.html#reproducible-research",
    "title": "Lab 1 - Overview",
    "section": "Reproducible Research",
    "text": "Reproducible Research\nReproducibility is the hallmark of science, which is based on empirical observations coupled with explanatory models. While reproducibility encompasses the full science lifecycle, and includes issues such as methodological consistency and treatment of bias, in this course we will focus on computational reproducibility: the ability to document data, analyses, and models sufficiently for other researchers to be able to understand and ideally re-execute the computations that lead to scientific results and conclusions. With current publishing practices, this can be difficult because data are typically unavailable, the method sections of papers do not detail the computational approaches used, and analyses and models are often conducted in graphical programs, or, when scripted analyses are employed, the code is not available. In this course we will learn how to write code that is integrated into reproducible reports."
  },
  {
    "objectID": "labs/lab1_overview.html#data-science",
    "href": "labs/lab1_overview.html#data-science",
    "title": "Lab 1 - Overview & Getting Started",
    "section": "",
    "text": "The field of data science has grown tremendously over the last decade and the two programming languages, R and Python, used in analyzing genomic data are the most popular languages for data science. This made it easy to transfer bioinformatics skills to diverse fields.\nHere a few links that I will go over in lab:\n\nWhat is Data Science? \n\nWhat is Data Science? 8 Skills That Will Get You Hired\n\nOpen Science is Kinder Science\n\nData Carpentry\n\nBuilding a local community of practice in scientific programming for life scientists"
  },
  {
    "objectID": "labs/lab1_overview.html#high-performance-computing-unity-and-unix",
    "href": "labs/lab1_overview.html#high-performance-computing-unity-and-unix",
    "title": "Lab 1 - Overview & Getting Started",
    "section": "",
    "text": "In this course we will learn to write basic unix commands, run bioinformatics software from the command line and allocate computer resources for submitting large jobs. UMass has modern High Performance Computing system, Unity, and excellent staff members to help get you going and trouble shoot issues. Working on HPCs has become much easier with the advent of web interfaces that look and work much like software running on your computer."
  },
  {
    "objectID": "labs/lab1_overview.html#r",
    "href": "labs/lab1_overview.html#r",
    "title": "Lab 1 - Overview & Getting Started",
    "section": "",
    "text": "R is the largest and most comprehensive public domain statistical computing environment. The core R package is enhanced by several hundred user-supplied add-on packages, including many for gene expression analysis, in the  Comprehensive R Archive Network (CRAN). Omegahat Project for Statistical Computing.  BioConductor is an open source and open development software project for the analysis and comprehension of genomic data and is based primarily on the R programming language. R and Bioconductor are free, Open Source and available for Windows, MacOS and a wide variety of UNIX platforms.\n\n\nReproducibility is the hallmark of science, which is based on empirical observations coupled with explanatory models. While reproducibility encompasses the full science lifecycle, and includes issues such as methodological consistency and treatment of bias, in this course we will focus on computational reproducibility: the ability to document data, analyses, and models sufficiently for other researchers to be able to understand and ideally re-execute the computations that lead to scientific results and conclusions. With current publishing practices, this can be difficult because data are typically unavailable, the method sections of papers do not detail the computational approaches used, and analyses and models are often conducted in graphical programs, or, when scripted analyses are employed, the code is not available. In this course we will learn how to write code that is integrated into reproducible reports.\n\n\n\nMany introductory and advance tutorials have been developed for R. Here are a few\n\nThe offical R manuals\nCRAN’s Introduction to R\nR for Data Science by Garrett Grolemund and Hadley Wickham\nR Graphics Cookbook by Winston Chang\nData Carpentries Genomic Workshop Sessions\nData Analysis and Visualization in R for Ecologists\n\nThere are also many workshops and online R courses that you could take to follow up what you learn in this class."
  },
  {
    "objectID": "labs/lab1_overview.html#what-is-bioinformatics-and-data-science",
    "href": "labs/lab1_overview.html#what-is-bioinformatics-and-data-science",
    "title": "Lab 1 - Overview & Getting Started",
    "section": "",
    "text": "Bioinformatics is the field of science in which biology, computer science, statistics and information technology merge into a single discipline. There are three important sub-disciplines within bioinformatics:\n\nThe development of new algorithms and statistics with which to assess relationships among members of large data sets.\nThe development and implementation of tools that enable efficient access and management of different types of information.\nThe analysis and interpretation of various types of data including nucleotide and amino acid sequences, protein domains, and protein structures.\n\n\n\n\nis a term coined in response to the high demand of techniques and resources for handling the explosion of molecular data.\nis a buzzword to describe a growing field.\nbenefits from the physicists, chemists and mathematicians crossing over into biology.\nis a collection of tools.\nis way of thinking about a problem!\n\n\n\n\nIn order to make new algorithms and data sources available to biologists someone needs to write applications that include these algorithms and create new databases. Often this is first done by academic research groups. Later redone by private companies when market is large and profitable enough. There is a large gap between what is done by research groups and companies. Sometimes this is filled by large government funded projects, but not usually in time for most researchers. This is why bioinformatics and programming skills have become very valuable."
  },
  {
    "objectID": "methods/NEON_plot_samples.html",
    "href": "methods/NEON_plot_samples.html",
    "title": "Visualizing NEON samples within a plot",
    "section": "",
    "text": "R code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(viridis)\n\n\nLoad file\n\n\nR code\nneon.plot.samples &lt;- read_csv(\"../data/NEON_metadata/harvard_seasonal_study_coordinates.csv\")\n\n\nPlot of samples per plot per year at all HARV metagenome plots\n\n\nR code\nneon.plot.samples |&gt; \n  ggplot(aes(x=coreCoordinateX, y = coreCoordinateY, color = sampleTiming)) +\n  # add lines and annotate subplots\n  geom_hline(yintercept=20, color = \"gray\") +\n  geom_vline(xintercept=20, color = \"gray\") + \n  annotate(\"text\", x = 5, y = 5, label = \"21\", color = \"black\", size = 2) +\n  annotate(\"text\", x = 5, y = 35, label = \"39\", color = \"black\", size = 2) +\n  annotate(\"text\", x = 35, y = 5, label = \"23\", color = \"black\", size = 2) +\n  annotate(\"text\", x = 35, y = 35, label = \"41\", color = \"black\", size = 2) +\n  geom_rect(aes(xmin = 0, xmax = 40, ymin = 0, ymax = 40), fill = NA, color = \"black\", linewidth = .1) +\n  # add rectangle with no soil sampling area\n  geom_rect(aes(xmin = 10, xmax = 30, ymin = 10, ymax = 30), fill = \"white\", color = \"grey\", linewidth = .1) +\n  annotate(\"text\", x = 20, y = 23, label = \"No soil\", color = \"black\", size = 2) +\n  annotate(\"text\", x = 20, y = 17, label = \"sampling area\", color = \"black\", size = 2) +\n  # sampling points\n  geom_point(aes(shape = candidate)) +\n  labs(title = \"Plot position of organic soil samples from HARV in 2024\") +\n  facet_wrap(~plotID) +\n  theme_minimal() +\n  coord_fixed()\n\n\n\n\n\n\n\n\n\n## Use pairwise distances and compute a total or average distance between the points in each set.\n### 33\n\n\nR code\n# Define two sets of 3 points\nset1 &lt;- matrix(c(2.5, 15, 12, 31.5, 0.5, 35.5), ncol = 2, byrow = TRUE)  # Points: (21,2.5), (2,2), (3,3)\nset2 &lt;- matrix(c(36, 19.5, 31.5, 16.5, 24.5, 0), ncol = 2, byrow = TRUE)  # Points: (1,1), (1.5,1.5), (2,2)\n\n# Function to compute total pairwise distance\ntotal_distance &lt;- function(points) {\n  dist_matrix &lt;- dist(points)  # Computes all pairwise distances\n  sum(dist_matrix)\n}\n\n# Compare sets\ndist1 &lt;- total_distance(set1)\ndist2 &lt;- total_distance(set2)\n\ncat(\"Set 1 total distance:\", dist1, \"\\n\")\n\n\nSet 1 total distance: 51.81256 \n\n\nR code\ncat(\"Set 2 total distance:\", dist2, \"\\n\")\n\n\nSet 2 total distance: 45.97024 \n\n\nR code\nif (dist1 &lt; dist2) {\n  cat(\"Set 1 is closer together.\\n\")\n} else {\n  cat(\"Set 2 is closer together.\\n\")\n}\n\n\nSet 2 is closer together.\n\n\n## Use pairwise distances and compute a total or average distance between the points in each set.\n### 34\n\n\nR code\n# Define two sets of 3 points\nset1 &lt;- matrix(c(16.5, 31.5, 11, 34, 30, 28), ncol = 2, byrow = TRUE)  # Points: (21,2.5), (2,2), (3,3)\nset2 &lt;- matrix(c(3, 2, 19, 8, 13.5, 9), ncol = 2, byrow = TRUE)  # Points: (1,1), (1.5,1.5), (2,2)\n\n# Function to compute total pairwise distance\ntotal_distance &lt;- function(points) {\n  dist_matrix &lt;- dist(points)  # Computes all pairwise distances\n  sum(dist_matrix)\n}\n\n# Compare sets\ndist1 &lt;- total_distance(set1)\ndist2 &lt;- total_distance(set2)\n\ncat(\"Set 1 total distance:\", dist1, \"\\n\")\n\n\nSet 1 total distance: 39.91271 \n\n\nR code\ncat(\"Set 2 total distance:\", dist2, \"\\n\")\n\n\nSet 2 total distance: 35.29761 \n\n\nR code\nif (dist1 &lt; dist2) {\n  cat(\"Set 1 is closer together.\\n\")\n} else {\n  cat(\"Set 2 is closer together.\\n\")\n}\n\n\nSet 2 is closer together."
  },
  {
    "objectID": "labs/lab1_overview.html#generative-ai",
    "href": "labs/lab1_overview.html#generative-ai",
    "title": "Lab 1 - Overview & Getting Started",
    "section": "",
    "text": "You all are part of the first generation of generative AI users. The saying goes “AI won’t take your job, but someone using who knows how to use AI might.” Think of AI as a force multiplier. You have to learn to code and clearly state your problems before AI can to help you. This class will fully use generative AI in hopes that it will challenge us to think more creatively about problems and not stress out about syntax. Think first…ask questions…code…solve problem! We will use the UMass version of Microsoft Copilot Chat and Copilot integrated into RStudio. We will embrace “Vibe coding” (see What is Vibe Coding, Exactly?) and ride the waves."
  },
  {
    "objectID": "labs/lab1_overview.html#overview-1",
    "href": "labs/lab1_overview.html#overview-1",
    "title": "Lab 1 - Overview & Getting Started",
    "section": "",
    "text": "In recent years, the field of genomic analysis and bioinformatics has sifted towards requiring some knowledge of R, Python/Perl/C and the use of high performance computers (often requiring some fundamental Unix skills) available at national computing centers for working with large data sets. While there are many great software packages available for particular computational problems in evolutionary biology, many software programs do not have a user interface (e.g. drop down menus and such) and are run in command line mode. The lab sessions in this course have been designed to give students an introduction to working with R and packages used for genome and metagenome analyses. We are using recently release National Ecological Observatory Network data to design a course-based Undergraduate Research Experience (CURE). The first 4 weeks we will discuss to project space, discuss research ideas and formulate testable hypothesis or discovery driven approaches, design the experimental approaches. For a preview today I will give an overview of the project space."
  },
  {
    "objectID": "labs/lab1_overview.html#github",
    "href": "labs/lab1_overview.html#github",
    "title": "Lab 1 - Overview & Getting Started",
    "section": "",
    "text": "GitHub has become a popular way to manage, share and view code for open source projects. The tutorials created for this course will be written in Quarto and posted on GitHub. Thus, you will be able to continue to see course materials after the end of the semester. You will use and make a GitHub web site for your research project."
  },
  {
    "objectID": "labs/lab1_overview.html#course-based-undergraduate-research-experience-cure",
    "href": "labs/lab1_overview.html#course-based-undergraduate-research-experience-cure",
    "title": "Lab 1 - Overview & Getting Started",
    "section": "",
    "text": "We are using recently release National Ecological Observatory Network data to design a course-based Undergraduate Research Experience (CURE). The first 4 weeks we will discuss to project space, discuss research ideas and formulate testable hypothesis or discovery driven approaches, design the experimental approaches. For a preview today I will give an overview of the project space"
  },
  {
    "objectID": "labs/lab1_overview.html#dyiers",
    "href": "labs/lab1_overview.html#dyiers",
    "title": "Lab 1 - Overview & Getting Started",
    "section": "",
    "text": "You can do all of the R-based labs on your own computer. Follow these directions by the makers of RStudio, Posit. You will need to download the lab files from Unity or the course GitHub site."
  },
  {
    "objectID": "labs/lab1_overview.html#accessing-the-course-unity-resources",
    "href": "labs/lab1_overview.html#accessing-the-course-unity-resources",
    "title": "Lab 1 - Overview & Getting Started",
    "section": "Accessing the course Unity resources",
    "text": "Accessing the course Unity resources\nIf you haven’t already done so, request a Unity HPC account and access to our course directory\nTo request a Unity account and access our course directory.\n\nGo to Unity\nRequest an account\nGo to MyPIs, click on the + button and enter pi_bio678_umass_edu"
  },
  {
    "objectID": "labs/lab1_overview.html#r-and-rstudio",
    "href": "labs/lab1_overview.html#r-and-rstudio",
    "title": "Lab 1 - Overview & Getting Started",
    "section": "R and RStudio",
    "text": "R and RStudio\n\nRStudio using Open OnDemand\nOpen OnDemand makes supercomputing accessible through a web portal.\n\nGo to Unity\nOn the left menu select OpenOnDemand\nIn the top menu select Interactive Apps then RStudio\nSet the job duration for 4 hrs to cover the length of the lab. Otherwise set the time to what you anticipate needing.\nUnless otherwise suggested set CPU Core Count to 2 and the memory to 8 gb.\n\nClick Launch. It takes about a minute the job to start and then you can launch the RStudio Interface.\n\n\n\nRStudio Interface\nThe default R studio appearance includes 4 windows.\n\nThe R script(s) and data view (upper left window).\nConsole (bottom left window).\nWork space and history (upper right window).\nFiles, plots, packages and help (bottom right window).\n\n\n\n\nRStudio Screenshot\n\n\n\nThe R script(s) and data view window (upper left window)\nIn this window you can type directly into a file, run code and save the file for reuse. In this class we will work with Quarto files (discussed below).\n\n\nConsole Window (bottom left window)\nThe console is where you can type R commands and see output.\nType\n3 + 3\nTo better document and save your code write it in the Quarto documents rather than the console. On occasion we will use the console to access documentation and for other purposes.\n\n\nEnvironment and History tabs (upper right window)\nThe Environment tab shows all the active objects. If you have a data frame loaded, then click on the object will enable you to view the table. The History tab shows a list of commands used so far.\n\n\nFiles, Plots, Packages and Help (bottom right window)\nThere are data sets that come with the R package and used in tutorials. If you run the following command you will see a graph of related to the cars data set in the Plots window\n\n\n\nQuarto\nThe Quarto is a scientific publishing system. In this class we will use one of it’s simplest features, producing a report with the code and resulting output (graphs, tables, statistical analysis). Quarto can also be used to produce slides, web sites, scientific manuscripts and books. For example, and all the labs for this course and my research laboratory website were made using Quarto. Quarto wraps together many previous packages used for publishing with R.\nTo use Quarto with R, the rmarkdown R package is installed. There are some differences between a Quarto and R Markdown document, but overall they are very similar.\n\nProducing Lab Reports with Quarto\nIn RStudio select File &gt; New File &gt; Quarto Document. Add a title (e.g. Lab 1) and your name then create the document. Notice the your file says untitled with an asterisk. Save your file (e.g. lab1). This will automatically add the .qmd extension to your file (lab1.qmd). ALWAYS SAVE YOUR FILE BEFORE YOU START WORKING AND OFTEN WHILE WORKING.\nThe top section of the document delineated by the --- is called the YAML block. In this template it contains your the title, your name, the output type (html) and the editor preference (visual).You can also work with your file directly with the source code by clicking the source icon.\nThe following lines of code in your YAML block with generate a table of contents (toc) as shown at the top of this lab. The line with embed-resources creates a stand alone html file. This is also availabe as lab_template.qmd in our course directory /work/pi_bio678_umass_edu\n---\ntitle: \"Lab 1\"\nauthor: \"Your name\"\nformat:\n  html:\n    toc: true\n    toc_float: true\n    embed-resources: true\neditor: visual\nexecute: \n  warning: false\n  message: false\n---\nThe text with the white background is in rmarkdown. The icons in the same section as the visual icon you can easily made the text in bold or in italics, change the text from normal to a header, create bulleted or numbered lists, insert html links, add images, insert tables and more.\nThe text with the gray background is in R code chunks. Click on the green play icon in the top right corner of the code chunk to run the code.\nClick on the Render icon. This will run the code, show the output and create a html file that is automatically saved to your directory (look for the lab1.html file) and will automatically open this file in your browser.\nCreate new code chunk by clicking on the green +C icon to the right of the Render icon. In the code chunk type plot(cars). Then click the run the code to see a graph of the cars data set that comes preloaded into R.\n\n\nR code\nplot(cars)"
  },
  {
    "objectID": "labs/lab1_overview.html#writing-r-code",
    "href": "labs/lab1_overview.html#writing-r-code",
    "title": "Lab 1 - Overview & Getting Started",
    "section": "Writing R code",
    "text": "Writing R code\n\nAssignment statements\nAll R statements where you create objects are called assignment statements and the form “object_name &lt;- value”\n\n\nR code\nx &lt;- 3\n\n\nSimply typing x will give the value of x\n\n\nR code\nx\n\n\n[1] 3\n\n\nYou will make lots of assignments and &lt;- is a pain to type. Instead, use RStudio’s keyboard shortcut: Alt + - (the minus sign). Notice that RStudio automagically surrounds &lt;- with spaces, which is a good code formatting practice. An equals sign = will work in place of &lt;-, but it will cause confusion later so keep to the convention of using &lt;- to make assignments\n\n\nObject Names\nObject names must start with a letter, and can only contain letters, numbers, underscores and periods. You want your object names to be descriptive, so you’ll need a convention for multiple words. I recommend snake_case where you separate lowercase words with an underscore. Note that R is case sensitive, e.g., object names gene, GENE, Gene are all different.\n\n\nR code\ngenome_size &lt;- 3100000000\n\n\nImportant note: since there are many built-in functions in R, make sure that the new object names you assign are not already used by the system. A simple way of checking this is to type in the name you want to use. If the system returns an error message telling you that such object is not found, it is safe to use the name.\n\n\nCharacters\nA character object is used to represent string values in R. It is defined by double quotes ““.\n\n\nR code\nDNA &lt;- \"ATGAAA\"\nDNA\n\n\n[1] \"ATGAAA\"\n\n\n\n\nVectors\nA vector is a sequence of data elements of the same basic type. data elements in a vector are officially called components. Assignment operator (&lt;-) stores the value (object) on the right side of (&lt;-) expression in the left side. Once assigned, the object can be used just as an ordinary component of the computation. The c function concanenates the components into a vector.\n\n\nR code\nrandom_numbers &lt;- c(1,10,100)    \nrandom_numbers\n\n\n[1]   1  10 100\n\n\nNow you can do scalar computations on a vector\n\n\nR code\nrandom_numbers * 2\n\n\n[1]   2  20 200\n\n\nor use sum, sort, min, max, length and many other operations. For example\n\n\nR code\nsort(random_numbers)\n\n\n[1]   1  10 100\n\n\nYou can also do vector arithmatic\n\n\nR code\nrandom_numbers &lt;- c(1,10,100) \ny&lt;- c(1,2,3) \nrandom_numbers * y\n\n\n[1]   1  20 300\n\n\nVectors can also be made of characters\n\n\nR code\ncodons&lt;- c(\"AUG\", \"UAU\", \"UGA\") \ncodons\n\n\n[1] \"AUG\" \"UAU\" \"UGA\""
  },
  {
    "objectID": "labs/lab1_overview.html#exercises",
    "href": "labs/lab1_overview.html#exercises",
    "title": "Lab 1 - Overview & Getting Started",
    "section": "Exercises",
    "text": "Exercises\nYour lab report must have each exercise labeled with a header (e.g. ## Exercise 1) so that each one appears in the table of contents.\nYou will need to first export (download) the Lab1_yourname.html file to your computer, then upload the file to Canvas. In the bottom right corner click on the wheel icon then select Export.\n\nThe main goal for today’s lab is to create the lab report so we need a few exercises to fill it out\n\nExercise 1\nFor x = 2 and y = 15, compute the sum and difference of x and y\n\n\nExercise 2\nCreate a vector of the values 22, 62, 148, 43 and 129. Multiple the vector by 5.\n\n\nExercise 3\nCreate a vector of the nucleotides A, T, C and G. Remember to put a “” around each letter. Arrange the nucleotides alphabetically using the sort function sort(vector_name)"
  },
  {
    "objectID": "labs/lab1s_quarto.html",
    "href": "labs/lab1s_quarto.html",
    "title": "Lab S1 - Quarto",
    "section": "",
    "text": "Quarto\nDifferences between R Markdown and Quarto\nYAML blocks\nCode blocks\n\n\n\n\n\nQuarto is the name of an open-source publishing system used for technical and scientific writing.\nIt lets you combine text, code, and outputs in one document—perfect for data science, research, and reproducible reports.\nYou can write in Python, R, Julia, or Observable JavaScript, and publish to formats like HTML, PDF, Word, and even full websites\nIt’s considered the next generation of R Markdown, and works with tools like Jupyter Notebooks, VS Code, and RStudio.\nBecause it is based on R Markdown, there is a wealth of related resources and books published on and using R Markdown. See some of on the examples books using R we will periodically using in the course.\n\n\n\n\nR Markdown and Quarto are both tools for creating dynamic documents that combine code, text, and outputs (like plots or tables), but they differ in terms of design philosophy, features, and flexibility. Here’s a breakdown of their key differences:\n\n\n\nR Markdown:\n\nDeveloped by RStudio.\nPrimarily designed for R users.\nBuilt on top of knitr and Pandoc.\nDeeply integrated into the RStudio IDE.\n\nQuarto:\n\nAlso developed by RStudio, but as a next-generation tool.\nLanguage-agnostic: supports R, Python, Julia, and Observable JavaScript.\nUses Pandoc directly (not knitr).\nDesigned to unify and modernize the workflow across languages.\n\n\n\n\n\n\nR Markdown: Best suited for R. Python support is possible but less seamless.\nQuarto: First-class support for multiple languages in a single document. You can mix R, Python, Julia, and JavaScript.\n\n\n\n\n\nR Markdown:\n\nUses YAML front matter for metadata.\nCode chunks are written using triple backticks with language identifiers.\n\nQuarto:\n\nSimilar structure but more consistent and extensible YAML.\nSupports Markdown extensions like callouts, citations, cross-referencing, and more.\n\n\n\n\n\n\nR Markdown:\n\nSupports HTML, PDF, Word, slides (via xaringan or ioslides), etc.\nCustomization can be complex for advanced layouts.\n\nQuarto:\n\nSupports all R Markdown formats plus:\n\nReveal.js slides.\nBooks and websites with built-in navigation and styling.\nJupyter-style notebooks.\n\nEasier to configure and customize outputs.\n\n\n\n\n\n\nR Markdown:\n\nExecutes code using knitr (for R) or reticulate (for Python).\nLess control over execution environment.\n\nQuarto:\n\nUses Jupyter kernels or R directly.\nBetter support for notebook-style interactivity and execution control.\n\n\n\n\n\n\nR Markdown:\n\nNo native concept of a “project” beyond RStudio projects.\n\nQuarto:\n\nSupports Quarto Projects: collections of documents with shared configuration.\nIdeal for books, blogs, websites, and multi-document workflows.\n\n\n\n\n\n\nQuarto:\n\nMore modern and extensible.\nSupports Lua filters, Markdown extensions, cross-referencing, citations, and interactive widgets.\nBetter support for version control and CI/CD workflows.\n\n\n\n\n\n\n\n\nFeature\nR Markdown\nQuarto\n\n\n\n\nLanguage Support\nPrimarily R\nR, Python, Julia, JS\n\n\nExecution Engine\nknitr\nJupyter / native\n\n\nOutput Formats\nMany\nMore + easier config\n\n\nInteractivity\nLimited\nRich (widgets, JS)\n\n\nProject Support\nBasic\nFull project system\n\n\nExtensibility\nModerate\nHigh\n\n\nIdeal For\nR-centric reports\nMulti-language docs\n\n\n\n\n\n\n\n\nLet’s go the the Quarto documentation for Markdown basics and more details on Figures\nIt is important to specific where the figure is located relative to your .qmd file. The path can be relative (giant_virus.jpg) or (images/giant_virus.jpg) or absolute (/home/pi_jlb_umass_edu/images/giant_virus.jpg).\n\n\n\ngiant virus\n\n\n\n\n\nIn Quarto, a YAML block is a section at the top of a document that contains metadata and configuration settings. YAML stands for “YAML Ain’t Markup Language”, and it’s used to define things like the document title, author, output format, and more.\n\n\nA YAML block is enclosed by triple dashes (---) at the beginning and end:\n---\ntitle: \"My Analysis Report\"\nauthor: \"Jeffrey Blanchard\"\ndate: \"2025-09-08\"\nformat: html\neditor: visual\n---\nThis block tells Quarto: - The title of the document. - The author name. - The date to display. - The output format (e.g., HTML, PDF, Word). - The editor preference (e.g., visual or source).\n\n\n\nHere are some frequently used fields:\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\ntitle\nTitle of the document\n\n\nauthor\nAuthor name(s)\n\n\ndate\nDate of publication\n\n\nformat\nOutput format (e.g., html, pdf, docx, revealjs)\n\n\ntoc\nTable of contents (true or false)\n\n\nnumber-sections\nNumber section headings\n\n\ntheme\nVisual theme for HTML or slides\n\n\ncode-fold\nWhether code chunks can be collapsed\n\n\nexecute\nControls code execution (e.g., echo, eval, freeze)\n\n\nbibliography\nPath to .bib file for citations\n\n\nfilters\nLua filters for advanced customization\n\n\n\n\n\n\n---\ntitle: \"Data Exploration\"\nauthor: \"Jeffrey Blanchard\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    theme: cosmo\n    embed-resources: true\nexecute:\n  echo: true\n  freeze: auto\n---\nThis configures: - An HTML output with a table of contents. - Collapsible code chunks. - A Bootstrap theme (cosmo). - Code execution settings.\nImportant for our class is the embed-resources: true line. This creates a single html file with the figures embeded in the file. If this line is not in the YAML block a new directory will be created which contains the images and is linked to the html file. This means that if you turn in this html file (without using embed-resources: true) the images will not be shown.\n\n\n\nQuarto YAML is: - More consistent and extensible. - Supports nested configuration (e.g., format.html.toc). - Easier to manage across multi-format outputs (e.g., HTML and PDF from one source).\nIn Quarto, R code chunks are sections of code embedded in your document that get executed when the document is rendered. These chunks are enclosed in triple backticks and start with {r}. You can customize their behavior using chunk options, which control things like whether the code is shown, whether it’s executed, how results are displayed, and more.\n\n\n\n\n\n\n\n\nR code\nsummary(cars)\n\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\n\n\n\nHere’s a categorized list of the most useful options:\n\n\n\n\n\nOption\nDescription\n\n\n\n\neval\nWhether to evaluate the code (TRUE or FALSE)\n\n\necho\nShow the code in the output (TRUE or FALSE)\n\n\ninclude\nInclude both code and output (TRUE or FALSE)\n\n\nerror\nShow errors in output (TRUE or FALSE)\n\n\nwarning\nShow warnings (TRUE or FALSE)\n\n\nmessage\nShow messages (TRUE or FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\nresults\nHow to display results (\"markup\", \"asis\", \"hide\")\n\n\nfig.width\nWidth of plots (in inches)\n\n\nfig.height\nHeight of plots (in inches)\n\n\nfig.cap\nCaption for figures\n\n\nfig.align\nAlignment of figures (\"left\", \"center\", \"right\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\ncache\nCache results to avoid re-running code\n\n\nfreeze\nFreeze output to avoid re-execution unless explicitly updated\n\n\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\ntidy\nAutomatically tidy code before execution\n\n\ncollapse\nCollapse code and output together\n\n\ncomment\nPrefix for output lines\n\n\n\n\n\n\n\n{r pressure-plot, fig.width=6, fig.height=4, fig.cap=\"Pressure vs Temperature\"}\n\n\nR code\nplot(pressure)\n\n\n\n\n\nPressure vs Temperature\n\n\n\n\nThis chunk: - Hides the code (echo=FALSE) - Sets figure size - Adds a caption - Names the chunk (pressure-plot) for reference\n\n\n\n\nTry a different Quarto theme\n\n\n\nCreate a lab report that has\n\nA link out to an external web site.\nAn image embedded.\nA table of contents using the YAML block\nThe code folded using the YAML block\nA code chunk with plot(cars) in which the plot is sized to a figure width of 3 and height of 2\nA code chunk in which the output, but not the code is in the rendered file.\nTry a different Quarto theme.\n\n\n\n\nThis lab was created with assistance from UMass Copilot"
  },
  {
    "objectID": "labs/lab1s_quarto.html#learning-objectives",
    "href": "labs/lab1s_quarto.html#learning-objectives",
    "title": "Lab S1 - Quarto",
    "section": "",
    "text": "Quarto\nDifferences between R Markdown and Quarto\nYAML blocks\nCode blocks"
  },
  {
    "objectID": "labs/lab1s_quarto.html#quarto",
    "href": "labs/lab1s_quarto.html#quarto",
    "title": "Lab S1 - Quarto",
    "section": "",
    "text": "Quarto is the name of an open-source publishing system used for technical and scientific writing.\nIt lets you combine text, code, and outputs in one document—perfect for data science, research, and reproducible reports.\nYou can write in Python, R, Julia, or Observable JavaScript, and publish to formats like HTML, PDF, Word, and even full websites\nIt’s considered the next generation of R Markdown, and works with tools like Jupyter Notebooks, VS Code, and RStudio.\nBecause it is based on R Markdown, there is a wealth of related resources and books published on and using R Markdown. See some of on the examples books using R we will periodically using in the course."
  },
  {
    "objectID": "labs/lab1s_quarto.html#differences-between-r-markdown-and-quarto",
    "href": "labs/lab1s_quarto.html#differences-between-r-markdown-and-quarto",
    "title": "Lab S1 - Quarto",
    "section": "",
    "text": "R Markdown and Quarto are both tools for creating dynamic documents that combine code, text, and outputs (like plots or tables), but they differ in terms of design philosophy, features, and flexibility. Here’s a breakdown of their key differences:\n\n\n\nR Markdown:\n\nDeveloped by RStudio.\nPrimarily designed for R users.\nBuilt on top of knitr and Pandoc.\nDeeply integrated into the RStudio IDE.\n\nQuarto:\n\nAlso developed by RStudio, but as a next-generation tool.\nLanguage-agnostic: supports R, Python, Julia, and Observable JavaScript.\nUses Pandoc directly (not knitr).\nDesigned to unify and modernize the workflow across languages.\n\n\n\n\n\n\nR Markdown: Best suited for R. Python support is possible but less seamless.\nQuarto: First-class support for multiple languages in a single document. You can mix R, Python, Julia, and JavaScript.\n\n\n\n\n\nR Markdown:\n\nUses YAML front matter for metadata.\nCode chunks are written using triple backticks with language identifiers.\n\nQuarto:\n\nSimilar structure but more consistent and extensible YAML.\nSupports Markdown extensions like callouts, citations, cross-referencing, and more.\n\n\n\n\n\n\nR Markdown:\n\nSupports HTML, PDF, Word, slides (via xaringan or ioslides), etc.\nCustomization can be complex for advanced layouts.\n\nQuarto:\n\nSupports all R Markdown formats plus:\n\nReveal.js slides.\nBooks and websites with built-in navigation and styling.\nJupyter-style notebooks.\n\nEasier to configure and customize outputs.\n\n\n\n\n\n\nR Markdown:\n\nExecutes code using knitr (for R) or reticulate (for Python).\nLess control over execution environment.\n\nQuarto:\n\nUses Jupyter kernels or R directly.\nBetter support for notebook-style interactivity and execution control.\n\n\n\n\n\n\nR Markdown:\n\nNo native concept of a “project” beyond RStudio projects.\n\nQuarto:\n\nSupports Quarto Projects: collections of documents with shared configuration.\nIdeal for books, blogs, websites, and multi-document workflows.\n\n\n\n\n\n\nQuarto:\n\nMore modern and extensible.\nSupports Lua filters, Markdown extensions, cross-referencing, citations, and interactive widgets.\nBetter support for version control and CI/CD workflows.\n\n\n\n\n\n\n\n\nFeature\nR Markdown\nQuarto\n\n\n\n\nLanguage Support\nPrimarily R\nR, Python, Julia, JS\n\n\nExecution Engine\nknitr\nJupyter / native\n\n\nOutput Formats\nMany\nMore + easier config\n\n\nInteractivity\nLimited\nRich (widgets, JS)\n\n\nProject Support\nBasic\nFull project system\n\n\nExtensibility\nModerate\nHigh\n\n\nIdeal For\nR-centric reports\nMulti-language docs"
  },
  {
    "objectID": "labs/lab1s_quarto.html#yaml-blocks-in-quarto",
    "href": "labs/lab1s_quarto.html#yaml-blocks-in-quarto",
    "title": "Lab S1 - Quarto",
    "section": "",
    "text": "In Quarto, a YAML block is a section at the top of a document that contains metadata and configuration settings. YAML stands for “YAML Ain’t Markup Language”, and it’s used to define things like the document title, author, output format, and more.\n\n\nA YAML block is enclosed by triple dashes (---) at the beginning and end:\n---\ntitle: \"My Analysis Report\"\nauthor: \"Jeffrey Blanchard\"\ndate: \"2025-09-08\"\nformat: html\neditor: visual\n---\nThis block tells Quarto: - The title of the document. - The author name. - The date to display. - The output format (e.g., HTML, PDF, Word). - The editor preference (e.g., visual or source).\n\n\n\nHere are some frequently used fields:\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\ntitle\nTitle of the document\n\n\nauthor\nAuthor name(s)\n\n\ndate\nDate of publication\n\n\nformat\nOutput format (e.g., html, pdf, docx, revealjs)\n\n\ntoc\nTable of contents (true or false)\n\n\nnumber-sections\nNumber section headings\n\n\ntheme\nVisual theme for HTML or slides\n\n\ncode-fold\nWhether code chunks can be collapsed\n\n\nexecute\nControls code execution (e.g., echo, eval, freeze)\n\n\nbibliography\nPath to .bib file for citations\n\n\nfilters\nLua filters for advanced customization\n\n\n\n\n\n\n---\ntitle: \"Data Exploration\"\nauthor: \"Jeffrey Blanchard\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    theme: cosmo\n    embed-resources: true\nexecute:\n  echo: true\n  freeze: auto\n---\nThis configures: - An HTML output with a table of contents. - Collapsible code chunks. - A Bootstrap theme (cosmo). - Code execution settings.\nImportant for our class is the embed-resources: true line. This creates a single html file with the figures embeded in the file. If this line is not in the YAML block a new directory will be created which contains the images and is linked to the html file. This means that if you turn in this html file (without using embed-resources: true) the images will not be shown.\n\n\n\nQuarto YAML is: - More consistent and extensible. - Supports nested configuration (e.g., format.html.toc). - Easier to manage across multi-format outputs (e.g., HTML and PDF from one source).\nIn Quarto, R code chunks are sections of code embedded in your document that get executed when the document is rendered. These chunks are enclosed in triple backticks and start with {r}. You can customize their behavior using chunk options, which control things like whether the code is shown, whether it’s executed, how results are displayed, and more."
  },
  {
    "objectID": "labs/lab1s_quarto.html#r-code-chunks-in-quarto",
    "href": "labs/lab1s_quarto.html#r-code-chunks-in-quarto",
    "title": "Lab S1 - Quarto",
    "section": "",
    "text": "R code\nsummary(cars)\n\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\n\n\n\nHere’s a categorized list of the most useful options:\n\n\n\n\n\nOption\nDescription\n\n\n\n\neval\nWhether to evaluate the code (TRUE or FALSE)\n\n\necho\nShow the code in the output (TRUE or FALSE)\n\n\ninclude\nInclude both code and output (TRUE or FALSE)\n\n\nerror\nShow errors in output (TRUE or FALSE)\n\n\nwarning\nShow warnings (TRUE or FALSE)\n\n\nmessage\nShow messages (TRUE or FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\nresults\nHow to display results (\"markup\", \"asis\", \"hide\")\n\n\nfig.width\nWidth of plots (in inches)\n\n\nfig.height\nHeight of plots (in inches)\n\n\nfig.cap\nCaption for figures\n\n\nfig.align\nAlignment of figures (\"left\", \"center\", \"right\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\ncache\nCache results to avoid re-running code\n\n\nfreeze\nFreeze output to avoid re-execution unless explicitly updated\n\n\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\ntidy\nAutomatically tidy code before execution\n\n\ncollapse\nCollapse code and output together\n\n\ncomment\nPrefix for output lines\n\n\n\n\n\n\n\n{r pressure-plot, fig.width=6, fig.height=4, fig.cap=\"Pressure vs Temperature\"}\n\n\nR code\nplot(pressure)\n\n\n\n\n\nPressure vs Temperature\n\n\n\n\nThis chunk: - Hides the code (echo=FALSE) - Sets figure size - Adds a caption - Names the chunk (pressure-plot) for reference"
  },
  {
    "objectID": "labs/lab1s_quarto.html#acknowledgements",
    "href": "labs/lab1s_quarto.html#acknowledgements",
    "title": "Lab S1 - Quarto",
    "section": "",
    "text": "This lab was created with assistance from UMass Copilot"
  },
  {
    "objectID": "labs/lab2_AI_ggplot.html",
    "href": "labs/lab2_AI_ggplot.html",
    "title": "Learning R with the help of AI tools starting graphing using ggplot2",
    "section": "",
    "text": "Generative AI\nInstalling R packages\nBuilt-in R data sets and data set packages\nggplot2"
  },
  {
    "objectID": "labs/lab1s_quarto.html#adding-links-and-figures-in-a-quarto-document",
    "href": "labs/lab1s_quarto.html#adding-links-and-figures-in-a-quarto-document",
    "title": "Lab S1 - Quarto",
    "section": "",
    "text": "Let’s go the the Quarto documentation for Markdown basics and more details on Figures\nIt is important to specific where the figure is located relative to your .qmd file. The path can be relative (giant_virus.jpg) or (images/giant_virus.jpg) or absolute (/home/pi_jlb_umass_edu/images/giant_virus.jpg).\n\n\n\ngiant virus"
  },
  {
    "objectID": "labs/lab1s_quarto.html#exercises",
    "href": "labs/lab1s_quarto.html#exercises",
    "title": "Lab S1 - Quarto",
    "section": "",
    "text": "Create a lab report that has\n\nA link out to an external web site.\nAn image embedded.\nA table of contents using the YAML block\nThe code folded using the YAML block\nA code chunk with plot(cars) in which the plot is sized to a figure width of 3 and height of 2\nA code chunk in which the output, but not the code is in the rendered file.\nTry a different Quarto theme."
  },
  {
    "objectID": "labs/lab1s_quarto.html#quarto-themes",
    "href": "labs/lab1s_quarto.html#quarto-themes",
    "title": "Lab S1 - Quarto",
    "section": "",
    "text": "Try a different Quarto theme"
  },
  {
    "objectID": "labs/lab2_AI_ggplot.html#learning-objectives",
    "href": "labs/lab2_AI_ggplot.html#learning-objectives",
    "title": "Learning R with the help of AI tools starting graphing using ggplot2",
    "section": "",
    "text": "Generative AI\nInstalling R packages\nBuilt-in R data sets and data set packages\nggplot2"
  },
  {
    "objectID": "labs/lab2_AI_ggplot.html#generative-ai",
    "href": "labs/lab2_AI_ggplot.html#generative-ai",
    "title": "Learning R with the help of AI tools starting graphing using ggplot2",
    "section": "Generative AI",
    "text": "Generative AI\n\nClimate Change\n\nTrends in Atmospheric Carbon Dioxide (CO2)\nNASA GISS Surface Temperature Analysis (GISTEMP v4)\nGenerative AI’s environmental costs are soaring — and mostly secret\n\n\n\nAI and your future jobs\nAI won’t take your job, but someone using who knows how to use AI might. Think of AI as a force multiplier. You have to learn to code first before you can use AI to help you. Google recently reported that about 25% of its new code is AI-generated.\n\n\nUMass and Generative AI\n\nUMass Guidance on Generative Artificial Intelligence\nUMass Responsible Use of Generative AI\nGenAI Products\n\n\n\nCopilot\nMicrosoft designed Copilot to work off of the latest version of OpenAI’s GPT model, GPT-4. GPT-5 is coming soon.\n\ngithub co-pilot\ngithub education\nMicrosoft Introduction to GitHub Copilot\nIntroduction to GitHub CoPilot videos\nSet up co-pilot for learning\nHow to write better prompts for GitHub Copilot\n\n\n\nAI and R\n\nRStudio github copilot\nAI Assisted Coding in RStudio Integrating OpenAI’s ChatGPT into RStudio is now possible with “Chattr”, “GPT Studio” and “GitHub Copilot”. These new tools will help you find the right functions and commands and to quickly generate code snippets to save you time.\n\n\n\nVibe coding\n\nVibe coding\nWhat is vibe coding, exactly?\nWhat Is Vibe Coding? Definition, Tools, Pros, and Cons\n\n\n\nGithub, Git and AI\n\nYou can use GitHub and Git to collaborate on work\nUsing co-pilot as your tutor\nchattr\n\n\n\nBioinformatics and AI\n\nA data-intelligence-intensive bioinformatics copilot system for large-scale omics research and scientific insights\nBioinformatics AI: Driving Future Biological Breakthroughs\n\n\n\nAI and Scholarly Publishing\n\nAmerican Society of Microbiology Guidelines for Authors Using AI Tools\nChatGPT: guidelines for responsible use (Nature)\nGuidelines from COPE\nICMJE guidance\nWebinar and article from STM integrity hub on use of generative AI in scholarly publishing\nCSR Review Matters Blog (nih.gov)"
  },
  {
    "objectID": "labs/lab2_AI_ggplot.html#introduction-to-r-graphics",
    "href": "labs/lab2_AI_ggplot.html#introduction-to-r-graphics",
    "title": "Learning R with the help of AI tools starting graphing using ggplot2",
    "section": "Introduction to R Graphics",
    "text": "Introduction to R Graphics\nR provides comprehensive graphics utilities for visualizing and exploring scientific data. To date we have been making a few plots using the R Base Graphics. In addition, several more recent graphics environments extend these utilities. These include the grid, lattice and ggplot2 packages. All have the roles, but ggplot2 environment that is part of the Tidyverse package has become popular and is now used for many R packages and in scientific publications.\n\nggplot2 and the Grammar of Graphics\nggplot2 is meant to be an implementation of the Grammar of Graphics, hence the gg in ggplot. The basic notion is that there is a grammar to the composition of graphical components in statistical graphics. By directly controlling that grammar, you can generate a large set of carefully constructed graphics from a relatively small set of operations. As Hadley Wickham (2010), the author of ggplot2 said,\n“A good grammar will allow us to gain insight into the composition of complicated graphics, and reveal unexpected connections between seemingly different graphics.”\n\n\nTutorials and resources\nYou can make amazing graphs with ggplot, but there is a long learning curve so we will have multiple lab sessions on ggplot and graphing. Here are a few different resources for ggplot.\n\nHadley Wickham and Garrett Grolemund released the second edition of R for Data Science.\nData Carpentry’s Data Analysis and Visualization in R for Ecologists\nFor those with a visual learning style there is Maria Nattestad’s Youtube videos\nThe ggplot cheatsheet"
  },
  {
    "objectID": "labs/lab2_AI_ggplot.html#on-the-computer",
    "href": "labs/lab2_AI_ggplot.html#on-the-computer",
    "title": "Learning R with the help of AI tools starting graphing using ggplot2",
    "section": "On the Computer",
    "text": "On the Computer\n\nCreate and save your Quarto Markdown (qmd) file\nJust like last week we will be writing our code in a Quarto Markdown (qmd) file. Remember to use the following formatting in your YAML block. You can add different themes or change the parameters below, but you need to put in the embed-resources: true line true into the YAML block.\n---\ntitle: \"Lab 2 Data Visualization\"\nauthor: \"You\"\nformat:\n  html:\n    toc: true\n    toc_float: true\n    embed-resources: true\nexecute: \n  warning: false\n  message: false\n---\n\n\nInstalling and loading R packages\nIn this course we will work with many different R packages that will need to be installed on your computer. I have already installed most of these packages for students on Posit Cloud. If you are working on your own computer or on Unity, you can install them using Tools &gt; Install Packages. You only need to install a package once!\nTo work with an R package load it with the library command. I always load my packages at the beginning of my files.\n\n\nR code\nlibrary(tidyverse)\n\n\n\n\nData for today’s lab\nIn most labs we will be loading in data from files (e.g. our 23andME SNP data). Today and next week for simplicity we will work with data sets that come with R and the are available as R packages.\n\nData sets (data frames) that come with R\nR contains pre-loaded data sets that will see in many examples posted on the internet. The mtcars and iris data sets are very popular. You can see the whole list by typing data(). This will pop up a window with a list of the data sets. Include #| eval: false within your R code chunk if you want to show but not run code. You can also use the older R Markdown style of including it in the header ```{r eval = FALSE}\n\n\nR code\ndata()\n\n\nIn class we will talk more about the structure of a data set, which can be summarized using the str command\n\n\nR code\nglimpse(iris)\n\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nYou can see the whole data set by typing the name iris or by typing view(iris) which will pop up a window with the data set. However we don’t want to show all 150 observations (rows) of the iris data set in this document. We can use the head command to show just the first 5 rows.\n\n\nR code\nhead(iris)\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\n\n\nData sets that are part of R packages\nR for Data Science uses the palmerpenguins package, “which includes the penguins dataset containing body measurements for penguins on three islands in the Palmer Archipelago, and the ggthemes package, which offers a colorblind safe color palette. We will load these for our work today.” You likely will need to install these packages before loading the libraries.\n\n\nR code\nlibrary(palmerpenguins)\nlibrary(ggthemes)\n\n\nData Analysis and Visualization in R for Ecologists uses the ratdat package, a long-term dataset from Portal, Arizona, in the Chihuahuan desert.\n\n\nR code\nlibrary(ratdat)\n\n\nThe help command can be used to learn more about the palmerpenguins and ratdat packages. After running the below commands, in the right bottom corner under the Help tab the package documentation can be viewed. I used #| eval: false in the below code chunk.\n\n\nR code\nhelp(package=\"palmerpenguins\")\nhelp(package=\"ratdat\")"
  },
  {
    "objectID": "labs/lab2_AI_ggplot.html#exercises",
    "href": "labs/lab2_AI_ggplot.html#exercises",
    "title": "Learning R with the help of AI tools starting graphing using ggplot2",
    "section": "Exercises",
    "text": "Exercises\n\nR for Data Science Chapter 1\nToday we will walk through Chapter 1 of R for Data Science. By putting the examples and exercises in our own Quarto Markdown file, we can create own personal path through the Chapter. Make are readable report by delineating the sections (e.g. 1.2.3 Creating a ggplot) with hashtags so they are visible in your report outline. Include all of the example code in the chapter in your report (In addition to the exercises).\nWorking through the exercises is a great time to explore changing the code with or without Copilot! Answers to all the questions are available online thanks to Martin Lukic and others. I recommend not using these, but learn how to use Copilot to help when your are not sure and to ask me questions during class, help sessions or email.\nIn your report include notes on the places you used Copilot and your prompts. One way to do this would be to have\n\nExercise 1\n\nEx 1 Copilot notes\nThere are probably better ways to do this. Think of one that works for you and clearly communicates to me your strategies.\n\n\nEx 1 code chunk\n\n\n\n\nWhat to upload to Canvas\nAfter you Render the qmd file to an html file, export the file to your computer and upload it to Canvas."
  },
  {
    "objectID": "labs/lab3_tibbles.html",
    "href": "labs/lab3_tibbles.html",
    "title": "Lab 3 : Data Transformation with dplyr",
    "section": "",
    "text": "Data Transformation using dplyr"
  },
  {
    "objectID": "labs/lab3_tibbles.html#learning-objectives",
    "href": "labs/lab3_tibbles.html#learning-objectives",
    "title": "Lab 3 : Data Transformation with dplyr",
    "section": "",
    "text": "Data Transformation using dplyr"
  },
  {
    "objectID": "labs/lab3_tibbles.html#load-libaries",
    "href": "labs/lab3_tibbles.html#load-libaries",
    "title": "Lab 3 : Data Transformation with dplyr",
    "section": "Load libaries",
    "text": "Load libaries\n\n\nR code\nlibrary(tidyverse)\nlibrary(nycflights13)"
  },
  {
    "objectID": "labs/lab3_tibbles.html#introduction-to-data-transformation",
    "href": "labs/lab3_tibbles.html#introduction-to-data-transformation",
    "title": "Lab 3 : Data Transformation with dplyr",
    "section": "Introduction to Data Transformation",
    "text": "Introduction to Data Transformation\n\nTables\nHow they are displayed in your qmd file is different from how they are rendered into a html, pdf and other files.\n\n\nPipes and shortcuts\nIn the last few years |&gt; pipe was introduced as a simpler alternative to the %&gt;% pipe that has been used in R and Tidyverse for the last 10 years. In many online examples you will see the %&gt;% used and at times in code from generative AI. For many uses in this class they are interchangeable.\nThe shortcut keys for generative the |&gt; is Ctrl/Cmd + Shift + M.\nThe shortcut keys for a new R code chuck are trl + Alt + I\n\n\nChecking each line of codes are you write it\nToday we will see in Chapter 4 the following code chunk\n\n\nR code\nflights |&gt;\n  filter(dest == \"IAH\") |&gt; \n  group_by(year, month) |&gt; \n  summarize(\n    arr_delay = mean(arr_delay, na.rm = TRUE)\n  )\n\n\n# A tibble: 12 × 3\n# Groups:   year [1]\n    year month arr_delay\n   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;\n 1  2013     1     4.16 \n 2  2013     2     5.40 \n 3  2013     3    -1.19 \n 4  2013     4    14.8  \n 5  2013     5     0.972\n 6  2013     6    11.1  \n 7  2013     7    11    \n 8  2013     8     0.705\n 9  2013     9   -10.6  \n10  2013    10     1.81 \n11  2013    11    -1.78 \n12  2013    12    14.5  \n\n\nIf I was writing the code I would check (run the code chunk) each line as a wrote it to make sure I was getting the right result and to simplify trouble shooting error messages\n\n\nR code\nflights |&gt;\n  filter(dest == \"IAH\") \n\n\n# A tibble: 7,198 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      623            627        -4      933            932\n 4  2013     1     1      728            732        -4     1041           1038\n 5  2013     1     1      739            739         0     1104           1038\n 6  2013     1     1      908            908         0     1228           1219\n 7  2013     1     1     1028           1026         2     1350           1339\n 8  2013     1     1     1044           1045        -1     1352           1351\n 9  2013     1     1     1114            900       134     1447           1222\n10  2013     1     1     1205           1200         5     1503           1505\n# ℹ 7,188 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\nR code\nflights |&gt;\n  filter(dest == \"IAH\") |&gt; \n  group_by(year, month)\n\n\n# A tibble: 7,198 × 19\n# Groups:   year, month [12]\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      623            627        -4      933            932\n 4  2013     1     1      728            732        -4     1041           1038\n 5  2013     1     1      739            739         0     1104           1038\n 6  2013     1     1      908            908         0     1228           1219\n 7  2013     1     1     1028           1026         2     1350           1339\n 8  2013     1     1     1044           1045        -1     1352           1351\n 9  2013     1     1     1114            900       134     1447           1222\n10  2013     1     1     1205           1200         5     1503           1505\n# ℹ 7,188 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\nR code\nflights |&gt;\n  filter(dest == \"IAH\") |&gt; \n  group_by(year, month) |&gt; \n  summarize(\n    arr_delay = mean(arr_delay, na.rm = TRUE)\n  )\n\n\n# A tibble: 12 × 3\n# Groups:   year [1]\n    year month arr_delay\n   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;\n 1  2013     1     4.16 \n 2  2013     2     5.40 \n 3  2013     3    -1.19 \n 4  2013     4    14.8  \n 5  2013     5     0.972\n 6  2013     6    11.1  \n 7  2013     7    11    \n 8  2013     8     0.705\n 9  2013     9   -10.6  \n10  2013    10     1.81 \n11  2013    11    -1.78 \n12  2013    12    14.5  \n\n\n\n\nAssignment\nIn the first lab with went over assignment of a number or a character sting to a variable\nx &lt;- 2\nThe above code does not create a new variable. After running the code flights is unchanged. This is good in many situations working with large data because we don’t want to be creating new variables that use up more computer memory and it is easier to keep track of fewer variables. If we wish to save the end results, we can assign this to a new variable (e.g. IAH_arr_delay_by_month)\n\n\nR code\nIAH_arr_delay_by_month &lt;- flights |&gt;\n  filter(dest == \"IAH\") |&gt; \n  group_by(year, month) |&gt; \n  summarize(\n    arr_delay = mean(arr_delay, na.rm = TRUE)\n  )\n\n\nNotice that nothing prints out. The new table is put in the data object IAH_arr_delay_by_month. Now you could use this object repeatedly in your code without running the larger code chunk above each time. You can view IAH_arr_delay_by_month by using view(IAH_arr_delay_by_month) or clicking on the object in the Environment window.\n\n\nWriting pseudo code\nWas there a flight on every month of 2013?\nBefore writing any code it is best to break this down into the tasks we need to accomplish\n\nfilter flight data set to the year 2013\nshow only 1 row for each month\ndisplay table to see if each month is present or count to see if rows equal 12\n\nThis is actually the hard part of solving a coding challenge. Writing the codes is relatively easy when you know the steps. This is the greatest challenge in using Generative AI to assist you in coding.\n\n\nR code\nflights |&gt; \n  filter(year == 2013) |&gt; \n  distinct(month)\n\n\n# A tibble: 12 × 1\n   month\n   &lt;int&gt;\n 1     1\n 2    10\n 3    11\n 4    12\n 5     2\n 6     3\n 7     4\n 8     5\n 9     6\n10     7\n11     8\n12     9"
  },
  {
    "objectID": "labs/lab3_tibbles.html#exercises",
    "href": "labs/lab3_tibbles.html#exercises",
    "title": "Lab 3 : Data Transformation with dplyr",
    "section": "Exercises",
    "text": "Exercises\nR for Data Science Chapter 3.\nToday we will walk through Chapter 3 Data Transformation in R for Data Science. As we did last week, by putting the examples and exercises in our own Quarto Markdown file, we can create own personal path through the Chapter.\n\nWhat to upload to Canvas\nAfter you Render the qmd file to an html file, export the file to your computer and upload it to Canvas."
  },
  {
    "objectID": "labs/lab3_data_transformation.html",
    "href": "labs/lab3_data_transformation.html",
    "title": "Lab 3 : Data Transformation with dplyr",
    "section": "",
    "text": "Data Transformation using dplyr"
  },
  {
    "objectID": "labs/lab3_data_transformation.html#learning-objectives",
    "href": "labs/lab3_data_transformation.html#learning-objectives",
    "title": "Lab 3 : Data Transformation with dplyr",
    "section": "",
    "text": "Data Transformation using dplyr"
  },
  {
    "objectID": "labs/lab3_data_transformation.html#load-libaries",
    "href": "labs/lab3_data_transformation.html#load-libaries",
    "title": "Lab 3 : Data Transformation with dplyr",
    "section": "Load libaries",
    "text": "Load libaries\n\n\nR code\nlibrary(tidyverse)\nlibrary(nycflights13)"
  },
  {
    "objectID": "labs/lab3_data_transformation.html#introduction-to-data-transformation",
    "href": "labs/lab3_data_transformation.html#introduction-to-data-transformation",
    "title": "Lab 3 : Data Transformation with dplyr",
    "section": "Introduction to Data Transformation",
    "text": "Introduction to Data Transformation\n\nTables\nHow they are displayed in your qmd file is different from how they are rendered into a html, pdf and other files.\n\n\nPipes and shortcuts\nIn the last few years |&gt; pipe was introduced as a simpler alternative to the %&gt;% pipe that has been used in R and Tidyverse for the last 10 years. In many online examples you will see the %&gt;% used and at times in code from generative AI. For many uses in this class they are interchangeable.\nThe shortcut keys for generative the |&gt; is Ctrl/Cmd + Shift + M.\nThe shortcut keys for a new R code chuck are trl + Alt + I\n\n\nChecking each line of codes are you write it\nToday we will see in Chapter 4 the following code chunk\n\n\nR code\nflights |&gt;\n  filter(dest == \"IAH\") |&gt; \n  group_by(year, month) |&gt; \n  summarize(\n    arr_delay = mean(arr_delay, na.rm = TRUE)\n  )\n\n\n# A tibble: 12 × 3\n# Groups:   year [1]\n    year month arr_delay\n   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;\n 1  2013     1     4.16 \n 2  2013     2     5.40 \n 3  2013     3    -1.19 \n 4  2013     4    14.8  \n 5  2013     5     0.972\n 6  2013     6    11.1  \n 7  2013     7    11    \n 8  2013     8     0.705\n 9  2013     9   -10.6  \n10  2013    10     1.81 \n11  2013    11    -1.78 \n12  2013    12    14.5  \n\n\nIf I was writing the code I would check (run the code chunk) each line as a wrote it to make sure I was getting the right result and to simplify trouble shooting error messages\n\n\nR code\nflights |&gt;\n  filter(dest == \"IAH\") \n\n\n# A tibble: 7,198 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      623            627        -4      933            932\n 4  2013     1     1      728            732        -4     1041           1038\n 5  2013     1     1      739            739         0     1104           1038\n 6  2013     1     1      908            908         0     1228           1219\n 7  2013     1     1     1028           1026         2     1350           1339\n 8  2013     1     1     1044           1045        -1     1352           1351\n 9  2013     1     1     1114            900       134     1447           1222\n10  2013     1     1     1205           1200         5     1503           1505\n# ℹ 7,188 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\nR code\nflights |&gt;\n  filter(dest == \"IAH\") |&gt; \n  group_by(year, month)\n\n\n# A tibble: 7,198 × 19\n# Groups:   year, month [12]\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      623            627        -4      933            932\n 4  2013     1     1      728            732        -4     1041           1038\n 5  2013     1     1      739            739         0     1104           1038\n 6  2013     1     1      908            908         0     1228           1219\n 7  2013     1     1     1028           1026         2     1350           1339\n 8  2013     1     1     1044           1045        -1     1352           1351\n 9  2013     1     1     1114            900       134     1447           1222\n10  2013     1     1     1205           1200         5     1503           1505\n# ℹ 7,188 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\nR code\nflights |&gt;\n  filter(dest == \"IAH\") |&gt; \n  group_by(year, month) |&gt; \n  summarize(\n    arr_delay = mean(arr_delay, na.rm = TRUE)\n  )\n\n\n# A tibble: 12 × 3\n# Groups:   year [1]\n    year month arr_delay\n   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;\n 1  2013     1     4.16 \n 2  2013     2     5.40 \n 3  2013     3    -1.19 \n 4  2013     4    14.8  \n 5  2013     5     0.972\n 6  2013     6    11.1  \n 7  2013     7    11    \n 8  2013     8     0.705\n 9  2013     9   -10.6  \n10  2013    10     1.81 \n11  2013    11    -1.78 \n12  2013    12    14.5  \n\n\n\n\nAssignment\nIn the first lab with went over assignment of a number or a character sting to a variable\nx &lt;- 2\nThe above code does not create a new variable. After running the code flights is unchanged. This is good in many situations working with large data because we don’t want to be creating new variables that use up more computer memory and it is easier to keep track of fewer variables. If we wish to save the end results, we can assign this to a new variable (e.g. IAH_arr_delay_by_month)\n\n\nR code\nIAH_arr_delay_by_month &lt;- flights |&gt;\n  filter(dest == \"IAH\") |&gt; \n  group_by(year, month) |&gt; \n  summarize(\n    arr_delay = mean(arr_delay, na.rm = TRUE)\n  )\n\n\nNotice that nothing prints out. The new table is put in the data object IAH_arr_delay_by_month. Now you could use this object repeatedly in your code without running the larger code chunk above each time. You can view IAH_arr_delay_by_month by using view(IAH_arr_delay_by_month) or clicking on the object in the Environment window.\n\n\nWriting pseudo code\nWas there a flight on every month of 2013?\nBefore writing any code it is best to break this down into the tasks we need to accomplish\n\nfilter flight data set to the year 2013\nshow only 1 row for each month\ndisplay table to see if each month is present or count to see if rows equal 12\n\nThis is actually the hard part of solving a coding challenge. Writing the codes is relatively easy when you know the steps. This is the greatest challenge in using Generative AI to assist you in coding.\n\n\nR code\nflights |&gt; \n  filter(year == 2013) |&gt; \n  distinct(month)\n\n\n# A tibble: 12 × 1\n   month\n   &lt;int&gt;\n 1     1\n 2    10\n 3    11\n 4    12\n 5     2\n 6     3\n 7     4\n 8     5\n 9     6\n10     7\n11     8\n12     9"
  },
  {
    "objectID": "labs/lab3_data_transformation.html#exercises",
    "href": "labs/lab3_data_transformation.html#exercises",
    "title": "Lab 3 : Data Transformation with dplyr",
    "section": "Exercises",
    "text": "Exercises\nR for Data Science Chapter 3.\nToday we will walk through Chapter 3 Data Transformation in R for Data Science. As we did last week, by putting the examples and exercises in our own Quarto Markdown file, we can create own personal path through the Chapter.\n\nWhat to upload to Canvas\nAfter you Render the qmd file to an html file, export the file to your computer and upload it to Canvas."
  },
  {
    "objectID": "labs/lab4_coding_import.html",
    "href": "labs/lab4_coding_import.html",
    "title": "Lab 4 : Coding Basics, Style and Data Read/Write",
    "section": "",
    "text": "Comments\nFunctions\nGood code style\nReading from and writing to files"
  },
  {
    "objectID": "labs/lab4_coding_import.html#learning-objectives",
    "href": "labs/lab4_coding_import.html#learning-objectives",
    "title": "Lab 4 : Coding Basics, Style and Data Read/Write",
    "section": "",
    "text": "Comments\nFunctions\nGood code style\nReading from and writing to files"
  },
  {
    "objectID": "labs/lab4_coding_import.html#exercises",
    "href": "labs/lab4_coding_import.html#exercises",
    "title": "Lab 4 : Coding Basics, Style and Data Read/Write",
    "section": "Exercises",
    "text": "Exercises\nR for Data Science.\nToday we will go through three short chapters in R for Data Science.\n\nChapter 2 Workflow: basics\nChapter 4 Workflow: code style\nChapter 7 Data import.\n\nAs we did last week, put the examples and exercises in a Quarto Markdown file with well organized headings and an outline.\n\nWhat to upload to Canvas\nAfter you Render the qmd file to an html file, export the file to your computer and upload it to Canvas."
  },
  {
    "objectID": "labs/lab4_NEON_tables.html#our-project-space",
    "href": "labs/lab4_NEON_tables.html#our-project-space",
    "title": "Learning R with the help of AI tools starting graphing using ggplot2",
    "section": "Our Project Space",
    "text": "Our Project Space\nNEON has produced metagenomic data as data product since 2014. The Joint Genome Institute has recently annotated version all reads prior to 20222 Gs0144570. However, these metagenomes have a low sequencing depth and therefore are difficult to use for assembling bacterial genomes.\nLast year JGI and NEON collaborated to produce metagenomes which are ~10-fold deeper which allow for better assemble of reads into gene length and greater fragments. We will work with the NEON 2023 data Gs0166454 study sets. By choosing Select Columns for Table we can download metadata associated with each metagenome."
  },
  {
    "objectID": "labs/lab4_NEON_tables.html#on-the-computer",
    "href": "labs/lab4_NEON_tables.html#on-the-computer",
    "title": "Learning R with the help of AI tools starting graphing using ggplot2",
    "section": "On the computer",
    "text": "On the computer"
  },
  {
    "objectID": "labs/EvoGeno_Lab8_tables.html",
    "href": "labs/EvoGeno_Lab8_tables.html",
    "title": "PathoGen",
    "section": "",
    "text": "Introduction to the National Ecological Observatory Network\nIntroduction to the JGI and the Integrated Microbial Genomes and Microbiomes (IMG/MER)\nCreating nicely displayed and interactive tables for reports\nWrangling the NEON data"
  },
  {
    "objectID": "labs/EvoGeno_Lab8_tables.html#learning-objectives",
    "href": "labs/EvoGeno_Lab8_tables.html#learning-objectives",
    "title": "PathoGen",
    "section": "",
    "text": "Introduction to the National Ecological Observatory Network\nIntroduction to the JGI and the Integrated Microbial Genomes and Microbiomes (IMG/MER)\nCreating nicely displayed and interactive tables for reports\nWrangling the NEON data"
  },
  {
    "objectID": "labs/EvoGeno_Lab8_tables.html#background",
    "href": "labs/EvoGeno_Lab8_tables.html#background",
    "title": "PathoGen",
    "section": "Background",
    "text": "Background\nWe will going over the following in class\n\nNEON\nJoint Genome Institute and the Integrated Microbial Genomes and Microbiomes(IMG/MER)\nDOE JGI Metagenome Workflow\nTerabase-scale metagenome coassembly with MetaHipMer\nJGI GOLD database summary of our project"
  },
  {
    "objectID": "labs/EvoGeno_Lab8_tables.html#on-the-computer",
    "href": "labs/EvoGeno_Lab8_tables.html#on-the-computer",
    "title": "PathoGen",
    "section": "On the Computer",
    "text": "On the Computer\n\n\nR code\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(DT)\n\n\n\nkable\nDon’t display tables with thousands of rows. If you do you may run out of memory and/or you will generate a very large html file.\nBy default, R Markdown displays data frames and matrixes as they would be in the R terminal (in a monospaced font). You have seen already what they look like. Here is an example for the iris data set that is preloaded when you start R. We will make a subset of the table.\n\n\nR code\niris_setosa &lt;- iris %&gt;% \nfilter(Species == \"setosa\") %&gt;% \nfilter(Sepal.Length &gt; 5)\n\n\nTo make a table more readable in a report the knitr::kable function works nice for simple customizable tables.\n\n\nR code\nlibrary(knitr)\nkable(iris_setosa)\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n5.4\n3.7\n1.5\n0.2\nsetosa\n\n\n5.8\n4.0\n1.2\n0.2\nsetosa\n\n\n5.7\n4.4\n1.5\n0.4\nsetosa\n\n\n5.4\n3.9\n1.3\n0.4\nsetosa\n\n\n5.1\n3.5\n1.4\n0.3\nsetosa\n\n\n5.7\n3.8\n1.7\n0.3\nsetosa\n\n\n5.1\n3.8\n1.5\n0.3\nsetosa\n\n\n5.4\n3.4\n1.7\n0.2\nsetosa\n\n\n5.1\n3.7\n1.5\n0.4\nsetosa\n\n\n5.1\n3.3\n1.7\n0.5\nsetosa\n\n\n5.2\n3.5\n1.5\n0.2\nsetosa\n\n\n5.2\n3.4\n1.4\n0.2\nsetosa\n\n\n5.4\n3.4\n1.5\n0.4\nsetosa\n\n\n5.2\n4.1\n1.5\n0.1\nsetosa\n\n\n5.5\n4.2\n1.4\n0.2\nsetosa\n\n\n5.5\n3.5\n1.3\n0.2\nsetosa\n\n\n5.1\n3.4\n1.5\n0.2\nsetosa\n\n\n5.1\n3.8\n1.9\n0.4\nsetosa\n\n\n5.1\n3.8\n1.6\n0.2\nsetosa\n\n\n5.3\n3.7\n1.5\n0.2\nsetosa\n\n\n\n\n\nkable works for small tables, but even 22 rows is too much to display in a report. If you have larger tables and/or want to make them interactive, the DT works well.\n\n\nDT\n\n\nR code\nlibrary(DT)\ndatatable(iris_setosa)\n\n\n\n\n\n\nThere are two options for using datatable in your R code chunk. (1) Bound the code chunk you want to present by datatable\n\n\nR code\ndatatable(\n  iris %&gt;% \n    filter(Species == \"setosa\") %&gt;% \n    filter(Sepal.Length &gt; 5)\n)\n\n\n\n\n\n\nOr create a new object\n\n\nR code\niris_setosa &lt;- iris %&gt;% \n  filter(Species == \"setosa\") %&gt;% \n  filter(Sepal.Length &gt; 5)\n\ndatatable(iris_setosa)\n\n\n\n\n\n\n\n\nOther popular table making packages\nHere are a few other popular table making packages\n\nkableExtra\nGT\nreactable\n\n\n\nExamples using the NEON data table\nOur data that we will work with today can be found by searching metagenome bins associated with GOLD Study ID Gs0161344 IMG/MER\nA description of the column headers for the file we will work with\n\nBin ID - Metagenome Assemble Genome (MAG) ID\nGenome Name - The metagenome sample name\nIMG Genome ID - The metagenome sample ID\nBin Quality - An estimate of the quality of the bin or MAG\nBin Lineage - Taxonomic lineage using the JGI system\nGTDB-Tk Taxonomy Lineage - Taxonomic lineage using GTDB\nBin Methods - The methods for binning contigs and quality control\nCreated By - The process by which the bins were created\nDate Added - Date sample/metagenome was added\nBin Completeness - An estimate of the completeness of the MAG\nBin Contamination - An estimate of the contamination of the MAG\nTotal Number of Bases - MAG size in bases\n5s rRNA - Count of 5s rRNAs in the MAG\n16s rRNA - Count of 16S rRNAs in the MAG\n23s rRNA - Count of 23s rRNAs in the MAG\ntRNA Genes - Count of tRNA genes in the MAG\nGene Count - Count of number of genes in the MAG\nScaffold Count - Number of separate scaffold comprising the MAG. The ideal would be 1\nGOLD Study ID - The ID in the JGI GOLD database\n\nLet’s load the table into R\n\n\nR code\nNEON_MAGs &lt;- read_tsv(\"../data/NEON_metadata/exported_img_bins_Gs0166454_NEON.tsv\")\n\n\nAs always in the Environment window check to see if the table loaded as expect and what the object types are. Or you can do it in your R console\n\n\nR code\nhead(NEON_MAGs)\n\n\n# A tibble: 6 × 21\n  bin_oid     `Bin ID` `Genome Name` `IMG Genome ID` `Bin Quality` `Bin Lineage`\n  &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;                   &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;        \n1 3300075492… 3300075… Soil microbi…      3300075492 MQ            Bacteria     \n2 3300075492… 3300075… Soil microbi…      3300075492 MQ            Bacteria     \n3 3300075492… 3300075… Soil microbi…      3300075492 MQ            &lt;NA&gt;         \n4 3300075492… 3300075… Soil microbi…      3300075492 MQ            Bacteria; Ac…\n5 3300075492… 3300075… Soil microbi…      3300075492 MQ            Bacteria; Ac…\n6 3300075492… 3300075… Soil microbi…      3300075492 MQ            Bacteria; Ac…\n# ℹ 15 more variables: `GTDB Taxonomy Lineage` &lt;chr&gt;, `Bin Methods` &lt;chr&gt;,\n#   `Created By` &lt;chr&gt;, `Date Added` &lt;date&gt;, `Bin Completeness` &lt;dbl&gt;,\n#   `Bin Contamination` &lt;dbl&gt;, `Average Coverage` &lt;lgl&gt;,\n#   `Total Number of Bases` &lt;dbl&gt;, `5s rRNA` &lt;dbl&gt;, `16s rRNA` &lt;dbl&gt;,\n#   `23s rRNA` &lt;dbl&gt;, `tRNA Genes` &lt;dbl&gt;, `Gene Count` &lt;dbl&gt;,\n#   `Scaffold Count` &lt;dbl&gt;, `GOLD Study ID` &lt;chr&gt;\n\n\nR code\nstr(NEON_MAGs)\n\n\nspc_tbl_ [16,669 × 21] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ bin_oid              : chr [1:16669] \"3300075492_s0\" \"3300075492_s1\" \"3300075492_s100\" \"3300075492_s106\" ...\n $ Bin ID               : chr [1:16669] \"3300075492_s0\" \"3300075492_s1\" \"3300075492_s100\" \"3300075492_s106\" ...\n $ Genome Name          : chr [1:16669] \"Soil microbial communities from University of Notre Dame Environmental Research Center NEON Field Site, Michiga\"| __truncated__ \"Soil microbial communities from University of Notre Dame Environmental Research Center NEON Field Site, Michiga\"| __truncated__ \"Soil microbial communities from University of Notre Dame Environmental Research Center NEON Field Site, Michiga\"| __truncated__ \"Soil microbial communities from University of Notre Dame Environmental Research Center NEON Field Site, Michiga\"| __truncated__ ...\n $ IMG Genome ID        : num [1:16669] 3.3e+09 3.3e+09 3.3e+09 3.3e+09 3.3e+09 ...\n $ Bin Quality          : chr [1:16669] \"MQ\" \"MQ\" \"MQ\" \"MQ\" ...\n $ Bin Lineage          : chr [1:16669] \"Bacteria\" \"Bacteria\" NA \"Bacteria; Actinomycetota; Thermoleophilia; Solirubrobacterales\" ...\n $ GTDB Taxonomy Lineage: chr [1:16669] \"Bacteria; Acidobacteriota; Terriglobia; Acidoferrales; UBA7541; Acidoferrum\" \"Bacteria; Desulfobacterota_B; Binatia; Binatales; Binataceae; Binatus; Binatus soli\" \"Archaea; Thermoplasmatota; Thermoplasmata; UBA184; UBA184; UBA184\" \"Bacteria; Actinomycetota; Thermoleophilia; Solirubrobacterales; Solirubrobacteraceae; Palsa-744\" ...\n $ Bin Methods          : chr [1:16669] \"SemiBin2:v2.1.0, CheckM2:v1.0.2, GTDB-Tk:v2.4.0, GTDB-Tk-database:release220\" \"SemiBin2:v2.1.0, CheckM2:v1.0.2, GTDB-Tk:v2.4.0, GTDB-Tk-database:release220\" \"SemiBin2:v2.1.0, CheckM2:v1.0.2, GTDB-Tk:v2.4.0, GTDB-Tk-database:release220\" \"SemiBin2:v2.1.0, CheckM2:v1.0.2, GTDB-Tk:v2.4.0, GTDB-Tk-database:release220\" ...\n $ Created By           : chr [1:16669] \"IMG_PIPELINE\" \"IMG_PIPELINE\" \"IMG_PIPELINE\" \"IMG_PIPELINE\" ...\n $ Date Added           : Date[1:16669], format: \"2025-01-23\" \"2025-01-23\" ...\n $ Bin Completeness     : num [1:16669] 95.9 99.6 57.9 74 67.3 ...\n $ Bin Contamination    : num [1:16669] 6.31 0 2.39 8.19 3.95 6.54 5.24 0.63 0.61 4.56 ...\n $ Average Coverage     : logi [1:16669] NA NA NA NA NA NA ...\n $ Total Number of Bases: num [1:16669] 5600425 3706224 1233791 2084993 3809196 ...\n $ 5s rRNA              : num [1:16669] 1 1 1 1 0 0 0 1 1 0 ...\n $ 16s rRNA             : num [1:16669] 2 0 0 1 0 0 0 0 0 0 ...\n $ 23s rRNA             : num [1:16669] 2 1 1 1 0 0 0 1 0 0 ...\n $ tRNA Genes           : num [1:16669] 57 51 30 33 31 33 29 25 30 30 ...\n $ Gene Count           : num [1:16669] 4976 3617 1344 2284 3848 ...\n $ Scaffold Count       : num [1:16669] 26 45 210 322 398 85 527 435 243 409 ...\n $ GOLD Study ID        : chr [1:16669] \"Gs0166454\" \"Gs0166454\" \"Gs0166454\" \"Gs0166454\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   bin_oid = col_character(),\n  ..   `Bin ID` = col_character(),\n  ..   `Genome Name` = col_character(),\n  ..   `IMG Genome ID` = col_double(),\n  ..   `Bin Quality` = col_character(),\n  ..   `Bin Lineage` = col_character(),\n  ..   `GTDB Taxonomy Lineage` = col_character(),\n  ..   `Bin Methods` = col_character(),\n  ..   `Created By` = col_character(),\n  ..   `Date Added` = col_date(format = \"\"),\n  ..   `Bin Completeness` = col_double(),\n  ..   `Bin Contamination` = col_double(),\n  ..   `Average Coverage` = col_logical(),\n  ..   `Total Number of Bases` = col_double(),\n  ..   `5s rRNA` = col_double(),\n  ..   `16s rRNA` = col_double(),\n  ..   `23s rRNA` = col_double(),\n  ..   `tRNA Genes` = col_double(),\n  ..   `Gene Count` = col_double(),\n  ..   `Scaffold Count` = col_double(),\n  ..   `GOLD Study ID` = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nToday we are just going to work from the assembly of the individual metagenomes, so let’s remove the MAGs from the combined assembly. When working with variable names that have spaces or special characters enclose them with grave accent that looks like a single quote ’ ’, but is not Note that single and double quotes delimit character strings in your variables. They can be used interchangeably but double quotes are preferred, so single quotes are normally only used to delimit character strings containing double quotes.\nCount the number of MQ and HQ genomes\n\n\nR code\nNEON_MAGs %&gt;% \n  count(`Bin Quality`, sort = TRUE) \n\n\n# A tibble: 2 × 2\n  `Bin Quality`     n\n  &lt;chr&gt;         &lt;int&gt;\n1 MQ            15369\n2 HQ             1300\n\n\nMake a knitr::kable table of the bin quality counts\n\n\nR code\nkable(\n  NEON_MAGs %&gt;% \n   count(`Bin Quality`) \n)\n\n\n\n\n\nBin Quality\nn\n\n\n\n\nHQ\n1300\n\n\nMQ\n15369\n\n\n\n\n\nFilter so that Bin Quality = HQ and display in DT::datatable\n\n\nR code\ndatatable(\n  NEON_MAGs%&gt;% \n    filter(`Bin Quality` == \"HQ\")\n)\n\n\n\n\n\n\nSelect the GTDB taxonomy and the MAGs genome size then filter to all MAGs greater than 10,000,000 bases\n\n\nR code\nkable(\nNEON_MAGs %&gt;% \n  select(c(`GTDB Taxonomy Lineage`, `Total Number of Bases`)) %&gt;% \n  filter(`Total Number of Bases` &gt; 10000000)\n)\n\n\n\n\n\n\n\n\n\nGTDB Taxonomy Lineage\nTotal Number of Bases\n\n\n\n\nBacteria; Pseudomonadota; Gammaproteobacteria; Steroidobacterales; Steroidobacteraceae; 13-2-20CM-66-19\n10115899\n\n\nBacteria; Planctomycetota; Planctomycetia; Isosphaerales; Isosphaeraceae\n11932805\n\n\nBacteria; Actinomycetota; Actinomycetes; Streptomycetales; Catenulisporaceae; Catenulispora\n12420050\n\n\nBacteria; Planctomycetota; Planctomycetia; Isosphaerales; Isosphaeraceae; JAJPJC01\n13151516\n\n\nBacteria; Planctomycetota; Planctomycetia; Isosphaerales; Isosphaeraceae\n12046820\n\n\nBacteria; Planctomycetota; Planctomycetia; Isosphaerales; Isosphaeraceae; JAQGHR01\n12217509\n\n\nBacteria; Planctomycetota; Planctomycetia; Isosphaerales; Isosphaeraceae\n12942605\n\n\nBacteria; Actinomycetota; Actinomycetes; Mycobacteriales; Pseudonocardiaceae; Actinophytocola\n11293845\n\n\nBacteria; Planctomycetota; Planctomycetia; Isosphaerales; Isosphaeraceae\n14411455\n\n\nBacteria; Planctomycetota; Planctomycetia; Isosphaerales; Isosphaeraceae\n10890912\n\n\nBacteria; Planctomycetota; Planctomycetia; Pirellulales; JAICIG01; JAICLL01\n11379784\n\n\nBacteria; Acidobacteriota; Terriglobia; Bryobacterales; Bryobacteraceae; PALSA-243\n10475466\n\n\nNA\n10248256\n\n\nBacteria; Pseudomonadota; Alphaproteobacteria; Rhizobiales; Xanthobacteraceae\n10436042\n\n\nBacteria; Planctomycetota; Planctomycetia; Gemmatales; Gemmataceae\n13087515\n\n\nBacteria; Actinomycetota; Actinomycetes; Mycobacteriales; Pseudonocardiaceae; Actinophytocola\n10431101\n\n\nBacteria; Myxococcota; Polyangia; Polyangiales; JAFGIB01\n10030262\n\n\nBacteria; Myxococcota; Polyangia; Polyangiales; Polyangiaceae; JANYGI01\n12560711\n\n\nBacteria; Planctomycetota; Planctomycetia; Isosphaerales; Isosphaeraceae\n11026377\n\n\nBacteria; Planctomycetota; Planctomycetia; Gemmatales; Gemmataceae\n12479517\n\n\nBacteria; Acidobacteriota; Terriglobia; Bryobacterales; Bryobacteraceae; Solibacter\n10617884\n\n\nBacteria; Planctomycetota; Planctomycetia; Isosphaerales; Isosphaeraceae; JAQGHR01\n10431455\n\n\nBacteria; Planctomycetota; Planctomycetia; Isosphaerales; Isosphaeraceae\n12864765\n\n\nBacteria; Actinomycetota; Actinomycetes; Mycobacteriales; Mycobacteriaceae; Mycobacterium\n10583130\n\n\n\n\n\nWe can use the stringr package to filter based on a word or string of characters in a column\n\n\nR code\ndatatable(\nNEON_MAGs %&gt;% \n  filter(str_detect(`GTDB Taxonomy Lineage`, 'Bacteroidota'))\n)\n\n\n\n\n\n\nFilter to include only the samples from Yellowstone NP\n\n\nR code\ndatatable(\nNEON_MAGs %&gt;% \n  filter(str_detect(`Genome Name`, 'Yellowstone NP'))\n)\n\n\n\n\n\n\nSince the the taxonomic categories in GTDB-Tk Taxonomy Lineage are separated by the ; we can use the separate function to create new columns for each of the taxonomic categories. The remove = FALSE keeps the original GTDB-Tk Taxonomy Lineage column\n\n\nR code\nNEON_MAGs_tax &lt;- NEON_MAGs %&gt;% \n  separate(`GTDB Taxonomy Lineage`, c(\"Domain\", \"Phylum\", \"Class\", \"Order\", \"Family\", \"Genus\"), \"; \", remove = FALSE) \n\n\nCount the number of MAGs for each Phylum and display in DT::datatable\n\n\nR code\ndatatable(\n  NEON_MAGs_tax %&gt;% \n    count(Phylum, sort = TRUE)\n)\n\n\n\n\n\n\nNote that there is one category with no name. This were MAGs that were not annotated by GTDB using the JGI pipline (they are Archaea)\nThere is a lot of information in Genome Name. Let’s unpack it into separate columns. Note here where the double quotes and grave accents are used\n\n\nR code\nNEON_MAGs_tax_sample &lt;- NEON_MAGs_tax %&gt;% \n  # Get rid of the the common string \"Soil microbial communities from \"\n  mutate_at(\"Genome Name\", str_replace, \"Terrestrial soil microbial communities from \", \"\") %&gt;% \n  # Use the first `-` to split the column in two\n  separate(`Genome Name`, c(\"Site\",\"Sample Name\"), \" - \") %&gt;% \n  # Get rid of the the common string \"S-comp-1\"\n  mutate_at(\"Sample Name\", str_replace, \"-comp-1\", \"\") %&gt;%\n  # separate the Sample Name into Site ID and plot info\n  separate(`Sample Name`, c(\"Site ID\",\"subplot.layer.date\"), \"_\", remove = FALSE,) %&gt;% \n  # separate the plot info into 3 columns\n  separate(`subplot.layer.date`, c(\"Subplot\", \"Layer\", \"Date\"), \"-\") \n\n\nCheck this out in the Environment window to make sure with got it right.\nLet’s see which Site has the most MAGs\n\n\nR code\ndatatable(\n  NEON_MAGs_tax_sample %&gt;% \n    count(Site, sort = TRUE)\n)"
  },
  {
    "objectID": "labs/EvoGeno_Lab8_tables.html#exercises",
    "href": "labs/EvoGeno_Lab8_tables.html#exercises",
    "title": "PathoGen",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1\nUse view(iris) to see the whole data table. Subset the table based on a different species than was used in the example. Display the table using DT::datatable\n\n\nExercise 2\nDisplay using DT::datatable the NEON MAGs from the individual assemblies that have at least 1 16S rRNA\n\n\nExercise 3\nDisplay a table of the MAGs from Lower Teakettle with only the columns for the Genome Name, GTDB-Tk Taxonomy Lineage, and estimated MAG genome size.\n\n\nExercise 4\nDisplay a table with the Class counts at LBJ National Grasslands\n\n\nExercise 5\nDisplay a table with the counts for the Phylum Actinobacteriota at each Site"
  },
  {
    "objectID": "labs/lab5_NEON_tables.html#our-project-space",
    "href": "labs/lab5_NEON_tables.html#our-project-space",
    "title": "Lab 5 : Working with NEON Metagenome Assembled Genomes (MAGs / bins)",
    "section": "Our Project Space",
    "text": "Our Project Space\nNEON has produced metagenomic data as data product since 2014. The Joint Genome Institute has recently annotated version all reads prior to 20222 Gs0144570. However, these metagenomes have a low sequencing depth and therefore are difficult to use for assembling bacterial genomes.\nLast year JGI and NEON collaborated to produce metagenomes which are ~10-fold deeper which allow for better assemble of reads into gene length and greater fragments. We will work with the NEON 2023 and 2024 data Gs0166454 study sets."
  },
  {
    "objectID": "labs/lab5_NEON_tables.html#on-the-computer",
    "href": "labs/lab5_NEON_tables.html#on-the-computer",
    "title": "Lab 5 : Working with NEON Metagenome Assembled Genomes (MAGs / bins)",
    "section": "On the Computer",
    "text": "On the Computer\n\n\nR code\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(janitor)\nlibrary(DT)\n\n\n\nkable\nDon’t display tables with thousands of rows. If you do you may run out of memory and/or you will generate a very large html file.\nBy default, Quarto displays data frames and matrixes as they would be in the R terminal (in a monospaced font). You have seen already what they look like. Here is an example for the iris data set that is preloaded when you start R. We will make a subset of the table.\n\n\nR code\niris_setosa &lt;- iris |&gt; \nfilter(Species == \"setosa\") |&gt; \nfilter(Sepal.Length &gt; 5)\n\n\nTo make a table more readable in a report the knitr::kable function works nice for simple customizable tables.\n\n\nR code\nlibrary(knitr)\nkable(iris_setosa)\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n5.4\n3.7\n1.5\n0.2\nsetosa\n\n\n5.8\n4.0\n1.2\n0.2\nsetosa\n\n\n5.7\n4.4\n1.5\n0.4\nsetosa\n\n\n5.4\n3.9\n1.3\n0.4\nsetosa\n\n\n5.1\n3.5\n1.4\n0.3\nsetosa\n\n\n5.7\n3.8\n1.7\n0.3\nsetosa\n\n\n5.1\n3.8\n1.5\n0.3\nsetosa\n\n\n5.4\n3.4\n1.7\n0.2\nsetosa\n\n\n5.1\n3.7\n1.5\n0.4\nsetosa\n\n\n5.1\n3.3\n1.7\n0.5\nsetosa\n\n\n5.2\n3.5\n1.5\n0.2\nsetosa\n\n\n5.2\n3.4\n1.4\n0.2\nsetosa\n\n\n5.4\n3.4\n1.5\n0.4\nsetosa\n\n\n5.2\n4.1\n1.5\n0.1\nsetosa\n\n\n5.5\n4.2\n1.4\n0.2\nsetosa\n\n\n5.5\n3.5\n1.3\n0.2\nsetosa\n\n\n5.1\n3.4\n1.5\n0.2\nsetosa\n\n\n5.1\n3.8\n1.9\n0.4\nsetosa\n\n\n5.1\n3.8\n1.6\n0.2\nsetosa\n\n\n5.3\n3.7\n1.5\n0.2\nsetosa\n\n\n\n\n\nkable works for small tables, but even 22 rows is too much to display in a report. If you have larger tables and/or want to make them interactive, the DT works well.\n\n\nDT\n\n\nR code\nlibrary(DT)\ndatatable(iris_setosa)\n\n\n\n\n\n\nThere are two options for using datatable in your R code chunk. (1) Bound the code chunk you want to present by datatable\n\n\nR code\ndatatable(\n  iris |&gt; \n    filter(Species == \"setosa\") |&gt; \n    filter(Sepal.Length &gt; 5)\n)\n\n\n\n\n\n\nOr create a new object\n\n\nR code\niris_setosa &lt;- iris |&gt; \n  filter(Species == \"setosa\") |&gt; \n  filter(Sepal.Length &gt; 5)\n\ndatatable(iris_setosa)\n\n\n\n\n\n\n\n\nOther popular table making packages\nHere are a few other popular table making packages\n\nkableExtra\nGT\nreactable\n\n\n\nExamples using the NEON data table\nOur data that we will work with today can be found by searching metagenome bins associated with GOLD Study ID Gs0166454 IMG/MER\nA description of the column headers for the file we will work with\n\nBin ID - Metagenome Assemble Genome (MAG) ID\nGenome Name - The metagenome sample name\nIMG Genome ID - The metagenome sample ID\nBin Quality - An estimate of the quality of the bin or MAG\nBin Lineage - Taxonomic lineage using the JGI system\nGTDB-Tk Taxonomy Lineage - Taxonomic lineage using GTDB\nBin Methods - The methods for binning contigs and quality control\nCreated By - The process by which the bins were created\nDate Added - Date sample/metagenome was added\nBin Completeness - An estimate of the completeness of the MAG\nBin Contamination - An estimate of the contamination of the MAG\nTotal Number of Bases - MAG size in bases\n5s rRNA - Count of 5s rRNAs in the MAG\n16s rRNA - Count of 16S rRNAs in the MAG\n23s rRNA - Count of 23s rRNAs in the MAG\ntRNA Genes - Count of tRNA genes in the MAG\nGene Count - Count of number of genes in the MAG\nScaffold Count - Number of separate scaffold comprising the MAG. The ideal would be 1\nGOLD Study ID - The ID in the JGI GOLD database\n\nLet’s load the table into R\n\n\nR code\n# This is the location used for Github\nNEON_MAGs &lt;- read_tsv(\"../data/NEON_metadata/exported_img_bins_Gs0166454_NEON.tsv\")\n# This is the location used for the class data directory on Unity\n# NEON_MAGs &lt;- read_tsv(\"/work/pi_bio678_umass_edu/data_NEON/exported_img_bins_Gs0166454_NEON.tsv\")\n\n\nAs always in the Environment window check to see if the table loaded as expect and what the object types are. Or you can do it in your R console\n\n\nR code\nhead(NEON_MAGs)\n\n\n# A tibble: 6 × 21\n  bin_oid     `Bin ID` `Genome Name` `IMG Genome ID` `Bin Quality` `Bin Lineage`\n  &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;                   &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;        \n1 3300075492… 3300075… Soil microbi…      3300075492 MQ            Bacteria     \n2 3300075492… 3300075… Soil microbi…      3300075492 MQ            Bacteria     \n3 3300075492… 3300075… Soil microbi…      3300075492 MQ            &lt;NA&gt;         \n4 3300075492… 3300075… Soil microbi…      3300075492 MQ            Bacteria; Ac…\n5 3300075492… 3300075… Soil microbi…      3300075492 MQ            Bacteria; Ac…\n6 3300075492… 3300075… Soil microbi…      3300075492 MQ            Bacteria; Ac…\n# ℹ 15 more variables: `GTDB Taxonomy Lineage` &lt;chr&gt;, `Bin Methods` &lt;chr&gt;,\n#   `Created By` &lt;chr&gt;, `Date Added` &lt;date&gt;, `Bin Completeness` &lt;dbl&gt;,\n#   `Bin Contamination` &lt;dbl&gt;, `Average Coverage` &lt;lgl&gt;,\n#   `Total Number of Bases` &lt;dbl&gt;, `5s rRNA` &lt;dbl&gt;, `16s rRNA` &lt;dbl&gt;,\n#   `23s rRNA` &lt;dbl&gt;, `tRNA Genes` &lt;dbl&gt;, `Gene Count` &lt;dbl&gt;,\n#   `Scaffold Count` &lt;dbl&gt;, `GOLD Study ID` &lt;chr&gt;\n\n\nR code\nstr(NEON_MAGs)\n\n\nspc_tbl_ [16,669 × 21] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ bin_oid              : chr [1:16669] \"3300075492_s0\" \"3300075492_s1\" \"3300075492_s100\" \"3300075492_s106\" ...\n $ Bin ID               : chr [1:16669] \"3300075492_s0\" \"3300075492_s1\" \"3300075492_s100\" \"3300075492_s106\" ...\n $ Genome Name          : chr [1:16669] \"Soil microbial communities from University of Notre Dame Environmental Research Center NEON Field Site, Michiga\"| __truncated__ \"Soil microbial communities from University of Notre Dame Environmental Research Center NEON Field Site, Michiga\"| __truncated__ \"Soil microbial communities from University of Notre Dame Environmental Research Center NEON Field Site, Michiga\"| __truncated__ \"Soil microbial communities from University of Notre Dame Environmental Research Center NEON Field Site, Michiga\"| __truncated__ ...\n $ IMG Genome ID        : num [1:16669] 3.3e+09 3.3e+09 3.3e+09 3.3e+09 3.3e+09 ...\n $ Bin Quality          : chr [1:16669] \"MQ\" \"MQ\" \"MQ\" \"MQ\" ...\n $ Bin Lineage          : chr [1:16669] \"Bacteria\" \"Bacteria\" NA \"Bacteria; Actinomycetota; Thermoleophilia; Solirubrobacterales\" ...\n $ GTDB Taxonomy Lineage: chr [1:16669] \"Bacteria; Acidobacteriota; Terriglobia; Acidoferrales; UBA7541; Acidoferrum\" \"Bacteria; Desulfobacterota_B; Binatia; Binatales; Binataceae; Binatus; Binatus soli\" \"Archaea; Thermoplasmatota; Thermoplasmata; UBA184; UBA184; UBA184\" \"Bacteria; Actinomycetota; Thermoleophilia; Solirubrobacterales; Solirubrobacteraceae; Palsa-744\" ...\n $ Bin Methods          : chr [1:16669] \"SemiBin2:v2.1.0, CheckM2:v1.0.2, GTDB-Tk:v2.4.0, GTDB-Tk-database:release220\" \"SemiBin2:v2.1.0, CheckM2:v1.0.2, GTDB-Tk:v2.4.0, GTDB-Tk-database:release220\" \"SemiBin2:v2.1.0, CheckM2:v1.0.2, GTDB-Tk:v2.4.0, GTDB-Tk-database:release220\" \"SemiBin2:v2.1.0, CheckM2:v1.0.2, GTDB-Tk:v2.4.0, GTDB-Tk-database:release220\" ...\n $ Created By           : chr [1:16669] \"IMG_PIPELINE\" \"IMG_PIPELINE\" \"IMG_PIPELINE\" \"IMG_PIPELINE\" ...\n $ Date Added           : Date[1:16669], format: \"2025-01-23\" \"2025-01-23\" ...\n $ Bin Completeness     : num [1:16669] 95.9 99.6 57.9 74 67.3 ...\n $ Bin Contamination    : num [1:16669] 6.31 0 2.39 8.19 3.95 6.54 5.24 0.63 0.61 4.56 ...\n $ Average Coverage     : logi [1:16669] NA NA NA NA NA NA ...\n $ Total Number of Bases: num [1:16669] 5600425 3706224 1233791 2084993 3809196 ...\n $ 5s rRNA              : num [1:16669] 1 1 1 1 0 0 0 1 1 0 ...\n $ 16s rRNA             : num [1:16669] 2 0 0 1 0 0 0 0 0 0 ...\n $ 23s rRNA             : num [1:16669] 2 1 1 1 0 0 0 1 0 0 ...\n $ tRNA Genes           : num [1:16669] 57 51 30 33 31 33 29 25 30 30 ...\n $ Gene Count           : num [1:16669] 4976 3617 1344 2284 3848 ...\n $ Scaffold Count       : num [1:16669] 26 45 210 322 398 85 527 435 243 409 ...\n $ GOLD Study ID        : chr [1:16669] \"Gs0166454\" \"Gs0166454\" \"Gs0166454\" \"Gs0166454\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   bin_oid = col_character(),\n  ..   `Bin ID` = col_character(),\n  ..   `Genome Name` = col_character(),\n  ..   `IMG Genome ID` = col_double(),\n  ..   `Bin Quality` = col_character(),\n  ..   `Bin Lineage` = col_character(),\n  ..   `GTDB Taxonomy Lineage` = col_character(),\n  ..   `Bin Methods` = col_character(),\n  ..   `Created By` = col_character(),\n  ..   `Date Added` = col_date(format = \"\"),\n  ..   `Bin Completeness` = col_double(),\n  ..   `Bin Contamination` = col_double(),\n  ..   `Average Coverage` = col_logical(),\n  ..   `Total Number of Bases` = col_double(),\n  ..   `5s rRNA` = col_double(),\n  ..   `16s rRNA` = col_double(),\n  ..   `23s rRNA` = col_double(),\n  ..   `tRNA Genes` = col_double(),\n  ..   `Gene Count` = col_double(),\n  ..   `Scaffold Count` = col_double(),\n  ..   `GOLD Study ID` = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nYou might also notice that many columns are surrounded by backticks. That’s because they contain spaces, breaking R’s usual rules for variable names; they’re non-syntactic names. To refer to these variables, you need to surround them with backticks, `. Last week in Chapter 7 we used janitor::clean_names() to use some heuristics to turn them all into snake case at once.\n\n\nR code\nNEON_MAGs &lt;- NEON_MAGs |&gt; janitor::clean_names()\n\n\nCount the number of MQ and HQ genomes\n\n\nR code\nNEON_MAGs |&gt; \n  count(bin_quality, sort = TRUE) \n\n\n# A tibble: 2 × 2\n  bin_quality     n\n  &lt;chr&gt;       &lt;int&gt;\n1 MQ          15369\n2 HQ           1300\n\n\nMake a knitr::kable table of the bin quality counts\n\n\nR code\nkable(\n  NEON_MAGs |&gt; \n   count(bin_quality) \n)\n\n\n\n\n\nbin_quality\nn\n\n\n\n\nHQ\n1300\n\n\nMQ\n15369\n\n\n\n\n\nFilter so that Bin Quality = HQ and display in DT::datatable\n\n\nR code\ndatatable(\n  NEON_MAGs|&gt; \n    filter(bin_quality == \"HQ\")\n)\n\n\n\n\n\n\nSelect the GTDB taxonomy and the MAGs genome size then filter to all MAGs greater than 10,000,000 bases\n\n\nR code\nkable(\nNEON_MAGs |&gt; \n  select(c(gtdb_taxonomy_lineage, total_number_of_bases)) |&gt; \n  filter(total_number_of_bases &gt; 10000000)\n)\n\n\n\n\n\n\n\n\n\ngtdb_taxonomy_lineage\ntotal_number_of_bases\n\n\n\n\nBacteria; Pseudomonadota; Gammaproteobacteria; Steroidobacterales; Steroidobacteraceae; 13-2-20CM-66-19\n10115899\n\n\nBacteria; Planctomycetota; Planctomycetia; Isosphaerales; Isosphaeraceae\n11932805\n\n\nBacteria; Actinomycetota; Actinomycetes; Streptomycetales; Catenulisporaceae; Catenulispora\n12420050\n\n\nBacteria; Planctomycetota; Planctomycetia; Isosphaerales; Isosphaeraceae; JAJPJC01\n13151516\n\n\nBacteria; Planctomycetota; Planctomycetia; Isosphaerales; Isosphaeraceae\n12046820\n\n\nBacteria; Planctomycetota; Planctomycetia; Isosphaerales; Isosphaeraceae; JAQGHR01\n12217509\n\n\nBacteria; Planctomycetota; Planctomycetia; Isosphaerales; Isosphaeraceae\n12942605\n\n\nBacteria; Actinomycetota; Actinomycetes; Mycobacteriales; Pseudonocardiaceae; Actinophytocola\n11293845\n\n\nBacteria; Planctomycetota; Planctomycetia; Isosphaerales; Isosphaeraceae\n14411455\n\n\nBacteria; Planctomycetota; Planctomycetia; Isosphaerales; Isosphaeraceae\n10890912\n\n\nBacteria; Planctomycetota; Planctomycetia; Pirellulales; JAICIG01; JAICLL01\n11379784\n\n\nBacteria; Acidobacteriota; Terriglobia; Bryobacterales; Bryobacteraceae; PALSA-243\n10475466\n\n\nNA\n10248256\n\n\nBacteria; Pseudomonadota; Alphaproteobacteria; Rhizobiales; Xanthobacteraceae\n10436042\n\n\nBacteria; Planctomycetota; Planctomycetia; Gemmatales; Gemmataceae\n13087515\n\n\nBacteria; Actinomycetota; Actinomycetes; Mycobacteriales; Pseudonocardiaceae; Actinophytocola\n10431101\n\n\nBacteria; Myxococcota; Polyangia; Polyangiales; JAFGIB01\n10030262\n\n\nBacteria; Myxococcota; Polyangia; Polyangiales; Polyangiaceae; JANYGI01\n12560711\n\n\nBacteria; Planctomycetota; Planctomycetia; Isosphaerales; Isosphaeraceae\n11026377\n\n\nBacteria; Planctomycetota; Planctomycetia; Gemmatales; Gemmataceae\n12479517\n\n\nBacteria; Acidobacteriota; Terriglobia; Bryobacterales; Bryobacteraceae; Solibacter\n10617884\n\n\nBacteria; Planctomycetota; Planctomycetia; Isosphaerales; Isosphaeraceae; JAQGHR01\n10431455\n\n\nBacteria; Planctomycetota; Planctomycetia; Isosphaerales; Isosphaeraceae\n12864765\n\n\nBacteria; Actinomycetota; Actinomycetes; Mycobacteriales; Mycobacteriaceae; Mycobacterium\n10583130\n\n\n\n\n\nWe can use the stringr package to filter based on a word or string of characters in a column\n\n\nR code\ndatatable(\nNEON_MAGs |&gt; \n  filter(str_detect(gtdb_taxonomy_lineage, 'Bacteroidota'))\n)\n\n\n\n\n\n\nFilter to include only the samples from Yellowstone NP\n\n\nR code\ndatatable(\nNEON_MAGs |&gt; \n  filter(str_detect(genome_name, 'Yellowstone NP'))\n)\n\n\n\n\n\n\nSince the the taxonomic categories in GTDB-Tk Taxonomy Lineage are separated by the ; we can use the separate function to create new columns for each of the taxonomic categories. The remove = FALSE keeps the original GTDB-Tk Taxonomy Lineage column\n\n\nR code\nNEON_MAGs_tax &lt;- NEON_MAGs |&gt; \n  separate(gtdb_taxonomy_lineage, c(\"domain\", \"phylum\", \"class\", \"order\", \"family\", \"genus\"), \"; \", remove = FALSE) \n\n\nCount the number of MAGs for each Phylum and display in DT::datatable\n\n\nR code\ndatatable(\n  NEON_MAGs_tax |&gt; \n    count(phylum, sort = TRUE)\n)\n\n\n\n\n\n\nNote that there is one category with no name. This were MAGs that were not annotated by GTDB using the JGI pipline (they are Archaea)\nThere is a lot of information in genome_name. Let’s unpack it into separate columns. Note here where the double quotes and grave accents are used\n\n\nR code\nNEON_MAGs_tax_sample &lt;- NEON_MAGs_tax |&gt; \n  # Get rid of the the common string \"Soil microbial communities from \"\n  mutate_at(\"genome_name\", str_replace, \"Terrestrial soil microbial communities from \", \"\") |&gt; \n  # Use the first `-` to split the column in two\n  separate(genome_name, c(\"site\",\"sample_name\"), \" - \") |&gt; \n  # Get rid of the the common string \"S-comp-1\"\n  mutate_at(\"sample_name\", str_replace, \"-comp-1\", \"\") |&gt;\n  # separate the Sample Name into Site ID and plot info\n  separate(sample_name, c(\"site_ID\",\"subplot.layer.date\"), \"_\", remove = FALSE,) |&gt; \n  # separate the plot info into 3 columns\n  separate(`subplot.layer.date`, c(\"subplot\", \"layer\", \"date\"), \"-\") \n\n\nCheck this out in the Environment window to make sure with got it right.\nLet’s see which Site has the most MAGs\n\n\nR code\ndatatable(\n  NEON_MAGs_tax_sample |&gt; \n    count(site, sort = TRUE)\n)"
  },
  {
    "objectID": "labs/lab5_NEON_tables.html",
    "href": "labs/lab5_NEON_tables.html",
    "title": "Lab 5 : Working with NEON Metagenome Assembled Genomes (MAGs / bins)",
    "section": "",
    "text": "Introduction to the National Ecological Observatory Network\nIntroduction to the JGI and the Integrated Microbial Genomes and Microbiomes (IMG/MER)\nCreating nicely displayed and interactive tables for reports\nWrangling the NEON data"
  },
  {
    "objectID": "labs/lab5_NEON_tables.html#learning-objectives",
    "href": "labs/lab5_NEON_tables.html#learning-objectives",
    "title": "Lab 5 : Working with NEON Metagenome Assembled Genomes (MAGs / bins)",
    "section": "",
    "text": "Introduction to the National Ecological Observatory Network\nIntroduction to the JGI and the Integrated Microbial Genomes and Microbiomes (IMG/MER)\nCreating nicely displayed and interactive tables for reports\nWrangling the NEON data"
  },
  {
    "objectID": "labs/lab5_NEON_tables.html#background",
    "href": "labs/lab5_NEON_tables.html#background",
    "title": "Lab 5 : Working with NEON Metagenome Assembled Genomes (MAGs / bins)",
    "section": "Background",
    "text": "Background\nWe will going over the following in class\n\nNEON\nJoint Genome Institute and the Integrated Microbial Genomes and Microbiomes(IMG/MER)\nDOE JGI Metagenome Workflow\nJGI GOLD database summary of our project"
  },
  {
    "objectID": "labs/lab5_NEON_tables.html#exercises",
    "href": "labs/lab5_NEON_tables.html#exercises",
    "title": "Lab 5 : Working with NEON Metagenome Assembled Genomes (MAGs / bins)",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1\nUse view(iris) to see the whole data table. Subset the table based on a different species than was used in the example. Display the table using DT::datatable\n\n\nExercise 2\nDisplay using DT::datatable the NEON MAGs that have at least 1 16S rRNA\n\n\nExercise 3\nDisplay a table of the MAGs from Lower Tombigbee with only the columns for the genome_name, gtdb_taxonomy_lineage, and estimated MAG genome size (total_number_of_bases).\n\n\nExercise 4\nDisplay a table with the counts of each class names at Lyndon B. Johnson National Grasslands.\n\n\nExercise 5\nDisplay a table with the counts for the Phylum Actinomycetota at each Site."
  },
  {
    "objectID": "labs/tesy.html",
    "href": "labs/tesy.html",
    "title": "Lab 5 : Working with NEON Metagenome Assembled Genomes (MAGs / bins)",
    "section": "",
    "text": "R code\nlibrary(tidyverse)\n\n\n\n\nR code\nsimple_csv &lt;- \"\n  x\n  10\n  .\n  20\n  30\"\n\n\n\n\nR code\nread_csv(simple_csv)\n\n\n# A tibble: 4 × 1\n  x    \n  &lt;chr&gt;\n1 10   \n2 .    \n3 20   \n4 30"
  },
  {
    "objectID": "labs/lab6_biblio.html",
    "href": "labs/lab6_biblio.html",
    "title": "Lab 6s : Adding references exported from Zotero to your quarto documents",
    "section": "",
    "text": "Adding references exported from Zotero to your report"
  },
  {
    "objectID": "labs/lab6_biblio.html#learning-objectives",
    "href": "labs/lab6_biblio.html#learning-objectives",
    "title": "Lab 6s : Adding references exported from Zotero to your quarto documents",
    "section": "",
    "text": "Adding references exported from Zotero to your report"
  },
  {
    "objectID": "labs/lab6_biblio.html#background",
    "href": "labs/lab6_biblio.html#background",
    "title": "Lab 6s : Adding references exported from zotero to your quarto documents",
    "section": "Background",
    "text": "Background\n\nZotero"
  },
  {
    "objectID": "labs/lab6_biblio.html#adding-references-to-your-report",
    "href": "labs/lab6_biblio.html#adding-references-to-your-report",
    "title": "Lab 6s : Adding references exported from Zotero to your quarto documents",
    "section": "Adding references to your report",
    "text": "Adding references to your report\n\nStep 1: Set up Zotero on your computer\nDownload Zotero to your computer and add the Zotero plugin to your web browser.\n\n\nStep 2: Get references from Zotero\nGo to PubMed and search for metagenome. Add 3 articles to your Zotero database\n\n\nStep 3: Export the 3 references from Zotero\n\nSelect the 3 references and select File &gt; Export Library\nExport in BibTeX format. This will generate a file with a .bib extension. Do not include any spaces in your file name.\nUpload the .bib file to Unity.\n\n\n\nStep 4: Add the file name of your references library to your YAML block\n---\ntitle: \"Lab 6\"\nauthor: \"Your name\"\nformat:\n  html:\n    toc: true\n    toc_float: true\n    embed-resources: true\nexecute: \n  warning: false\n  message: false\nbibliography: MyReferences.bib\n---\n\n\nStep 5: Cite the references in the text of your document\nMy reference file looks like this.\n@Misc{Chang2015,\n  Title                    = {shiny: Web Application Framework for R. R package version 0.12.1},\n\n  Author                   = {Chang, W. and Cheng, J. and Allaire, JJ. and Xie, Y. and McPherson, J. },\n  Year                     = {2015},\n\n  Type                     = {Computer Program},\n  Url                      = {http://CRAN.R-project.org/package=shiny}\n}\n\n\n@Article{RCoreTeam,\n  Title                    = {R: A Language and Environment for Statistical Computing},\n  Author                   = {{R Core Team}},\n  Year                     = {2015},\n\n  Type                     = {Journal Article},\n  Url                      = {http://www.R-project.org}\n}\nTo cite this in my document I would use the following format.\nApplication written in R [@RCoreTeam] using the Shiny framework [@Chang2015].\n\n\nAdd a section header for the references at the end of your document\nWhen you Render the document the references will be added to the end of the document. To define this section and to make it visable in our outline put # References on the last line (or the appropriate header level for your report)."
  },
  {
    "objectID": "labs/lab6_biblio.html#exercises",
    "href": "labs/lab6_biblio.html#exercises",
    "title": "Lab 6s : Adding references exported from Zotero to your quarto documents",
    "section": "Exercises",
    "text": "Exercises\nMake a document that has 3 references."
  },
  {
    "objectID": "labs/lab6_biblio.html#acknowledgements",
    "href": "labs/lab6_biblio.html#acknowledgements",
    "title": "Lab 6s : Adding references exported from Zotero to your quarto documents",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nOur labs are written in R (R Core Team 2015) and past labs have used th Shiny framework (Chang et al. 2015)."
  },
  {
    "objectID": "labs/lab1m_quarto.html",
    "href": "labs/lab1m_quarto.html",
    "title": "Lab 1s - Quarto",
    "section": "",
    "text": "Quarto\nDifferences between R Markdown and Quarto\nYAML blocks\nCode blocks\n\n\n\n\n\nQuarto is the name of an open-source publishing system used for technical and scientific writing.\nIt lets you combine text, code, and outputs in one document—perfect for data science, research, and reproducible reports.\nYou can write in Python, R, Julia, or Observable JavaScript, and publish to formats like HTML, PDF, Word, and even full websites\nIt’s considered the next generation of R Markdown, and works with tools like Jupyter Notebooks, VS Code, and RStudio.\nBecause it is based on R Markdown, there is a wealth of related resources and books published on and using R Markdown. See some of on the examples books using R we will periodically using in the course.\n\n\n\n\nR Markdown and Quarto are both tools for creating dynamic documents that combine code, text, and outputs (like plots or tables), but they differ in terms of design philosophy, features, and flexibility. Here’s a breakdown of their key differences:\n\n\n\nR Markdown:\n\nDeveloped by RStudio.\nPrimarily designed for R users.\nBuilt on top of knitr and Pandoc.\nDeeply integrated into the RStudio IDE.\n\nQuarto:\n\nAlso developed by RStudio, but as a next-generation tool.\nLanguage-agnostic: supports R, Python, Julia, and Observable JavaScript.\nUses Pandoc directly (not knitr).\nDesigned to unify and modernize the workflow across languages.\n\n\n\n\n\n\nR Markdown: Best suited for R. Python support is possible but less seamless.\nQuarto: First-class support for multiple languages in a single document. You can mix R, Python, Julia, and JavaScript.\n\n\n\n\n\nR Markdown:\n\nUses YAML front matter for metadata.\nCode chunks are written using triple backticks with language identifiers.\n\nQuarto:\n\nSimilar structure but more consistent and extensible YAML.\nSupports Markdown extensions like callouts, citations, cross-referencing, and more.\n\n\n\n\n\n\nR Markdown:\n\nSupports HTML, PDF, Word, slides (via xaringan or ioslides), etc.\nCustomization can be complex for advanced layouts.\n\nQuarto:\n\nSupports all R Markdown formats plus:\n\nReveal.js slides.\nBooks and websites with built-in navigation and styling.\nJupyter-style notebooks.\n\nEasier to configure and customize outputs.\n\n\n\n\n\n\nR Markdown:\n\nExecutes code using knitr (for R) or reticulate (for Python).\nLess control over execution environment.\n\nQuarto:\n\nUses Jupyter kernels or R directly.\nBetter support for notebook-style interactivity and execution control.\n\n\n\n\n\n\nR Markdown:\n\nNo native concept of a “project” beyond RStudio projects.\n\nQuarto:\n\nSupports Quarto Projects: collections of documents with shared configuration.\nIdeal for books, blogs, websites, and multi-document workflows.\n\n\n\n\n\n\nQuarto:\n\nMore modern and extensible.\nSupports Lua filters, Markdown extensions, cross-referencing, citations, and interactive widgets.\nBetter support for version control and CI/CD workflows.\n\n\n\n\n\n\n\n\nFeature\nR Markdown\nQuarto\n\n\n\n\nLanguage Support\nPrimarily R\nR, Python, Julia, JS\n\n\nExecution Engine\nknitr\nJupyter / native\n\n\nOutput Formats\nMany\nMore + easier config\n\n\nInteractivity\nLimited\nRich (widgets, JS)\n\n\nProject Support\nBasic\nFull project system\n\n\nExtensibility\nModerate\nHigh\n\n\nIdeal For\nR-centric reports\nMulti-language docs\n\n\n\n\n\n\n\n\nLet’s go the the Quarto documentation for Markdown basics and more details on Figures\nIt is important to specific where the figure is located relative to your .qmd file. The path can be relative (giant_virus.jpg) or (images/giant_virus.jpg) or absolute (/home/pi_jlb_umass_edu/images/giant_virus.jpg).\n\n\n\ngiant virus\n\n\n\n\n\nIn Quarto, a YAML block is a section at the top of a document that contains metadata and configuration settings. YAML stands for “YAML Ain’t Markup Language”, and it’s used to define things like the document title, author, output format, and more.\n\n\nA YAML block is enclosed by triple dashes (---) at the beginning and end:\n---\ntitle: \"My Analysis Report\"\nauthor: \"Jeffrey Blanchard\"\ndate: \"2025-09-08\"\nformat: html\neditor: visual\n---\nThis block tells Quarto: - The title of the document. - The author name. - The date to display. - The output format (e.g., HTML, PDF, Word). - The editor preference (e.g., visual or source).\n\n\n\nHere are some frequently used fields:\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\ntitle\nTitle of the document\n\n\nauthor\nAuthor name(s)\n\n\ndate\nDate of publication\n\n\nformat\nOutput format (e.g., html, pdf, docx, revealjs)\n\n\ntoc\nTable of contents (true or false)\n\n\nnumber-sections\nNumber section headings\n\n\ntheme\nVisual theme for HTML or slides\n\n\ncode-fold\nWhether code chunks can be collapsed\n\n\nexecute\nControls code execution (e.g., echo, eval, freeze)\n\n\nbibliography\nPath to .bib file for citations\n\n\nfilters\nLua filters for advanced customization\n\n\n\n\n\n\n---\ntitle: \"Data Exploration\"\nauthor: \"Jeffrey Blanchard\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    theme: cosmo\n    embed-resources: true\nexecute:\n  echo: true\n  freeze: auto\n---\nThis configures: - An HTML output with a table of contents. - Collapsible code chunks. - A Bootstrap theme (cosmo). - Code execution settings.\nImportant for our class is the embed-resources: true line. This creates a single html file with the figures embeded in the file. If this line is not in the YAML block a new directory will be created which contains the images and is linked to the html file. This means that if you turn in this html file (without using embed-resources: true) the images will not be shown.\n\n\n\nQuarto YAML is: - More consistent and extensible. - Supports nested configuration (e.g., format.html.toc). - Easier to manage across multi-format outputs (e.g., HTML and PDF from one source).\nIn Quarto, R code chunks are sections of code embedded in your document that get executed when the document is rendered. These chunks are enclosed in triple backticks and start with {r}. You can customize their behavior using chunk options, which control things like whether the code is shown, whether it’s executed, how results are displayed, and more.\n\n\n\n\n\n\n\n\nR code\nsummary(cars)\n\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\n\n\n\nHere’s a categorized list of the most useful options:\n\n\n\n\n\nOption\nDescription\n\n\n\n\neval\nWhether to evaluate the code (TRUE or FALSE)\n\n\necho\nShow the code in the output (TRUE or FALSE)\n\n\ninclude\nInclude both code and output (TRUE or FALSE)\n\n\nerror\nShow errors in output (TRUE or FALSE)\n\n\nwarning\nShow warnings (TRUE or FALSE)\n\n\nmessage\nShow messages (TRUE or FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\nresults\nHow to display results (\"markup\", \"asis\", \"hide\")\n\n\nfig.width\nWidth of plots (in inches)\n\n\nfig.height\nHeight of plots (in inches)\n\n\nfig.cap\nCaption for figures\n\n\nfig.align\nAlignment of figures (\"left\", \"center\", \"right\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\ncache\nCache results to avoid re-running code\n\n\nfreeze\nFreeze output to avoid re-execution unless explicitly updated\n\n\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\ntidy\nAutomatically tidy code before execution\n\n\ncollapse\nCollapse code and output together\n\n\ncomment\nPrefix for output lines\n\n\n\n\n\n\n\n{r pressure-plot, fig.width=6, fig.height=4, fig.cap=\"Pressure vs Temperature\"}\n\n\nR code\nplot(pressure)\n\n\n\n\n\nPressure vs Temperature\n\n\n\n\nThis chunk: - Hides the code (echo=FALSE) - Sets figure size - Adds a caption - Names the chunk (pressure-plot) for reference\n\n\n\n\nTry a different Quarto theme\n\n\n\nCreate a lab report that has\n\nA link out to an external web site.\nAn image embedded.\nA table of contents using the YAML block\nThe code folded using the YAML block\nA code chunk with plot(cars) in which the plot is sized to a figure width of 3 and height of 2\nA code chunk in which the output, but not the code is in the rendered file.\nTry a different Quarto theme.\n\n\n\n\nThis lab was created with assistance from UMass Copilot"
  },
  {
    "objectID": "labs/lab1m_quarto.html#learning-objectives",
    "href": "labs/lab1m_quarto.html#learning-objectives",
    "title": "Lab 1s - Quarto",
    "section": "",
    "text": "Quarto\nDifferences between R Markdown and Quarto\nYAML blocks\nCode blocks"
  },
  {
    "objectID": "labs/lab1m_quarto.html#quarto",
    "href": "labs/lab1m_quarto.html#quarto",
    "title": "Lab 1s - Quarto",
    "section": "",
    "text": "Quarto is the name of an open-source publishing system used for technical and scientific writing.\nIt lets you combine text, code, and outputs in one document—perfect for data science, research, and reproducible reports.\nYou can write in Python, R, Julia, or Observable JavaScript, and publish to formats like HTML, PDF, Word, and even full websites\nIt’s considered the next generation of R Markdown, and works with tools like Jupyter Notebooks, VS Code, and RStudio.\nBecause it is based on R Markdown, there is a wealth of related resources and books published on and using R Markdown. See some of on the examples books using R we will periodically using in the course."
  },
  {
    "objectID": "labs/lab1m_quarto.html#differences-between-r-markdown-and-quarto",
    "href": "labs/lab1m_quarto.html#differences-between-r-markdown-and-quarto",
    "title": "Lab 1s - Quarto",
    "section": "",
    "text": "R Markdown and Quarto are both tools for creating dynamic documents that combine code, text, and outputs (like plots or tables), but they differ in terms of design philosophy, features, and flexibility. Here’s a breakdown of their key differences:\n\n\n\nR Markdown:\n\nDeveloped by RStudio.\nPrimarily designed for R users.\nBuilt on top of knitr and Pandoc.\nDeeply integrated into the RStudio IDE.\n\nQuarto:\n\nAlso developed by RStudio, but as a next-generation tool.\nLanguage-agnostic: supports R, Python, Julia, and Observable JavaScript.\nUses Pandoc directly (not knitr).\nDesigned to unify and modernize the workflow across languages.\n\n\n\n\n\n\nR Markdown: Best suited for R. Python support is possible but less seamless.\nQuarto: First-class support for multiple languages in a single document. You can mix R, Python, Julia, and JavaScript.\n\n\n\n\n\nR Markdown:\n\nUses YAML front matter for metadata.\nCode chunks are written using triple backticks with language identifiers.\n\nQuarto:\n\nSimilar structure but more consistent and extensible YAML.\nSupports Markdown extensions like callouts, citations, cross-referencing, and more.\n\n\n\n\n\n\nR Markdown:\n\nSupports HTML, PDF, Word, slides (via xaringan or ioslides), etc.\nCustomization can be complex for advanced layouts.\n\nQuarto:\n\nSupports all R Markdown formats plus:\n\nReveal.js slides.\nBooks and websites with built-in navigation and styling.\nJupyter-style notebooks.\n\nEasier to configure and customize outputs.\n\n\n\n\n\n\nR Markdown:\n\nExecutes code using knitr (for R) or reticulate (for Python).\nLess control over execution environment.\n\nQuarto:\n\nUses Jupyter kernels or R directly.\nBetter support for notebook-style interactivity and execution control.\n\n\n\n\n\n\nR Markdown:\n\nNo native concept of a “project” beyond RStudio projects.\n\nQuarto:\n\nSupports Quarto Projects: collections of documents with shared configuration.\nIdeal for books, blogs, websites, and multi-document workflows.\n\n\n\n\n\n\nQuarto:\n\nMore modern and extensible.\nSupports Lua filters, Markdown extensions, cross-referencing, citations, and interactive widgets.\nBetter support for version control and CI/CD workflows.\n\n\n\n\n\n\n\n\nFeature\nR Markdown\nQuarto\n\n\n\n\nLanguage Support\nPrimarily R\nR, Python, Julia, JS\n\n\nExecution Engine\nknitr\nJupyter / native\n\n\nOutput Formats\nMany\nMore + easier config\n\n\nInteractivity\nLimited\nRich (widgets, JS)\n\n\nProject Support\nBasic\nFull project system\n\n\nExtensibility\nModerate\nHigh\n\n\nIdeal For\nR-centric reports\nMulti-language docs"
  },
  {
    "objectID": "labs/lab1m_quarto.html#adding-links-and-figures-in-a-quarto-document",
    "href": "labs/lab1m_quarto.html#adding-links-and-figures-in-a-quarto-document",
    "title": "Lab 1s - Quarto",
    "section": "",
    "text": "Let’s go the the Quarto documentation for Markdown basics and more details on Figures\nIt is important to specific where the figure is located relative to your .qmd file. The path can be relative (giant_virus.jpg) or (images/giant_virus.jpg) or absolute (/home/pi_jlb_umass_edu/images/giant_virus.jpg).\n\n\n\ngiant virus"
  },
  {
    "objectID": "labs/lab1m_quarto.html#yaml-blocks-in-quarto",
    "href": "labs/lab1m_quarto.html#yaml-blocks-in-quarto",
    "title": "Lab 1s - Quarto",
    "section": "",
    "text": "In Quarto, a YAML block is a section at the top of a document that contains metadata and configuration settings. YAML stands for “YAML Ain’t Markup Language”, and it’s used to define things like the document title, author, output format, and more.\n\n\nA YAML block is enclosed by triple dashes (---) at the beginning and end:\n---\ntitle: \"My Analysis Report\"\nauthor: \"Jeffrey Blanchard\"\ndate: \"2025-09-08\"\nformat: html\neditor: visual\n---\nThis block tells Quarto: - The title of the document. - The author name. - The date to display. - The output format (e.g., HTML, PDF, Word). - The editor preference (e.g., visual or source).\n\n\n\nHere are some frequently used fields:\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\ntitle\nTitle of the document\n\n\nauthor\nAuthor name(s)\n\n\ndate\nDate of publication\n\n\nformat\nOutput format (e.g., html, pdf, docx, revealjs)\n\n\ntoc\nTable of contents (true or false)\n\n\nnumber-sections\nNumber section headings\n\n\ntheme\nVisual theme for HTML or slides\n\n\ncode-fold\nWhether code chunks can be collapsed\n\n\nexecute\nControls code execution (e.g., echo, eval, freeze)\n\n\nbibliography\nPath to .bib file for citations\n\n\nfilters\nLua filters for advanced customization\n\n\n\n\n\n\n---\ntitle: \"Data Exploration\"\nauthor: \"Jeffrey Blanchard\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    theme: cosmo\n    embed-resources: true\nexecute:\n  echo: true\n  freeze: auto\n---\nThis configures: - An HTML output with a table of contents. - Collapsible code chunks. - A Bootstrap theme (cosmo). - Code execution settings.\nImportant for our class is the embed-resources: true line. This creates a single html file with the figures embeded in the file. If this line is not in the YAML block a new directory will be created which contains the images and is linked to the html file. This means that if you turn in this html file (without using embed-resources: true) the images will not be shown.\n\n\n\nQuarto YAML is: - More consistent and extensible. - Supports nested configuration (e.g., format.html.toc). - Easier to manage across multi-format outputs (e.g., HTML and PDF from one source).\nIn Quarto, R code chunks are sections of code embedded in your document that get executed when the document is rendered. These chunks are enclosed in triple backticks and start with {r}. You can customize their behavior using chunk options, which control things like whether the code is shown, whether it’s executed, how results are displayed, and more."
  },
  {
    "objectID": "labs/lab1m_quarto.html#r-code-chunks-in-quarto",
    "href": "labs/lab1m_quarto.html#r-code-chunks-in-quarto",
    "title": "Lab 1s - Quarto",
    "section": "",
    "text": "R code\nsummary(cars)\n\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\n\n\n\nHere’s a categorized list of the most useful options:\n\n\n\n\n\nOption\nDescription\n\n\n\n\neval\nWhether to evaluate the code (TRUE or FALSE)\n\n\necho\nShow the code in the output (TRUE or FALSE)\n\n\ninclude\nInclude both code and output (TRUE or FALSE)\n\n\nerror\nShow errors in output (TRUE or FALSE)\n\n\nwarning\nShow warnings (TRUE or FALSE)\n\n\nmessage\nShow messages (TRUE or FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\nresults\nHow to display results (\"markup\", \"asis\", \"hide\")\n\n\nfig.width\nWidth of plots (in inches)\n\n\nfig.height\nHeight of plots (in inches)\n\n\nfig.cap\nCaption for figures\n\n\nfig.align\nAlignment of figures (\"left\", \"center\", \"right\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\ncache\nCache results to avoid re-running code\n\n\nfreeze\nFreeze output to avoid re-execution unless explicitly updated\n\n\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\ntidy\nAutomatically tidy code before execution\n\n\ncollapse\nCollapse code and output together\n\n\ncomment\nPrefix for output lines\n\n\n\n\n\n\n\n{r pressure-plot, fig.width=6, fig.height=4, fig.cap=\"Pressure vs Temperature\"}\n\n\nR code\nplot(pressure)\n\n\n\n\n\nPressure vs Temperature\n\n\n\n\nThis chunk: - Hides the code (echo=FALSE) - Sets figure size - Adds a caption - Names the chunk (pressure-plot) for reference"
  },
  {
    "objectID": "labs/lab1m_quarto.html#quarto-themes",
    "href": "labs/lab1m_quarto.html#quarto-themes",
    "title": "Lab 1s - Quarto",
    "section": "",
    "text": "Try a different Quarto theme"
  },
  {
    "objectID": "labs/lab1m_quarto.html#exercises",
    "href": "labs/lab1m_quarto.html#exercises",
    "title": "Lab 1s - Quarto",
    "section": "",
    "text": "Create a lab report that has\n\nA link out to an external web site.\nAn image embedded.\nA table of contents using the YAML block\nThe code folded using the YAML block\nA code chunk with plot(cars) in which the plot is sized to a figure width of 3 and height of 2\nA code chunk in which the output, but not the code is in the rendered file.\nTry a different Quarto theme."
  },
  {
    "objectID": "labs/lab1m_quarto.html#acknowledgements",
    "href": "labs/lab1m_quarto.html#acknowledgements",
    "title": "Lab 1s - Quarto",
    "section": "",
    "text": "This lab was created with assistance from UMass Copilot"
  },
  {
    "objectID": "labs/lab2s_biblio.html",
    "href": "labs/lab2s_biblio.html",
    "title": "Lab S2 : Adding References to Your Report with Zotero",
    "section": "",
    "text": "Adding references exported from Zotero to your report"
  },
  {
    "objectID": "labs/lab2s_biblio.html#learning-objectives",
    "href": "labs/lab2s_biblio.html#learning-objectives",
    "title": "Lab S2 : Adding References to Your Report with Zotero",
    "section": "",
    "text": "Adding references exported from Zotero to your report"
  },
  {
    "objectID": "labs/lab2s_biblio.html#adding-references-to-your-report",
    "href": "labs/lab2s_biblio.html#adding-references-to-your-report",
    "title": "Lab S2 : Adding References to Your Report with Zotero",
    "section": "Adding references to your report",
    "text": "Adding references to your report\n\nStep 1: Set up Zotero on your computer\nDownload Zotero to your computer and add the Zotero plugin to your web browser.\n\n\nStep 2: Get references from Zotero\nGo to PubMed and search for metagenome. Add 3 articles to your Zotero database\n\n\nStep 3: Export the 3 references from Zotero\n\nSelect the 3 references and select File &gt; Export Library\nExport in BibTeX format. This will generate a file with a .bib extension. Do not include any spaces in your file name.\nUpload the .bib file to Unity.\n\n\n\nStep 4: Add the file name of your references library to your YAML block\n---\ntitle: \"Lab 6\"\nauthor: \"Your name\"\nformat:\n  html:\n    toc: true\n    toc_float: true\n    embed-resources: true\nexecute: \n  warning: false\n  message: false\nbibliography: MyReferences.bib\n---\n\n\nStep 5: Cite the references in the text of your document\nMy reference file looks like this.\n@Misc{Chang2015,\n  Title                    = {shiny: Web Application Framework for R. R package version 0.12.1},\n\n  Author                   = {Chang, W. and Cheng, J. and Allaire, JJ. and Xie, Y. and McPherson, J. },\n  Year                     = {2015},\n\n  Type                     = {Computer Program},\n  Url                      = {http://CRAN.R-project.org/package=shiny}\n}\n\n\n@Article{RCoreTeam,\n  Title                    = {R: A Language and Environment for Statistical Computing},\n  Author                   = {{R Core Team}},\n  Year                     = {2015},\n\n  Type                     = {Journal Article},\n  Url                      = {http://www.R-project.org}\n}\nTo cite this in my document I would use the following format.\nApplication written in R [@RCoreTeam] using the Shiny framework [@Chang2015].\nOpen your .bib file to see the appropriate @ names for your references.\n\n\nStep 6: Add a section header for the references at the end of your document\nWhen you Render the document the references will be added to the end of the document. To define this section and to make it visible in our outline put # References on the last line (or the appropriate header level for your report)."
  },
  {
    "objectID": "labs/lab2s_biblio.html#exercises",
    "href": "labs/lab2s_biblio.html#exercises",
    "title": "Lab S2 : Adding References to Your Report with Zotero",
    "section": "Exercises",
    "text": "Exercises\nCreate and turn in a report that has 3 references."
  },
  {
    "objectID": "labs/lab2s_biblio.html#acknowledgements",
    "href": "labs/lab2s_biblio.html#acknowledgements",
    "title": "Lab S2 : Adding References to Your Report with Zotero",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nOur labs are written in R (R Core Team 2015) and past labs have used th Shiny framework (Chang et al. 2015)."
  },
  {
    "objectID": "labs/lab6_NEON_pub_graphics.html",
    "href": "labs/lab6_NEON_pub_graphics.html",
    "title": "Lab 6 : ggplotting with NEON MAG data",
    "section": "",
    "text": "Be able to add title, axis labels, legends, colors to ggplot graphs\nResize graphs within R code chunks\nPrint graphics to a file (e.g. jpeg, pdf)\nLoading images into Markdown\nMaking interactive graphs"
  },
  {
    "objectID": "labs/lab6_NEON_pub_graphics.html#learning-objectives",
    "href": "labs/lab6_NEON_pub_graphics.html#learning-objectives",
    "title": "Lab 6 : ggplotting with NEON MAG data",
    "section": "",
    "text": "Be able to add title, axis labels, legends, colors to ggplot graphs\nResize graphs within R code chunks\nPrint graphics to a file (e.g. jpeg, pdf)\nLoading images into Markdown\nMaking interactive graphs"
  },
  {
    "objectID": "labs/lab6_NEON_pub_graphics.html#fine-tuning-ggplots",
    "href": "labs/lab6_NEON_pub_graphics.html#fine-tuning-ggplots",
    "title": "Lab 6 : ggplotting with NEON MAG data",
    "section": "Fine tuning ggplots",
    "text": "Fine tuning ggplots\nToday we will build upon the graphing approaches in the with all the Data Carpentry ggplot tutorial\nThe Cookbook for R by Winston Chang is also great for tidying up our graphs.\nHere are a couple of cheat sheets that can be useful\n\nR Studio ggplot2 cheatsheet\nRMarkdown cheatsheet\n\n\nFirst load the libraries\n\n\nR code\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(plotly)\nlibrary(DT)\nlibrary(lubridate)\n\n\n\n\nBasic graph labels\n\n\nR code\n  ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n    geom_point(aes(color=Species, shape=Species)) +\n    labs(title = \"Iris Sepal Length vs Wide\", x = \"Sepal Length\", y = \"Sepal Width\", color = \"Plant Species\", shape = \"Plant Species\") \n\n\n\n\n\n\n\n\n\n\n\nThemes\nhttps://r-charts.com/ggplot2/themes/\n\n\nR code\n  ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n    geom_point(aes(color=Species, shape=Species)) +\n    labs(title = \"Iris Sepal Length vs Wide\", x = \"Sepal Length\", y = \"Sepal Width\", color = \"Plant Species\", shape = \"Plant Species\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nColors\n\n\nR code\n  ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n    geom_point(color = \"red\", aes(shape = Species))+\n    labs(title = \"Iris Sepal Length vs Wide\", x = \"Sepal Length\", y = \"Sepal Width\") \n\n\n\n\n\n\n\n\n\n\n\nR code\n  ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n    geom_point(aes(color = Species, shape = Species)) +\n    scale_color_manual(values=c(\"blue\", \"purple\", \"red\")) +\n    labs(title = \"Iris Sepal Length vs Wide\", x = \"Sepal Length\", y = \"Sepal Width\") \n\n\n\n\n\n\n\n\n\n\n\nR code\n  ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n    geom_point(aes(color = Species, shape = Species)) +\n    scale_color_brewer(palette=\"Dark2\") +\n    labs(title = \"Iris Sepal Length vs Wide\", x = \"Sepal Length\", y = \"Sepal Width\") \n\n\n\n\n\n\n\n\n\n\n\nR code\nlibrary(viridisLite)\n  ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n    geom_point(aes(fill = Species), color = \"black\", pch=21) +\n    labs(title = \"Iris Sepal Length vs Wide\", x = \"Sepal Length\", y = \"Sepal Width\") \n\n\n\n\n\n\n\n\n\nviridisLite - Colorblind-Friendly Color Maps for R](https://sjmgarnier.github.io/viridisLite/)\n\n\nR code\nlibrary(viridisLite)\n  ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n    geom_point(aes(color = Species, shape = Species)) +\n    scale_colour_viridis_d() +\n    labs(title = \"Iris Sepal Length vs Wide\", x = \"Sepal Length\", y = \"Sepal Width\") \n\n\n\n\n\n\n\n\n\n\n\nUnderstanding directory paths\nHere’s an overview of Unix directory paths, which are fundamental to navigating and managing files in Unix-like operating systems (such as on Unity). There directory paths that are listed in examples will often be different than the ones you need to use on Unity.\n\nAbsolute vs. Relative Paths\nAbsolute Path: Starts from the root directory (/) and specifies the full path to a file or directory.\nExample: /home/jlb/images/star_virus.png\nRelative Path: Starts from the current working directory.\nExample: images/star_virus.png (if you’re already in /home/jlb)\nTo find your absolute path in RStudiio. Type in the console (bottow left window) getwd()\n&gt; getwd()\n[1] \"/home/jlb_umass_edu\"\nOften I will put the data sets we are working with in our course directory which has the path\n/work/pi_bio678_umass_edu/data_NEON/\ntherefore to load the data into RStudio\nNEON_MAGs &lt;- read_tsv(\"/work/pi_bio678_umass_edu/data_NEON/exported_img_bins_Gs0166454_NEON.tsv\")\nI suggest you make a new folder for today’s lab in names images in your home directory. When you create or plot a graph you can use the relative path\n\"images/iris_example_plot1.pdf\"\nor the absolute path\n\"home/jlb/images/iris_example_plot1.pdf\"\n\n\n\nControlling graph size in Markdown\nThe dimensions of an individual graph in the Markdown document be adjusted by specifying the graph dimensions in the header for the r code chunk.\n#| fig-height: 20\n#| fig-width: 8\nThis is similar to what we used in previous labs where we wanted to show but not run the code\n#| eval: false\nIf you want to run the code but not show the code\n#| echo: false\n\n\nGraphic Output\nYou may have realized that you can export plots in R Studio by clicking on Export in the Plots window that appears after you make a graph. You can save as a pdf, svg, tiff, png, bmp, jpeg and eps. You can also write the output directly to a file. This is particularly useful for controling the final dimensions in a reproducible way and for manuscripts.\n\nCookbook for R - Output to a file - PDF, PNG, TIFF, SVG \n\n\n\nR code\n# Plot graph to a pdf outputfile\npdf(\"../images/iris_example_plot1.pdf\", width=6, height=3)\nggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + \n  geom_point() +\n  labs(title = \"Iris Sepal Length vs Wide\", x = \"Sepal Length\", y = \"Sepal Width\") \ndev.off()\n\n\npng \n  2 \n\n\n\n\nR code\n# Plot graph to a png outputfile\nppi &lt;- 300\npng(\"../images/iris_example_plot2.png\", width=6*ppi, height=4*ppi, res=ppi)\nggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + \n  geom_point()\ndev.off()\n\n\npng \n  2 \n\n\nFor more details on sizing output Cookbook for R - Output to a file - PDF, PNG, TIFF, SVG \n\n\nMarkdown loading images\nSometimes it is useful in controlling the image layout for a report to file with the graph and then subsequently load it into the .qmd file. This works with png files, but not pdfs. You can also upload images made with other bioinformatic tools into your report.\n![Iris example plot](../images/iris_example_plot2.png) \n\n\n\nIris example plot\n\n\nAnother way to present a graph without the code is adding echo = FALSE within the r{} chunk - {r echo = FALSE}. This prevents code, but not the results from appearing in the knitr file.\n\n\nInteractive graphs\nWith plotly/ggplotly (https://plot.ly/ggplot2/) you can make interactive graphs in your lab report.\n\n\nR code\nlibrary(plotly)\n\n\n\n\nR code\n# Version 1\nggplotly(\n  ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + \n    geom_point()\n )\n\n\n\n\n\n\n\n\nR code\n# Version 2\np &lt;- ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + \n  geom_point()\nggplotly(p)"
  },
  {
    "objectID": "labs/lab6_NEON_pub_graphics.html#examples-with-the-neon-data",
    "href": "labs/lab6_NEON_pub_graphics.html#examples-with-the-neon-data",
    "title": "Lab 6 : ggplotting with NEON MAG data",
    "section": "Examples with the NEON data",
    "text": "Examples with the NEON data\nLet’s load the table into R. This week I have made a number of changes to the data table to get it into a format in which we can work with both the soil and freshwater MAGs\n\nThis first block of code is for both the freshwater and soil MAGs\n\n\nR code\n# This is the location used for Github\nNEON_MAGs_prelim &lt;- read_tsv(\"../data/NEON_metadata/exported_img_bins_Gs0166454_NEON.tsv\") |&gt; \n# This is the location used for the class data directory on Unity\n# NEON_MAGs_prelim &lt;- read_tsv(\"/work/pi_bio678_umass_edu/data_NEON/exported_img_bins_Gs0166454_NEON.tsv\") |&gt;\n  \n  clean_names() |&gt; \n  \n  # Add a new column community corresponding to different communities names in the genome_name\n  mutate(community = case_when(\n    str_detect(genome_name, \"Freshwater sediment microbial communities\") ~ \"Freshwater sediment microbial communitie\",\n    str_detect(genome_name, \"Freshwater biofilm microbial communities\") ~ \"Freshwater biofilm microbial communities\",\n    str_detect(genome_name, \"Freshwater microbial communities\") ~ \"Freshwater microbial communities\",\n    str_detect(genome_name, \"Soil microbial communities\") ~ \"Soil microbial communities\",\n    TRUE ~ NA_character_\n  )) |&gt; \n  \n  # Create a column type that is either Freshwater or Soil\n  mutate(type = case_when(\n    str_detect(genome_name, \"Freshwater sediment microbial communities\") ~ \"Freshwater\",\n    str_detect(genome_name, \"Freshwater biofilm microbial communities\") ~ \"Freshwater\",\n    str_detect(genome_name, \"Freshwater microbial communities\") ~ \"Freshwater\",\n    str_detect(genome_name, \"Soil microbial communities\") ~ \"Soil\",\n    TRUE ~ NA_character_\n  )) |&gt; \n  \n  # Get rid of the communities strings\n  mutate_at(\"genome_name\", str_replace, \"Freshwater sediment microbial communities from \", \"\") |&gt; \n  mutate_at(\"genome_name\", str_replace, \"Freshwater biofilm microbial communities from\", \"\") |&gt; \n  mutate_at(\"genome_name\", str_replace, \"Freshwater microbial communities from \", \"\") |&gt; \n  mutate_at(\"genome_name\", str_replace, \"Soil microbial communities from \", \"\") |&gt; \n\n  # separate site from sample name \n  separate(genome_name, c(\"site\",\"sample_name\"), \" - \") |&gt;   \n  \n  # Deal with these unknow fields in the sample name by creating a new column and removing them from the sample name\n  mutate(sample_unknown = case_when(\n    str_detect(sample_name, \".SS.\") ~ \"SS\",\n    str_detect(sample_name, \".C0.\") ~ \"C0\",\n    str_detect(sample_name, \".C1.\") ~ \"C1\",\n    str_detect(sample_name, \".C2.\") ~ \"C2\",\n    TRUE ~ NA_character_\n  )) |&gt; \n\n# These fields are all associated with \"Freshwater microbial communities from...   \n# SS - near stream sensor\n# C0 - non-stratified lake/river surface near buoy\n# C1 - stratified lake surface/epilimnion near buoy\n# C2 - stratified lake hypolimnion near buoy\n# EPIPSAMMON  - biofilm on sand/silt\n# EPILITHON - biofilm on rocks/cobbles\n# EPIPHYTON - biofilm that grows on the stems and leaves of aquatic plants\n  \n# These fields are all associated with \"Freshwater biofilm microbial communities from\n# EPILITHON - biofilm on rocks/cobbles\n  \n# These fields are all associated with \"Freshwater sediment microbial communities from \n# EPIPSAMMON - biofilm on sand/silt\n  \n  mutate_at(\"sample_name\", str_replace, \".SS\", \"\") |&gt; \n  mutate_at(\"sample_name\", str_replace, \".C0\", \"\") |&gt; \n  mutate_at(\"sample_name\", str_replace, \".C1\", \"\") |&gt; \n  mutate_at(\"sample_name\", str_replace, \".C2\", \"\") |&gt; \n  \n\n  # Get rid of the the common strings at the end of sample names\n  mutate_at(\"sample_name\", str_replace, \"-GEN-DNA1\", \"\") |&gt;  \n  mutate_at(\"sample_name\", str_replace, \"-COMP-DNA1\", \"\") |&gt;  \n  mutate_at(\"sample_name\", str_replace, \"-COMP-DNA2\", \"\") |&gt;  \n  mutate_at(\"sample_name\", str_replace, \".DNA-DNA1\", \"\") |&gt;\n  mutate_at(\"sample_name\", str_replace, \"_v2\", \"\") |&gt;\n  mutate_at(\"sample_name\", str_replace, \" \\\\(version 2\\\\)\", \"\") |&gt;\n  mutate_at(\"sample_name\", str_replace, \" \\\\(version 3\\\\)\", \"\") |&gt;\n  \n# Separate out the taxonomy groups\n  separate(gtdb_taxonomy_lineage, c(\"domain\", \"phylum\", \"class\", \"order\", \"family\", \"genus\"), \"; \", remove = FALSE)\n\n\n\n\nThis block is for separating out the freshwater sample names\n\n\nR code\nNEON_MAGs_freshwater &lt;- NEON_MAGs_prelim |&gt;\n  filter(type == \"Freshwater\") |&gt;\n  # separate the Sample Name into Site ID and plot info\n  separate(sample_name, c(\"site_ID\",\"date\", \"layer\", \"subplot\"), \"\\\\.\", remove = FALSE) |&gt;\n  mutate(quadrant = NA_character_)\n\n\n\n\nThis block is for separating out the soil sample names\n\n\nR code\nNEON_MAGs_soil &lt;- NEON_MAGs_prelim |&gt;\n  filter(type == \"Soil\") |&gt;\n  # separate the Sample Name into Site ID and plot info\n  separate(sample_name, c(\"site_ID\",\"subplot.layer.date\"), \"_\", remove = FALSE,) |&gt; \n  # some sample names have 3 fields while others have a fourth field for the quadrant. This code create a field for the quadrant when present and adds na for samples from combined cores.\n  extract(\n    subplot.layer.date,\n    into = c(\"subplot\", \"layer\", \"quadrant\", \"date\"),\n    regex = \"^([^-]+)-([^-]+)(?:-([^-]+))?-([^-]+)$\",\n    remove = FALSE\n  ) |&gt;\n  mutate(quadrant = na_if(quadrant, \"\")) |&gt;\n  select(-subplot.layer.date)\n\n\n\n\nThis combines the soil and freshwater data frames\n\n\nR code\nNEON_MAGs &lt;-bind_rows(NEON_MAGs_freshwater, NEON_MAGs_soil) |&gt;\n  # This changes the format of the data column\n  mutate(date = ymd(date))\n\n\n\n\nFinding rows with NA\nIn some cases you will have NA on your graphs. If you would like to see which rows in a column contain NA use\n\n\nR code\nfilter(is.na(column_name))\n\n\nThese represent potential novel taxonomic groups. If you would like to replace NA with unknown or novel\n\n\nR code\nmutate(column_name = replace_na(column_name, \"novel\"))"
  },
  {
    "objectID": "labs/lab6_NEON_pub_graphics.html#exercises",
    "href": "labs/lab6_NEON_pub_graphics.html#exercises",
    "title": "Lab 6 : ggplotting with NEON MAG data",
    "section": "Exercises",
    "text": "Exercises\nFor all exercises make complete graphs that are report ready. Relabel the x-axis, y-axis and legend for clarity, add a title, add color and size appropriately. The NAs in the taxonomy indicate a novel species starting with the highest level. For example a NA in a class that has an assigned phylum Proteobacteria would be a novel class in the phylum Proteobacteria. To filter Class and Order based on NA.\n\n\nR code\nNEON_MAGs |&gt; \n  filter(is.na(class) | is.na(order))\n\n\n\nExercise 1\nWhat are the overall class MAG counts?\n\n\nExercise 2\nWhat are the MAG counts for each subplot. Color by site ID.\n\n\nExercise 3\nHow many novel bacteria were discovered (Show that number of NAs for each site)?\n\n\nExercise 4\nHow many novel bacterial MAGs are high quality vs medium quality?\n\n\nExercise 5\nWhat phyla have novel bacterial genera?\n\n\nExercise 6\nMake a stacked bar plot of the total number of MAGs at each site using Phylum as the fill.\n\n\nExercise 7\nUsing facet_wrap make plots of the total number of MAGs at each site for each phylum (e.g. similar to the example above but using the site ID and separating each graph by phylum.)\n\n\nExercise 8\nWhat is the relationship between MAGs genome size and the number of genes? Color by Phylum.\n\n\nExercise 9\nWhat is the relationship between scaffold count and MAG completeness?\n\n\nExercise 10\nSeparate out bin_id (e.g 3300078752_s0) into 2 columns metagenome_id and bin_num.\n\n\nExercise 11\nThe site column has strings like\n\nRio Cupeyes NEON Field Site, San Germán, Puerto Rico\nSycamore Creek NEON Field Site, Rio Verde, Arizona, USA\nSan Joaquin Experimental Range NEON Field site, Yosemite Lakes, California, USA\nSan Joaquin Experimental Range NEON Field Site, California, USA\nOak Ridge NEON Field Station, Tennessee, USA\n\nSeparate out this string into 4 columns site name (e.g. Rio Cupeyes), ’NEON field site(e.g. NEON Field Site/Station),region(e.g. Yosemite Lakes) andstate`.\nThis is a tough one that may require back and forth with copilot.\n\n\nExercise 12\nMake a graph showing the number of MAGs in each state."
  },
  {
    "objectID": "labs/lab6_NEON_pub_graphics.html#neon-graphs",
    "href": "labs/lab6_NEON_pub_graphics.html#neon-graphs",
    "title": "Lab 6 : ggplotting with NEON MAG data",
    "section": "NEON graphs",
    "text": "NEON graphs\n\nBar plots\n\nCounts produced by ggplot\nNote that in this graph ggplot produces the count automatically\n\n\nR code\nNEON_MAGs |&gt; \nmutate(phylum = replace_na(phylum, \"***NOVEL***\")) |&gt;\nggplot(aes(x = phylum)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\nUse the forcats package in tidyverse to put the counts in descending order\n\n\nR code\nNEON_MAGs |&gt; \nggplot(aes(x = fct_infreq(phylum))) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n\nCounts passed to ggplot\nThis is different code that creates the same graph as above. Note in this case the counts were first calculated in dplyr then passed to ggplot. Both x and y values are needed. Within geom_bar stat is set to “identify”\n\n\nR code\nNEON_MAGs |&gt; \n  count(phylum) |&gt; \nggplot(aes(x = phylum, y = n)) +\n  geom_col(stat = \"identity\") +\n  coord_flip()\n\n\n\n\n\n\n\n\n\nTo put in descending order\njgy\"Sepal Width #| fig-height: 10 #| fig-width: 8 NEON_MAGs |&gt;    count(phylum) |&gt;  ggplot(aes(x = reorder(phylum, n), y = n)) +   geom_col(stat = \"identity\") +   coord_flip()\n\n\nStacked vs multiple bar plots\n\n\nR code\nNEON_MAGs |&gt; \n  mutate(phylum = replace_na(phylum, \"***UNKNOWN***\")) |&gt; \nggplot(aes(x = fct_rev(fct_infreq(phylum)), fill = type)) +\n  geom_bar() +\n  coord_flip() +\n  labs(title = \"2023 and 2024 NEON MAG Taxonomic Distribution\", x = \"Count\", y = \"Phylum\") \n\n\n\n\n\n\n\n\n\n\n\nR code\nNEON_MAGs |&gt; \nmutate(phylum = replace_na(phylum, \"***NOVEL***\")) |&gt;\nggplot(aes(x = fct_rev(fct_infreq(phylum)), fill = type)) +\n  geom_bar(position = \"dodge\") +\n  coord_flip()\n\n\n\n\n\n\n\n\n\nNotice that the bars are of different width. This can be adjusted by setting the width\n\n\nR code\nNEON_MAGs |&gt; \nmutate(phylum = replace_na(phylum, \"***NOVEL***\")) |&gt; \nggplot(aes(x = fct_rev(fct_infreq(phylum)), fill = type)) +\n  geom_bar(position = position_dodge2(width = 0.9, preserve = \"single\")) +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple panels (facet_wrap)\n\n\nR code\nNEON_MAGs |&gt; \nggplot(aes(x = phylum, fill = type)) +\n  geom_bar(position = position_dodge2(width = 0.9, preserve = \"single\")) +\n  coord_flip() +\n  facet_wrap(vars(site_ID), scales = \"free\", ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\nHistogram\n\n\nR code\nNEON_MAGs |&gt; \nggplot(aes(x = total_number_of_bases)) +\n  geom_histogram(bins = 50) \n\n\n\n\n\n\n\n\n\n\n\nBox plot\n\n\nR code\nNEON_MAGs  |&gt;    \nggplot(aes(x = fct_infreq(phylum), y = total_number_of_bases)) +\n  geom_boxplot() +\n  theme(axis.text.x = element_text(angle=45, vjust=1, hjust=1))\n\n\n\n\n\n\n\n\n\n\n\nShowing each point in the above plot\n\n\nR code\nNEON_MAGs |&gt;   \nggplot(aes(x = fct_infreq(phylum), y = total_number_of_bases)) +\n  geom_point() +\n  coord_flip()"
  },
  {
    "objectID": "labs/lab6_freshwater.html",
    "href": "labs/lab6_freshwater.html",
    "title": "Lab 6 : ggplotting with NEON MAG data",
    "section": "",
    "text": "R code\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(lubridate)"
  },
  {
    "objectID": "labs/lab6_freshwater.html#examples-with-the-neon-data",
    "href": "labs/lab6_freshwater.html#examples-with-the-neon-data",
    "title": "Lab 6 : ggplotting with NEON MAG data",
    "section": "Examples with the NEON data",
    "text": "Examples with the NEON data\n\nThis first block of code is for both the freshwater and soil MAGs\n\n\nR code\n# This is the location used for Github\nNEON_MAGs_prelim &lt;- read_tsv(\"../data/NEON_metadata/exported_img_bins_Gs0166454_NEON.tsv\") |&gt; \n# This is the location used for the class data directory on Unity\n# NEON_MAGs &lt;- read_tsv(\"/work/pi_bio678_umass_edu/data_NEON/exported_img_bins_Gs0166454_NEON.tsv\")\n  \n  clean_names() |&gt; \n  \n  # Add a new column community corresponding to different communities names in the genome_name\n  mutate(community = case_when(\n    str_detect(genome_name, \"Freshwater sediment microbial communities\") ~ \"Freshwater sediment microbial communitie\",\n    str_detect(genome_name, \"Freshwater biofilm microbial communities\") ~ \"Freshwater biofilm microbial communities\",\n    str_detect(genome_name, \"Freshwater microbial communities\") ~ \"Freshwater microbial communities\",\n    str_detect(genome_name, \"Soil microbial communities\") ~ \"Soil microbial communities\",\n    TRUE ~ NA_character_\n  )) |&gt; \n  \n  # Create a column type that is either Freshwater or Soil\n  mutate(type = case_when(\n    str_detect(genome_name, \"Freshwater sediment microbial communities\") ~ \"Freshwater\",\n    str_detect(genome_name, \"Freshwater biofilm microbial communities\") ~ \"Freshwater\",\n    str_detect(genome_name, \"Freshwater microbial communities\") ~ \"Freshwater\",\n    str_detect(genome_name, \"Soil microbial communities\") ~ \"Soil\",\n    TRUE ~ NA_character_\n  )) |&gt; \n  \n  # Get rid of the communities strings\n  mutate_at(\"genome_name\", str_replace, \"Freshwater sediment microbial communities from \", \"\") |&gt; \n  mutate_at(\"genome_name\", str_replace, \"Freshwater biofilm microbial communities from\", \"\") |&gt; \n  mutate_at(\"genome_name\", str_replace, \"Freshwater microbial communities from \", \"\") |&gt; \n  mutate_at(\"genome_name\", str_replace, \"Soil microbial communities from \", \"\") |&gt; \n\n  # separate site from sample name \n  separate(genome_name, c(\"site\",\"sample_name\"), \" - \") |&gt;   \n  \n  # Deal with these unknow fields in the sample name by creating a new column and removing them from the sample name\n  mutate(sample_unknown = case_when(\n    str_detect(sample_name, \".SS.\") ~ \"SS\",\n    str_detect(sample_name, \".C0.\") ~ \"C0\",\n    str_detect(sample_name, \".C1.\") ~ \"C1\",\n    str_detect(sample_name, \".C2.\") ~ \"C2\",\n    TRUE ~ NA_character_\n  )) |&gt; \n  \n  mutate_at(\"sample_name\", str_replace, \".SS\", \"\") |&gt; \n  mutate_at(\"sample_name\", str_replace, \".C0\", \"\") |&gt; \n  mutate_at(\"sample_name\", str_replace, \".C1\", \"\") |&gt; \n  mutate_at(\"sample_name\", str_replace, \".C2\", \"\") |&gt; \n  \n\n  # Get rid of the the common strings at the end of sample names\n  mutate_at(\"sample_name\", str_replace, \"-GEN-DNA1\", \"\") |&gt;  \n  mutate_at(\"sample_name\", str_replace, \"-COMP-DNA1\", \"\") |&gt;  \n  mutate_at(\"sample_name\", str_replace, \"-COMP-DNA2\", \"\") |&gt;  \n  mutate_at(\"sample_name\", str_replace, \".DNA-DNA1\", \"\") |&gt;\n  mutate_at(\"sample_name\", str_replace, \"_v2\", \"\") |&gt;\n  mutate_at(\"sample_name\", str_replace, \" \\\\(version 2\\\\)\", \"\") |&gt;\n  mutate_at(\"sample_name\", str_replace, \" \\\\(version 3\\\\)\", \"\") |&gt;\n  \n# Separate out the taxonomy groups\n  separate(gtdb_taxonomy_lineage, c(\"domain\", \"phylum\", \"class\", \"order\", \"family\", \"genus\"), \"; \", remove = FALSE)\n\n\n\n\nThis block is for separating out the freshwater sample names\n\n\nR code\nNEON_MAGs_freshwater &lt;- NEON_MAGs_prelim |&gt;\n  filter(type == \"Freshwater\") |&gt;\n  # separate the Sample Name into Site ID and plot info\n  separate(sample_name, c(\"site_ID\",\"date\", \"layer\", \"subplot\"), \"\\\\.\", remove = FALSE) |&gt;\n  mutate(quadrant = NA_character_)\n\n\n\n\nThis block is for separating out the soil sample names\n\n\nR code\nNEON_MAGs_soil &lt;- NEON_MAGs_prelim |&gt;\n  filter(type == \"Soil\") |&gt;\n  # separate the Sample Name into Site ID and plot info\n  separate(sample_name, c(\"site_ID\",\"subplot.layer.date\"), \"_\", remove = FALSE,) |&gt; \n  # some sample names have 3 fields while others have a fourth field for the quadrant. This code create a field for the quadrant when present and adds na for samples from combined cores.\n  extract(\n    subplot.layer.date,\n    into = c(\"subplot\", \"layer\", \"quadrant\", \"date\"),\n    regex = \"^([^-]+)-([^-]+)(?:-([^-]+))?-([^-]+)$\",\n    remove = FALSE\n  ) |&gt;\n  mutate(quadrant = na_if(quadrant, \"\")) |&gt;\n  select(-subplot.layer.date)\n\n\n\n\nThis combines the soil and freshwater data frames\n\n\nR code\nNEON_MAGs &lt;-bind_rows(NEON_MAGs_freshwater, NEON_MAGs_soil) |&gt;\n  # This changes the format of the data column\n  mutate(date = ymd(date))"
  },
  {
    "objectID": "labs/lab7_covid.html",
    "href": "labs/lab7_covid.html",
    "title": "Lab 7 : Data Tidying, Transformation and Visualization with COVID-19 reporting data",
    "section": "",
    "text": "Understanding the sources of SARS-CoV-2 incidence reports\nAccessing data remotely\nWide and long table formats\nMore data visualization with ggplot2\nAnimation"
  },
  {
    "objectID": "labs/lab7_covid.html#learning-objectives",
    "href": "labs/lab7_covid.html#learning-objectives",
    "title": "Lab 7 : Data Tidying, Transformation and Visualization with COVID-19 reporting data",
    "section": "",
    "text": "Understanding the sources of SARS-CoV-2 incidence reports\nAccessing data remotely\nWide and long table formats\nMore data visualization with ggplot2\nAnimation"
  },
  {
    "objectID": "labs/lab7_covid.html#visualizing-covid-19-cases-deaths-and-recoveries",
    "href": "labs/lab7_covid.html#visualizing-covid-19-cases-deaths-and-recoveries",
    "title": "Lab 7 : Data Tidying, Transformation and Visualization with COVID-19 reporting data",
    "section": "Visualizing COVID-19 cases, deaths and recoveries",
    "text": "Visualizing COVID-19 cases, deaths and recoveries\nThe virus has been recently renamed based on phylogenetic analysis severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The disease caused by the virus is coronavirus disease (COVID-19). In this lab we will work with reporting data on COVID-19 cases, deaths and recoveries.\n\nIntroduction to JHU case tracking data\nResearchers (Ensheng Dong, Hongru Du, Lauren Gardner) at John Hopkins University developed an interactive dashboard to visual data and track reported cases of coronavirus disease 2019 (SARS-CoV-2) in real time. The underlying data is collated from the following sources was updated several times a day until March 2023 (For more recent views of the data see the (CDC tracker and NY Times Tracker)\nIt is important to understand that this data is only as accurate as the reporting and many cases of the disease go unreported because of a lack of testing. This some countries may have have confirmed cases because of more comprehensive testing. Thus, the reporting data represent a minimum number of cases.\nJHU researchers make data that goes into the dashboard available on Github repo for Novel Coronavirus (COVID-19) Cases. In this lab we will work with this data.\nLet’s take a look at the files and the structure of data in the files.\n\ncsse_covid_19_data\n\ncsse_covid_19_daily_reports\n\n03-11-2020.csv\n\n\n\nOpen up the file to look at the structure\nThe file contains the columns\nProvince/State Country/Region Last Update Confirmed Deaths Recovered Latitude Longitude\nIt is important to note that for some countries there is only one row, while for others (e.g. China and US) there are multiple rows representing different provinces or states. Thus, we will need to sum these rows to get a total count for the US and China when we make graphs. From experience in making this tutorial I know the Column names with / will cause errors in ggplot ()."
  },
  {
    "objectID": "labs/lab7_covid.html#on-the-computer",
    "href": "labs/lab7_covid.html#on-the-computer",
    "title": "Lab 7 : Data Tidying, Transformation and Visualization with COVID-19 reporting data",
    "section": "On the Computer",
    "text": "On the Computer\nLet’s start by loading tidyverse and a package lubridate for working with dates.\n\n\nR code\nlibrary(tidyverse)\nlibrary(lubridate)\n\n\n\nLoading data from a github repository\nWe could load data directly into R each time we render, it would make sure we have the most current data, but it is a time consuming step. This code is for loading the data from the site just once to your data folder. After you do this add the line #| eval: false to you code chunk so that you do not download the file each time your render (this will take a long time).\n\n\nR code\n download.file(url=\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\", \n               destfile = \"data/time_series_covid19_confirmed_global.csv\")\n\n\nThen we can load the data.\n\n\nR code\ntime_series_confirmed &lt;- read_csv(\"data/time_series_covid19_confirmed_global.csv\")|&gt;\n  rename(Province_State = \"Province/State\", Country_Region = \"Country/Region\")\n\n\nCheck the table properties to make sure the data imported as we expected. Click on the file in the top right corner of R Studio under Environment.\n\n\nData Tidying - Pivoting\nToday we will go over Chapter 5 in R for Data Sciences - Data Tiyding and Pivot\nAs noted above this data is in wide format. To convert to long format we can usepivot_longer\n\n\nR code\ntime_series_confirmed_long &lt;- time_series_confirmed |&gt; \n               pivot_longer(-c(Province_State, Country_Region, Lat, Long),\n                            names_to = \"Date\", values_to = \"Confirmed\") \n\n\n\n\nDates and time\nLet’s also change the format of Date to something that is easier to work with in graphs. See Chapter 17 in R for Data Sciences - Data Tiyding and Pivot\n\n\nR code\ntime_series_confirmed_long$Date &lt;- mdy(time_series_confirmed_long$Date)\n\n\nLet’s look at the format of the data frame of time_series_confirmed_long by clicking on it in the top right corner of R Studio under Environment."
  },
  {
    "objectID": "labs/lab7_covid.html#making-graphs-from-the-time-series-data",
    "href": "labs/lab7_covid.html#making-graphs-from-the-time-series-data",
    "title": "Lab 7 : Data Tidying, Transformation and Visualization with COVID-19 reporting data",
    "section": "Making Graphs from the time series data",
    "text": "Making Graphs from the time series data\nTo make a times series graph of the confirmed cases we need to summarize the Country date to count up the individual state data for the US.\n\n\nR code\ntime_series_confirmed_long|&gt; \n  group_by(Country_Region, Date) |&gt; \n  summarise(Confirmed = sum(Confirmed)) |&gt; \n  filter (Country_Region == \"US\") |&gt; \n  ggplot(aes(x = Date,  y = Confirmed)) + \n    geom_point() +\n    geom_line() +\n    ggtitle(\"US COVID-19 Confirmed Cases\")\n\n\n\n\n\n\n\n\n\nNow several countries on the same graph\n\n\nR code\ntime_series_confirmed_long |&gt; \n    group_by(Country_Region, Date) |&gt; \n    summarise(Confirmed = sum(Confirmed)) |&gt; \n    filter (Country_Region %in% c(\"China\",\"France\",\"Italy\", \n                                \"Korea, South\", \"US\")) |&gt; \n    ggplot(aes(x = Date,  y = Confirmed, color = Country_Region)) + \n      geom_point() +\n      geom_line() +\n      ggtitle(\"COVID-19 Confirmed Cases\")\n\n\n\n\n\n\n\n\n\nThe above graphs using the cumulative counts. Let’s make a new table with the daily counts using the tidverse/dyplr lag function which subtracts a row from the previous row.\n\n\nR code\ntime_series_confirmed_long_daily &lt;-time_series_confirmed_long |&gt; \n    group_by(Country_Region, Date) |&gt; \n    summarise(Confirmed = sum(Confirmed)) |&gt; \n    mutate(Daily = Confirmed - lag(Confirmed, default = first(Confirmed )))\n\n\nNow for a graph with the US data\n\n\nR code\ntime_series_confirmed_long_daily |&gt; \n    filter (Country_Region == \"US\") |&gt; \n    ggplot(aes(x = Date,  y = Daily, color = Country_Region)) + \n      geom_point() +\n      ggtitle(\"COVID-19 Confirmed Cases\")\n\n\n\n\n\n\n\n\n\nA line graph version of the above\n\n\nR code\ntime_series_confirmed_long_daily |&gt; \n    filter (Country_Region == \"US\") |&gt; \n    ggplot(aes(x = Date,  y = Daily, color = Country_Region)) + \n      geom_line() +\n      ggtitle(\"COVID-19 Confirmed Cases\")\n\n\n\n\n\n\n\n\n\nNow with a curve fit\n\n\nR code\ntime_series_confirmed_long_daily |&gt; \n    filter (Country_Region == \"US\") |&gt; \n    ggplot(aes(x = Date,  y = Daily, color = Country_Region)) + \n      geom_smooth() +\n      ggtitle(\"COVID-19 Confirmed Cases\")\n\n\n\n\n\n\n\n\n\nBy default, geom_smooth() adds a LOESS/LOWESS (Locally Weighted Scatterplot Smoothing) smoother to the data. That’s not what we’re after, though. Here is a fit using a generalized additive model (GAM)\n\n\nR code\ntime_series_confirmed_long_daily |&gt; \n    filter (Country_Region == \"US\") |&gt; \n    ggplot(aes(x = Date,  y = Daily, color = Country_Region)) + \n      geom_smooth(method = \"gam\", se = FALSE) +\n      ggtitle(\"COVID-19 Confirmed Cases\")"
  },
  {
    "objectID": "labs/lab7_covid.html#animated-graphs-with-gganimate",
    "href": "labs/lab7_covid.html#animated-graphs-with-gganimate",
    "title": "Lab 7 : Data Tidying, Transformation and Visualization with COVID-19 reporting data",
    "section": "Animated Graphs with gganimate",
    "text": "Animated Graphs with gganimate\nAnimated graphs when down right have a great visual impact. You can do this in R and have your animations embedded on your web page. Essentially gganimate creates a series of files that are encompassed in a gif file. In addition to having this gif as part of your report file, you can save the gif and use in a slide or other presentations. It just takes a few lines of code to covert and existing ggplot graph into an animation. See Tutorial for Getting Started with gganimate and gganimate: How to Create Plots with Beautiful Animation in R.\nThis are some important gganimate functions:\n\ntransition_*() defines how the data should be spread out and how it relates to itself across time.\nview_*() defines how the positional scales should change along the animation.\nshadow_*() defines how data from other points in time should be presented in the given point in time.\nenter_()/exit_() defines how new data should appear and how old data should disappear during the course of the animation.\nease_aes() defines how different aesthetics should be eased during transitions.\n\n\nInstalling gganimate and gifski\ngifski is a package for creating a gif file from gganimate. Use Tools &gt; Install Packages to install gifski and gganimate on Unity.\n\n\nR code\nlibrary(gganimate)\nlibrary(gifski)\ntheme_set(theme_bw())\n\n\n\n\nAn animation of the confirmed cases in select countries\n\n\nR code\ndaily_counts &lt;- time_series_confirmed_long_daily |&gt; \n      filter (Country_Region == \"US\")\n\np &lt;- ggplot(daily_counts, aes(x = Date,  y = Daily, color = Country_Region)) + \n        geom_point() +\n        ggtitle(\"Confirmed COVID-19 Cases\") +\n# gganimate lines  \n        geom_point(aes(group = seq_along(Date))) +\n        transition_reveal(Date) \n\n# make the animation\n animate(p, renderer = gifski_renderer(), end_pause = 15)\n\n\n\n\n\n\n\n\n\nYou can change the output to a gif file that can be used in slide presentations or an instagram post. After you make the gif set eval=FALSE in your report so that it doesn’t recreate the gif (this takes a fair amount of time) each time you Knit.\n\n\nR code\nanim_save(\"daily_counts_US.gif\", p)\n\n\n\n\nAnimation of confirmed deaths\n\nDownload the data\n\n\nR code\n# This download may take about 5 minutes. You only need to do this once so set `#| eval: false` in your qmd file\ndownload.file(url=\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv\", \n  destfile = \"data/time_series_covid19_deaths_global.csv\")\n\n\n\n\nData tidying, pivot and time\n\n\nR code\ntime_series_deaths_confirmed &lt;- read_csv(\"data/time_series_covid19_deaths_global.csv\")|&gt;\n  rename(Province_State = \"Province/State\", Country_Region = \"Country/Region\")\n\ntime_series_deaths_long &lt;- time_series_deaths_confirmed |&gt; \n    pivot_longer(-c(Province_State, Country_Region, Lat, Long),\n        names_to = \"Date\", values_to = \"Confirmed\") \n\ntime_series_deaths_long$Date &lt;- mdy(time_series_deaths_long$Date)\n\n\n\n\nMaking the animated graph\n\n\nR code\np &lt;- time_series_deaths_long |&gt;\n  filter (Country_Region %in% c(\"US\",\"Canada\", \"Mexico\",\"Brazil\",\"Egypt\",\"Ecuador\",\"India\", \"Netherlands\", \"Germany\", \"China\" )) |&gt;\n  ggplot(aes(x=Country_Region, y=Confirmed, color= Country_Region)) + \n    geom_point(aes(size=Confirmed)) + \n    transition_time(Date) + \n    labs(title = \"Cumulative Deaths: {frame_time}\") + \n    ylab(\"Deaths\") +\n    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n# make the animation\nanimate(p, renderer = gifski_renderer(), end_pause = 15)"
  },
  {
    "objectID": "labs/lab7_covid.html#exercises",
    "href": "labs/lab7_covid.html#exercises",
    "title": "Lab 7 : Data Tidying, Transformation and Visualization with COVID-19 reporting data",
    "section": "Exercises",
    "text": "Exercises\nPay attention to how your graphs look in today’s final rendered lab report. You will be docked points if the graphs do not look nice (e.g. overlapping column names, truncated legends, ets.)\n\nExercise 1\nGo through Chapter 5 in R for Data Sciences - Data Tiyding and Pivot](https://r4ds.hadley.nz/data-tidy.html) putting the examples and exerices into your report as in Lab 2 and 3\n\n\nExercise 2\nInstead of making a graph of 5 countries on the same graph as in the above example, use facet_wrap with scales=\"free_y\".\n\n\nExercise 3\nUsing the daily count of confirmed cases, make a single graph with 5 countries of your choosing.\n\n\nExercise 4\nPlot the cumulative deaths in the US, Canada and Mexico (you will need to download time_series_covid19_deaths_global.csv)\n\n\nExercise 5\nMake a graph with the countries of your choice using the daily deaths data\n\n\nExercise 6\nMake an animation of your choosing (do not use a graph with geom_smooth)"
  },
  {
    "objectID": "labs/lab3s_github.html",
    "href": "labs/lab3s_github.html",
    "title": "Lab S3 - “Connecting a Github repo site with a new RStudio project”",
    "section": "",
    "text": "Much of the material for this lesson was borrowed from or inspired by Matt Jones’ NCEAS Reproducible Research Techniques for Synthesis workshop"
  },
  {
    "objectID": "labs/lab3s_github.html#learning-objectives",
    "href": "labs/lab3s_github.html#learning-objectives",
    "title": "Lab S3 - “Connecting a Github repo site with a new RStudio project”",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nIn this lesson, you will learn:\n\nWhat computational reproducibility is and why it is useful\nHow version control can increase computational reproducibility\nHow to start a Github repo and Pages site\nto set up your own RStudio Project and sync with your GitHub repo\nHow to make a simple web page for your site"
  },
  {
    "objectID": "labs/lab3s_github.html#background",
    "href": "labs/lab3s_github.html#background",
    "title": "Lab S3 - “Connecting a Github repo site with a new RStudio project”",
    "section": "Background",
    "text": "Background\n\nReproducible Research\nReproducibility is the hallmark of science, which is based on empirical observations coupled with explanatory models. While reproducibility encompasses the full science lifecycle, and includes issues such as methodological consistency and treatment of bias, in this course we will focus on computational reproducibility: the ability to document data, analyses, and models sufficiently for other researchers to be able to understand and ideally re-execute the computations that led to scientific results and conclusions.\n\n\nWhat is needed for computational reproducibility?\nThe first step towards addressing these issues is to be able to evaluate the data, analyses, and models on which conclusions are drawn. Under current practice, this can be difficult because data are typically unavailable, the method sections of papers do not detail the computational approaches used, and analyses and models are often conducted in graphical programs, or, when scripted analyses are employed,the code is not available.\nAnd yet, this is easily remedied. Researchers can achieve computational reproducibility through open science approaches, including straightforward steps for archiving data and code openly along with the scientific workflows describing the provenance of scientific results (e.g., @hampton_tao_2015, @munafo_manifesto_2017).\n\n\nConceptualizing workflows\nScientific workflows encapsulate all of the steps from data acquisition, cleaning, transformation, integration, analysis, and visualization.\n\nWorkflows can range in detail from simple flowcharts to fully executable scripts. R scripts and python scripts are a textual form of a workflow, and when researchers publish specific versions of the scripts and data used in an analysis, it becomes far easier to repeat their computations and understand the provenance of their conclusions.\n\n\nThe problem with filenames\nEvery file in the scientific process changes. Manuscripts are edited. Figures get revised. Code gets fixed when problems are discovered. Data files get combined together, then errors are fixed, and then they are split and combined again. In the course of a single analysis, one can expect thousands of changes to files. And yet, all we use to track this are simplistic filenames. You might think there is a better way, and you’d be right: version control.\nVersion control systems help you track all of the changes to your files, without the spaghetti mess that ensues from simple file renaming. In version control systems like git, the system tracks not just the name of the file, but also its contents, so that when contents change, it can tell you which pieces went where. It tracks which version of a file a new version came from. So its easy to draw a graph showing all of the versions of a file, like this one:\n\nVersion control systems assign an identifier to every version of every file, and track their relationships. They also allow branches in those versions, and merging those branches back into the main line of work. They also support having multiple copies on multiple computers for backup, and for collaboration. And finally, they let you tag particular versions, such that it is easy to return to a set of files exactly as they were when you tagged them. For example, the exact versions of data, code, and narrative that were used when a manuscript was originallysubmitted might be eco-ms-1 in the graph above, and then when it was revised and resubmitted, it was done with tag eco-ms-2. A different paper was started and submitted with tag dens-ms-1, showing that you can be working on multiple manuscripts with closely related but not identical sets of code and data being used for each, and keep track of it all.\n\n\nVersion control and Collaboration using Git and GitHub\nFirst, just what are git and GitHub?\n\ngit: version control software used to track files in a folder (a repository)\n\ngit creates the versioned history of a repository\n\nGitHub: web site that allows users to store their git repositories and share them with others\n\n\n\nThe Git lifecycle\nAs a git user, you’ll need to understand the basic concepts associated with versioned sets of changes, and how they are stored and moved across repositories. Any given git repository can be cloned so that it exist both locally, and remotely. But each of these cloned repositories is simply a copy of all of the files and change history for those files, stored in git’s particular format. For our purposes, we can consider a git repository just a folder with a bunch of additional version-related metadata.\nIn a local git-enabled folder, the folder contains a workspace containing the current version of all files in the repository. These working files are linked to a hidden folder containing the ‘Local repository’, which contains all of the other changes made to the files, along with the version metadata.\nSo, when working with files using git, you can use git commands to indicate specifically which changes to the local working files should be staged for versioning (using the git add command), and when to record those changes as a version in the local repository (using the command git commit).\nThe remaining concepts are involved in synchronizing the changes in your local repository with changes in a remote repository. The git push command is used to send local changes up to a remote repository (possibly on GitHub), and the git pull command is used to fetch changes from a remote repository and merge them into the local repository.\n\n\ngit clone: to copy a whole remote repository to local\ngit add (stage): notify git to track particular changes\ngit commit: store those changes as a version\ngit pull: merge changes from a remote repository to our local repository\ngit push: copy changes from our local repository to a remote repository\ngit status: determine the state of all files in the local repository\ngit log: print the history of changes in a repository\n\nThose seven commands are the majority of what you need to successfully use git. But this is all super abstract, so let’s explore with some real examples."
  },
  {
    "objectID": "labs/lab3s_github.html#on-the-computer",
    "href": "labs/lab3s_github.html#on-the-computer",
    "title": "Lab S3 - “Connecting a Github repo site with a new RStudio project”",
    "section": "On the Computer",
    "text": "On the Computer\n\nOn the github website\n\nRegister for a github account on https://github.com/\nClick on the Repository link\nClick on the New Repository button to create a new repository.\nGive it a name, make it public, Add a readme file, Add R .gitignore file, Add Apache license.\nEnable a web page for the repo - Click of the settings wheel, in the left menu select pages then under branch select main and save. \nGo back to your main repo home page, in the right corner in the About section click the wheel. Select Use your GitHub Pages website. Save changes\nClick on the green code button and copy the https link. \n\n\n\nMake a local version of your github repo in Unity\n\nIn Your Home Directory click on File &gt; New Project thenNew Project from Git Repository`\nSelect Version Control\nSelect Git\nPaste in link your github site\nChoose the directory you want to use for the project\nPaste in the link to your Github repository (e.g https://github.com/jeffreyblanchard/pathogen-test.git) \n\n\n\nConfigure your local repo to connect with the github repo\n\nInstall the R package usethis using Tools &gt; Install Packages\nConfigure git with your username and email. This must be the username and email associated with your GitHub account.\n\n\n\nR code\nlibrary(usethis)\nuse_git_config(user.name = \"Jane Doe\", user.email = \"jane@example.org\")\n\n\n * Create a github token\n\n\nR code\nusethis::create_github_token()\n\n\nThis will open a web page on your github account. The recommended scopes will be pre-selected. This will be fine for now and you can change later if needed.\n\nClick “Generate token”.\nCopy the generated PAT (beginning with ghp)to your clipboard. Provide this PAT next time a Git operation asks for your password.\nTo link your PAT with your repo\n\n\n\nR code\ngitcreds::gitcreds_set() \n\n\nto get a prompt where you can paste your PAT: ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n\nMake an index file as the homepage for your Github repo\n\nIn RStudio open a new Quarto markdown template\nSave it as index.qmd (IMPORTANT - make sure it is all lower case)\nUse the YAML block from your labs\nAdd an About Me, Image and a link to lab 7 (or all labs)\nRender it to make index.html\nUnder the git tab in the upper right corner, select commit the changes`\nAdd a description of your changes in the commit messeage\nClick on Commit\nClose the window\nClick on Push to copy the index files to your github repo \nThis will automatically replace your homepage README.md file with index.html (it takes about 5 minutes before the page updates)\nIt will take about 5 minutes before you can see it on your github repo page\nYou can quickly spice up your page by adding a theme to your YAML block of your index.qmd (available themes are “default”, “bootstrap”, “cerulean”, “cosmo”, “darkly”, “flatly”, “journal”, “lumen”, “paper”, “readable”, “sandstone”, “simplex”, “spacelab”, “united”, “yeti”)\nYou could also try [prettydoc](https://prettydoc.statr.me/0\n\n\n\nExample Repo\nHere is what the example repo created above looks like."
  },
  {
    "objectID": "labs/lab3s_github.html#linking-an-existing-r-project-with-a-new-github-repo",
    "href": "labs/lab3s_github.html#linking-an-existing-r-project-with-a-new-github-repo",
    "title": "Lab S3 - “Connecting a Github repo site with a new RStudio project”",
    "section": "Linking an existing R project with a new Github repo",
    "text": "Linking an existing R project with a new Github repo\n\nOn the github website\n\nCreate a new repo by clicking on the new repository button.\nGive it a name, make it public. Do not add readme, .gitignore or license files\nCopy the link to the site.\n\n\n\nEnable git in Rstudio\n\nOpen your project in Rstudio and navigate to Tools -&gt; Version Control -&gt; Project Setup\nClick SVN/Git tab and select git as the version control system. It will ask you to initialize a new git repo and restart Rstudio\nAfter Rstudio reopens, confirm that there is a Git tab in the environment pane\n\n\n\nIn the R Console (bottom left)\n\nIn the R Console Add your git credentials with the usethis package and your token begining with ghp...\n\n\n\nR code\nlibrary(usethis)\nuse_git_config(user.name = \"jeffreyblanchard\", user.email = \"jlb@umass.edu\")\ngitcreds::gitcreds_set() \n\n\nIf you have forgotten your token. Create a new token using\n\n\nR code\nlibrary(usethis)\nusethis::create_github_token()\n\n\n\n\nIn the Terminal Window (bottom left)\nNow in the Terminal window establish a connection to your github repo and make your first commit\n\n\nR code\necho \"# test-project\" &gt;&gt; README.md\ngit add README.md\ngit commit -m \"first commit\"\ngit branch -M main\ngit remote add origin https://github.com/jeffreyblanchard/test-project.git\ngit push -u origin main\n\n\nThis only pushes the readme to github. Now under the Git tab in RStudio (top right pane) check your files, commit and then push.\n\n\nTo enable a web page for your repo\n\nEnable a web page for the repo - Click of the settings wheel, in the left menu select pages then under branch select main and save. \nGo back to your main repo page, in the right corner in the About section click the wheel. Select Use your GitHub Pages website. Save changes\nClick on the green code button and copy the https link."
  },
  {
    "objectID": "labs/lab3s_github.html#linking-an-existing-r-project-with-a-new-github-repo-not-intended-for-most-people",
    "href": "labs/lab3s_github.html#linking-an-existing-r-project-with-a-new-github-repo-not-intended-for-most-people",
    "title": "Lab S3 - “Connecting a Github repo site with a new RStudio project”",
    "section": "Linking an existing R project with a new Github repo (not intended for most people)",
    "text": "Linking an existing R project with a new Github repo (not intended for most people)\n\nOn the github website\n\nCreate a new repo by clicking on the new repository button.\nGive it a name, make it public. Do not add readme, .gitignore or license files\nCopy the link to the site.\n\n\n\nEnable git in Rstudio\n\nOpen your project in Rstudio and navigate to Tools -&gt; Version Control -&gt; Project Setup\nClick SVN/Git tab and select git as the version control system. It will ask you to initialize a new git repo and restart Rstudio\nAfter Rstudio reopens, confirm that there is a Git tab in the environment pane\n\n\n\nIn the R Console (bottom left)\n\nIn the R Console Add your git credentials with the usethis package and your token begining with ghp...\n\n\n\nR code\nlibrary(usethis)\nuse_git_config(user.name = \"jeffreyblanchard\", user.email = \"jlb@umass.edu\")\ngitcreds::gitcreds_set() \n\n\nIf you have forgotten your token. Create a new token using\n\n\nR code\nlibrary(usethis)\nusethis::create_github_token()\n\n\n\n\nIn the Terminal Window (bottom left)\nNow in the Terminal window establish a connection to your github repo and make your first commit\n\n\nR code\necho \"# test-project\" &gt;&gt; README.md\ngit add README.md\ngit commit -m \"first commit\"\ngit branch -M main\ngit remote add origin https://github.com/jeffreyblanchard/test-project.git\ngit push -u origin main\n\n\nThis only pushes the readme to github. Now under the Git tab in RStudio (top right pane) check your files, commit and then push.\n\n\nTo enable a web page for your repo\n\nEnable a web page for the repo - Click of the settings wheel, in the left menu select pages then under branch select main and save. \nGo back to your main repo page, in the right corner in the About section click the wheel. Select Use your GitHub Pages website. Save changes\nClick on the green code button and copy the https link."
  },
  {
    "objectID": "labs/lab8_ggtree.html",
    "href": "labs/lab8_ggtree.html",
    "title": "Lab 8 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "",
    "text": "This lab is a slighlty modified update of Stephen Turner’s ggtree tutorial. Many of the examples here were modified from the ggtree vignettes."
  },
  {
    "objectID": "labs/lab8_ggtree.html#acknowledgements",
    "href": "labs/lab8_ggtree.html#acknowledgements",
    "title": "Lab 8 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "",
    "text": "This lab is a slighlty modified update of Stephen Turner’s ggtree tutorial. Many of the examples here were modified from the ggtree vignettes."
  },
  {
    "objectID": "labs/lab8_ggtree.html#learning-objectives",
    "href": "labs/lab8_ggtree.html#learning-objectives",
    "title": "Lab 8 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nMultiple Sequence Alignment\nPhylogenetic Inference\nTree visualization using ggtree\n\n/work/pi_bio678_umass_edu/data/lab13/"
  },
  {
    "objectID": "labs/lab8_ggtree.html#sequence-alignment-background",
    "href": "labs/lab8_ggtree.html#sequence-alignment-background",
    "title": "Lab 8 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Sequence Alignment Background",
    "text": "Sequence Alignment Background\nSequence Alignment is one of the most fundamental operations of bioinformatics. Sequence alignments are used to decide if genes or proteins are related functionally, structurally and evolutionarily. It is also at the core of sequence search methods such as BLAST.\nThere are three principal methods of pair-wise sequence alignment\n\nDot matrix analysis.\nThe dynamic programming (or DP) algorithm.\nWord or k-tuple methods (such as those used in BLAST and FASTA - which we will discuss later)\n\n\nDot matrix analysis\nThis method involves constructing a matrix with one of the sequences to be compared running horizontally across the bottom, and the other running vertically along the left-hand side. Each entry of the matrix is a measure of similarity of those two residues on the horizontal and vertical sequence. The simplest scoring system, distinguishes only between identical (dots) and non-identical (blank) residues.\nThe dot plot displays graphically any possible sequence alignments as diagonals on the matrix. Insertions, deletions, direct repeats and inverted repeats can be visually detected on a dot plot. Dot plot can also detect regions of RNA that are self-complementary and thus might form secondary structure.\n\nRef:\nThis dot plot represents an alignment of two different strains of Helicobacter pylori. Forward matches are shown in red, while reverse matches are shown in green. This alignment clearly shows a major inversion event centered around the origin of replication.UM\n\n\nThe dynamic programming algorithm\n“Dynamic programming was formalized by mathematician Richard Bellman, who was working at RNAD corporation on optimal decision processes. He wanted to concoct an impressive name that would shield his work from US Secretary of Defense Charles Wilson, a man know to be hostile to mathematics research. His work involved times series and planning - thus ‘dynamic’ and ‘programming’ (note, nothing particularly to do with computer programming). Bellman especially liked ‘dynamic’ because”it’s impossible to use the work dynamic in the pejorative sense”; he figured dynamic programming was “something not even a Congressman could object to” (Bellman 1984).” (Eddy 2004)\nThe dynamic programming algorithm is at the heart of many bioinformatics programs including BLAST (sequence searching), FASTA (sequence searching), CLUSTALW (multiple sequence alignmnet), HMMER (profile searching), GENSCAN (gene finding), MFOLD (RNA folding), and PHYLIP (phylogenetic analysis). Dynamic programing was first used for global alignment of protein sequences by Needleman and Wunnsch (1970) and to find local alignments by Smith and Waterson (1981).\n  MLIGKVIGSVVSTRKN-LNVGKFMIVEPLK\n  |     |  |  ||   || ||||     |        Global Alignment\n  MVAARLI-VWANTRKTSLNLGKFMLAEIIK\n  \n              TRKN-LNVGKFM\n              ||   || ||||               Local Alignment\n              TRKTDLNLGKFML\n\n\n\nSubsitution penalties differ for particular nucleotides or aminoacids\nAmino acids are put together into proteins based on their three nucleotide codons, and most mutational events usually only change one nucleotide at time. For example, starting with the Alanine codon GCU and looking at all possible changes on a http://en.wikipedia.org/wiki/Genetic_code#RNA_codon_table\nGCU -&gt; UCU = Alanine (A) -&gt; Serine (S)\nGCU -&gt; ACU = Alanine (A) -&gt; Threonine (T)\nGCU -&gt; CCU = Alanine (A) -&gt; Proline (P)\nGCU -&gt; GUU = Alanine (A) -&gt; Valine (V)\nGCU -&gt; GAU = Alanine (A) -&gt; Aspartic Acid (D)\nGCU -&gt; GGU = Alanine (A) -&gt; Glycine (G)\nGCU -&gt; GCA = Alanine (A) -&gt; Alanine (A)\nGCU -&gt; GCC = Alanine (A) -&gt; Alanine (A)\nGCU -&gt; GCG = Alanine (A) -&gt; Alanine (A)\nThus, from the Alanine codon GCU with a single substitution we can only get to 6 of the 19 other amino acids and 33% of the changes are going to result in keeping Alanine at the site in the protein, the other six amino acids would result 11% of the time. Even if we look at all codons for Alanine, there are still 12 amino acids that can not be made by single substitution. Thus, the frequency of changing Alanine to another amino acid by a single nucleotide change would be\n       A   R   N   D   C   Q   E   G   H   I   L   K   M   F   P   S   T   W   Y   V\n    A .33  0   0  .06  0   0  .06 .11  0   0   0   0   0   0  .11 .11 .11  0   0  .11\n\n\n\nOrigin of protein substitution matrices and bioinformatics\nProtein substitution matrices were first developed by Margaret Dayhoff, one of the founders of the field of bioinformatics. She was particularly interested in the possibility of deducing the evolutionary relationships of organisms from protein sequences. Toward these ends she collected all the known protein sequences and, as a service to the scientific community, made them available to others in 1965 in a small book, the first Atlas of Protein Sequence and Structure.\nRef: http://en.wikipedia.org/wiki/Margaret_Oakley_Dayhoff\nFrom these sequences, she and her coworkers developed a model of protein evolution which resulted in the development of a set of widely used substitution matrices. These are frequently called Dayhoff or PAM (Percent Accepted Mutation) matrices and similar matrices are used in many areas of bioinformatics including: sequence similarity searches (e.g. BLAST), multiple sequence alignment tools (e.g ClustalW), phylogenetics and identifying functional regions of proteins.\n\n\nNatural selection governs which amino acid changes are observed\nJust like mutation mucks with the frequency matrix so does Natural Selection. Changes that occur between amino acids that have different biochemical properties are likely to affect the function of the protein. Therefore, a substitution is more likely to occur between amino acids with similar biochemical properties. In the above example with the Alanine codon a substitution that yields an amino acid change would result in mostly changes to other neutral amino acids, whereas the frequency of change to Aspartic Acid would probably be much lower. Amazingly, the genetic code has evolved to minimize changes between amino acids with different biochemical properties.\nAmino Acid          Side chain polarity Side chain acidity or basicity\n Alanine              nonpolar            neutral\n Arginine             polar                  strongly basic\n Asparagine           polar                  neutral\n Aspartic acid        polar                  acidic\n Cysteine             polar                  neutral\n Glutamic acid        polar                  acidic\n Glutamine            polar                  neutral\n Glycine              nonpolar               neutral\n Histidine            polar                  weakly basic\n Isoleucine           nonpolar               neutral\n Leucine              nonpolar               neutral\n Lysine               polar                  basic\n Methionine           nonpolar               neutral\n Phenylalanine        nonpolar               neutral\n Proline              nonpolar               neutral\n Serine               polar                  neutral\n Threonine            polar                  neutral\n Tryptophan           nonpolar               neutral\n Tyrosine             polar                  neutral\n Valine               nonpolar               neutral\n\n\n\nAccounting for multiple substitutions\nAs time goes on and sequence divergence gets larger it gets harder to account for multiple substitutions at the same amino acid position. PAM protein matrices are based on global alignments of closely related proteins. However, sequence changes over long evolutionary time scales are not well approximated by compounding small changes that occur over short time scales. For comparing more distantly related sequence other types of matrices are used. One of the most common is the The BLOSUM (BLOck SUbstitution Matrix) series of matrices created by Steve Henikoff and colleagues. Both the PAM and BLOSUM are not expressed as transformation frequencies but the probabilities of transformation are expressed by log-odds scores as shown below for a BLOSUM62 matrix.\n\nRef: http://en.wikipedia.org/wiki/Substitution_matrix"
  },
  {
    "objectID": "labs/lab8_ggtree.html#multiple-sequence-alignment",
    "href": "labs/lab8_ggtree.html#multiple-sequence-alignment",
    "title": "Lab 8 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Multiple Sequence Alignment",
    "text": "Multiple Sequence Alignment\nWhy is multiple seqence alignment difficult?\nIf sequences are all the same length, alignment is trivial:\n    KNMITGAAQMDGAILVVAATDGPMPQTREHVLLARQVEVP\n    KNMITGAAQMDGAILVVSATDGAMPQTKEHILLARQVGVP\n    KNMITGAAQMDGAILVVSATDGAMPQTKEHILLARQVGVP\n    KNMITGAAQMDGAILVVSAADGPMPQTREHILLARQVGVP\nThis sequence alignment is unambiguous because there is no length variation among the sequences. No indels are needed to make the alignment, and the ungapped sequences can simply be arranged together. However, if the sequenes are of various lengths, problem is more complex, potentially very complex:\n    RGSALKAVEAPNDPNHEA......YKPIQELLDAMDN.....YIPDPQRDVDKPFL\n    RGSALKALEGDAAYIEKVR..........ELMQAVDD.....NIPTPEREIDKPFL\n    RGSALKALE.....IEKVR..........ELMQAGDAAYVDDNIPTPEREIDKPFL\n    RGSALLALEEMHKNPKTKRGENEWVDKIWELLDAIDE.....YIPTPVRDVDKPFL\n    RGSALLALEQMHRNPKTRRGENEWVDKIWELLDAIDE.....YIPTPVRDVDKPFL\n    KGSALQALEALQANPKTARGEDKWVDRIWELLDAVDS.....YIPTPERATDKTFL\n    RGTARNALESPSKDIN....APEY.KCILELMNAVDE.....YIPTPQRAVDQPFL\n    KGSALQALE....NAE....DEEKTKCIWELLQAMDD.....YIPAPERDIDKPFL\n    KGSAFGAMS....NPE....DPESTKCVKELLESMDN.....YFDLPERDIDKPFL\n    RGSAFAAMS....KPD....DPAATKCLDELLDTMDK.....YFVIPERALDKPFL\nIn many cases the best position to place an indel is ambiguous. Ideally, one would know the phylogeny for the sequences; this would help infer the sequence of indels. Unfortunately one normally needs a multiple sequence alignment to make inferences about how the sequences are related. Most alignment algorithms make a quick approximation of phylogeny, and then base alignments on these. Sound circular? You are right and this is a challenging problem that is at the forefront of research in phylogenetics…the joint estimation of the alignment and phylogeny. For this class we will stick to the traditional method of first aligning sequences followed by phylogenetic analysis.\nProgressive alignment methods are efficient enough to implement on a large scale for many (100s to 1000s) sequences. Progressive alignment services are commonly available on publicly accessible web servers so users need not locally install the applications of interest. The most popular progressive alignment method has been the Clustal family Different portals or implementations can vary in user interface and make different parameters accessible to the user. Clustal Omega is used extensively for phylogenetic tree construction.\nThe basic steps in Clustal are:\n\nCalculate all possible pairwise alignments, record the score for each pair\nCalculate a guide tree based on the pairwise distances (algorithm: Neighbor Joining)\nFind the two most closely related sequences\nAlign these sequences (algorithm: Smith-Waterman).\n\nCalculate a consensus of this alignment\nReplace the two sequences with the consensus\nFind the two next-most closely related sequences (one of these could be a previously determined consensus sequence).\nIterate until all sequences have been aligned\n\nExpand the consensus sequences with the (gapped) original sequences\nReport the multiple sequence alignment\n\n\nSoftware for sequence alignment\nThere are many tools available for sequence alignment. The common tools are hosting at the European Bioinformatics Institute.\n\nPairwise sequence alignment\nMultiple sequence alignment\n\nThe most commonly used are Clustal, Muscle and MAFFT. MAFFT is commonly implemented for working with large data sets and where speed is important (e.g. web servers)"
  },
  {
    "objectID": "labs/lab8_ggtree.html#on-the-computer",
    "href": "labs/lab8_ggtree.html#on-the-computer",
    "title": "Lab 8 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "On the Computer",
    "text": "On the Computer\n\nPhylogenetic analysis\nCarl Woese sequenced isolated and sequenced ribosomal RNA to discover a new domain of life, the Archaea. His discovery was dependent on using phylogenetic methods to determine the relationship of his microbial sequence to other sequences in the database. Since his discovery, progress sequencing specfic genes from genomic DNA has greatly simplified the process of understanding microbial phylogenies. In this project you will be given a DNA sequence, then construct a multiple sequence alignment and do a phylogenetic analysis to show the relationships among the taxa represented by the DNA sequences.\nFrom a practical perspective, phylogenetic analysis is broken up into methods related to tree building and tree visualization. I have assembled a core group DNA sequences of small subunit ribosomal gene from our national DNA database Genbank to provide an example Tree_of_Life_Core_Sequences.txt. The files contains data for the following species.\n\nThermoplasma volcanium\nHalalkalicoccus jeotgali\nCandidatus Korarchaeum cryptofilum\nNanoarchaeum equitans\nChlorobium chlorochromatii\nBurkholderia cenocepacia\nRhizobium leguminosarum\nEscherichia coli\nArchaeoglobus fulgidus\nMethanocaldococcus jannaschii\nPyrococcus abyssi\nOryza sativa nuclear\nOryza sativa mitochondrion\nOryza sativa chloroplast\nSaccharomyces cerevisiae nuclear\nHomo sapies nuclear\nDrosophila yakuba nuclear\nAmphidinium carterae nuclear\nThermotoga lettingae\nProchlorococcus marinus\nTrypanosoma cruzi\n\n\n\nNGPhylogeny.fr - A quick and easy working flow including multiple sequence alignment, phylogenetic analysis and tree visualization\nThis web server is great place to learn the basic workflow for phylogenetic analysis and I use it on occasion when I just need a quick see of the relationships among a small set of sequences. Doing a phylogenetic analysis involves (1) assembling a group of sequences to evaluate, (2) aligning the sequences so that a distance can be calculate between the sequences, (3) determining the relationships among the sequences, and (4) visualizing the relationships. Step 1 was accomplished above by getting representative archaeal, bacterial and eukaryotic sequences from GenBank. Steps 2-4 will be done on web server that has been set up to run these steps at the same time. Go to the site Robust Phylogenetic Analysis For The Non-Specialist. Scroll down and under Phylogenetic Analysis select “One Click”. Upload or paste in your DNA sequences from the file in Moodle. Then click submit. This will start the analysis process. The numbers of the tree represent statistical support for the relationship. The more robust the relationship the closer the value will be to 1. The scale at the bottom represents sequence distance (e.g. 0.2 is 20%).\nThe newick tree file was added to the /work/pi_bio678_umass_edu/data/lab13/ directory"
  },
  {
    "objectID": "labs/lab8_ggtree.html#phylogenetic-tree-visualization",
    "href": "labs/lab8_ggtree.html#phylogenetic-tree-visualization",
    "title": "Lab 8 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Phylogenetic tree visualization",
    "text": "Phylogenetic tree visualization\nThe tree visualizations on NGPhylogeny.fr are ok for some purposes, but often for publications more tree editing and visualization features are needed. There are a number of packages available to bring trees to like including Figtree, Dendroscope and iTOL. ETE Toolkit is a framework for analyzing and visualizing trees in Python. For a more complete list see Wikipedia.\nHistorically most phylogenetic analysis and tree visualization has been done with independent software tools. There has been a growing move towards using R for phylogenetic analyses. The package ape: Analysis of Phylogenetics and Evolution can access popular multiple sequence alignment and phylogenetic analysis methods. ggtree extends the ‘ggplot2’ for visualization and annotation of phylogenetic trees with their annotation data. The author, Guangchuang Yu, has recently published an R book Data Integration, Manipulation and Visualization of Phylogenetic Trees.\n\nBioconductor\nBioconductor is an open source, open development software project for genomic data analysis. There are thousands of software packages available and an active user community. R/Bioconductor is one of the primary mechanisms for publishing new genomic data analysis tools. We will use Bioconductor in the this and the following weeks labs.\n\n\nInstalling Bioconductor, ggtree and Biostrings\nBioconductor, ggtree and Biostrings is already installed on Posit.\nInstall Bioconductor locally on your computer or Unity by typing in the R console\n\n\nR code\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(version = \"3.19\")\n\n\nYou may get an error message that BioConductor is already installed.\nIndividual packages that are not part of the BioConductor core such as ggtree and Biostrings can be installed using\n\n\nR code\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n\nBiocManager::install(\"ggtree\")\n\n\n\n\nR code\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n\nBiocManager::install(\"Biostrings\")"
  },
  {
    "objectID": "labs/lab8_ggtree.html#the-ggtree-package",
    "href": "labs/lab8_ggtree.html#the-ggtree-package",
    "title": "Lab 8 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "The ggtree Package",
    "text": "The ggtree Package\nggtree is an R package that extends ggplot2 for visualizating and annotating phylogenetic trees. It is available from Bioconductor. Bioconductor is a project to provide tools for analyzing and annotating various kinds of genomic data. You can search and browse Bioconductor packages here.\n\nggtree Bioconductor page: bioconductor.org/packages/ggtree.\nggtree homepage: guangchuangyu.github.io/ggtree (contains more information about the package, more documentation, a gallery of beautiful images, and links to related resources).\nggtree publication: Yu, Guangchuang Data Integration, Manipulation and Visualization of Phylogenetic Trees\n\nBioconductor packages usually have great documentation in the form of vignettes. Take a look at the landing page for ggtree – about halfway down the page under the “Documentation” heading there are multiple walkthrough tutorials directed to different applications and functionalities of ggtree, chock full of runnable examples and explanations.\nJust like R packages from CRAN, you only need to install Bioconductor packages once, then load them every time you start a new R session. Load the tidyverse package first.\n\n\nR code\nlibrary(tidyverse)\nlibrary(ggtree)\n# added treeio and ggimage library\nlibrary(treeio)\nlibrary(rphylopic)\nlibrary(ggimage)\n\n\nA note on masked functions: Take a second and look through some of the output that you see when you load ggtree after tidyverse. When you first installed ggtree it may have taken a while, because ggtree depends on a number of other R packages. Each of these, in turn, may depend on other packages. These are all loaded into your working environment when you load ggtree. Also notice the lines that start with The following objects are masked from 'package:.... One example of this is the collapse() function from dplyr. When ggtree was loaded, it loaded it’s own function called collapse(). Now, if you wanted to use dplyr’s collapse function, you’ll have to call it explicitly using this kind of syntax: dplyr::collapse(). See this Q&A thread for more."
  },
  {
    "objectID": "labs/lab8_ggtree.html#tree-import",
    "href": "labs/lab8_ggtree.html#tree-import",
    "title": "Lab 8 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Tree Import",
    "text": "Tree Import\nFrom the ggtree landing page take a look at the Tree Data Import vignette. There are many different software packages for creating phylogenetic trees from different types of data, and there are many formats for storing the resulting phylogenetic trees they produce.\nMost tree viewer software (including R packages) focus on Newick and Nexus file formats, and other evolution analysis software might also contain supporting evidence within the file that are ready for annotating a phylogenetic tree. ggtree supports several file formats, including:\n\nNewick\nNexus\nPhylip\nJplace\nNew Hampshire eXtended format (NHX)\n\nand software output from:\n\nBEAST\nEPA\nHYPHY\nPAML\nPHYLDOG\npplacer\nr8s\nRAxML\nRevBayes\n\nThe ggtree package implement several parser functions, including:\n\nread.tree for reading Newick files.\nread.phylip for reading Phylip files.\nread.jplace for reading Jplace files.\nread.nhx for reading NHX files.\nread.beast for parsing output of BEAST\nread.codeml for parsing output of CODEML (rst and mlc files)\nread.codeml_mlc for parsing mlc file (output of CODEML)\nread.hyphy for parsing output of HYPHY\nread.jplace for parsing jplace file including output from EPA and pplacer\nread.nhx for parsing NHX file including output from PHYLODOG and RevBayes\nread.paml_rst for parsing rst file (output of BASEML and CODEML)\nread.r8s for parsing output of r8s\nread.raxml for parsing output of RAxML"
  },
  {
    "objectID": "labs/lab8_ggtree.html#basic-trees",
    "href": "labs/lab8_ggtree.html#basic-trees",
    "title": "Lab 8 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Basic trees",
    "text": "Basic trees\nLet’s first import our tree data. We’re going to work with a made-up phylogeny with 13 taxa tree_newick.nwk. Let’s load the libraries you’ll need if you haven’t already, and then import the tree using read.tree(). Displaying the object itself really isn’t useful. The output just tells you a little bit about the tree itself.\n\n\nR code\nlibrary(tidyverse)\nlibrary(ggtree)\ntree &lt;- read.tree(\"data/lab13/tree_newick.nwk\")\ntree\n\n\nJust like with ggplot2 we created a basic canvas with ggplot(...) and added layers with +geom_???(), we can do the same here. The ggtree package gives us a geom_tree() function. Because ggtree is built on top of ggplot2, you get ggplot2’s default gray theme with white lines. You can override this with a theme from the ggtree package.\nBecause you’ll almost always want to add a tree geom and remove the default background and axes, the ggtree() function is essentially a shortcut for ggplot(...) + geom_tree() + theme_tree().\n\n\nR code\n# build a ggplot with a geom_tree\nggplot(tree) + geom_tree() + theme_tree()\n# This is convenient shorthand\nggtree(tree)\n\n\n\n\n\n\n\n\n\nThere’s also the treescale geom, which adds a scale bar, or alternatively, you can change the default ggtree() theme to theme_tree2(), which adds a scale on the x-axis. The horizontal dimension in this plot shows the amount of genetic change, and the branches and represent evolutionary lineages changing over time. The longer the branch in the horizonal dimension, the larger the amount of change, and the scale tells you this. The units of branch length are usually nucleotide substitutions per site – that is, the number of changes or substitutions divided by the length of the sequence (alternatively, it could represent the percent change, i.e., the number of changes per 100 bases).\n\n\nR code\n# add a scale\nggtree(tree) + geom_treescale()\n# or add the entire scale to the x axis with theme_tree2()\nggtree(tree) + theme_tree2()\n\n\n\n\n\n\n\n\n\nThe default is to plot a phylogram, where the x-axis shows the genetic change / evolutionary distance. If you want to disable scaling and produce a cladogram instead, set the branch.length=\"none\" option inside the ggtree() call. See ?ggtree for more.\n\n\nR code\nggtree(tree, branch.length=\"none\")\n\n\n\n\n\n\n\n\n\nThe ... option in the help for ?ggtree represents additional options that are further passed to ggplot(). You can use this to change aesthetics of the plot. Let’s draw a cladogram (no branch scaling) using thick blue dotted lines (note that I’m not mapping these aesthetics to features of the data with aes() – we’ll get to that later).\n\n\nR code\nggtree(tree, branch.length=\"none\", color=\"blue\", size=2, linetype=3)\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1\nLook at the help again for ?ggtree, specifically at the layout= option. By default, it produces a rectangular layout.\n\nCreate a slanted phylogenetic tree.\nCreate a circular phylogenetic tree.\nCreate a circular unscaled cladogram with thick red lines.\n\n\n\n\nOther tree geoms\nLet’s add additional layers. We can create a plot object, e.g., p, to store the basic layout of a ggplot, and add more layers to it as we desire. Let’s add node and tip points. Let’s finally label the tips.\n\n\nR code\n# create the basic plot\np &lt;- ggtree(tree)\n# add node points\np + geom_nodepoint()\n# add tip points\np + geom_tippoint()\n# Label the tips\np + geom_tiplab()\n\n\n\n\n\n\nExercise 2\nSimilar to how we change the aesthetics for the tree inside the ggtree() call, we can also change the aesthetics of the points themselves by passing graphical parameters inside the geom_nodepoint() or geom_tippoint() calls. Create a phylogeny with the following aesthetic characteristics:\n\ntips labeled in purple\npurple-colored diamond-shape tip points (hint: Google search “R point characters”)\nlarge semitransparent yellow node points (hint: alpha=)\nAdd a title with + ggtitle(...)"
  },
  {
    "objectID": "labs/lab8_ggtree.html#tree-annotation",
    "href": "labs/lab8_ggtree.html#tree-annotation",
    "title": "Lab 8 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Tree annotation",
    "text": "Tree annotation\nThe geom_tiplab() function adds some very rudimentary annotation. Let’s take annotation a bit further.\n\nInternal node number\nBefore we can go further we need to understand how ggtree is handling the tree structure internally. Some of the functions in ggtree for annotating clades need a parameter specifying the internal node number. To get the internal node number, user can use geom_text to display it, where the label is an aesthetic mapping to the “node variable” stored inside the tree object (think of this like the continent variable inside the gapminder object). We also supply the hjust option so that the labels aren’t sitting right on top of the nodes.\n\n\nR code\nggtree(tree) + geom_text(aes(label=node), hjust=-.3)\n\n\n\n\n\n\n\n\n\nAnother way to get the internal node number is using MRCA() function by providing a vector of taxa names (created using c(\"taxon1\", \"taxon2\")).. The function will return node number of input taxa’s most recent commond ancestor (MRCA). First, re-create the plot so you can choose which taxa you want to grab the MRCA from.\n\n\nR code\nggtree(tree) + geom_tiplab()\n\n\n\n\n\n\n\n\n\nLet’s grab the most recent common ancestor for taxa C+E, and taxa G+H. We can use MRCA() to get the internal node numbers. Go back to the node-labeled plot from before to confirm this.\n\n\nR code\nMRCA(tree, c(\"C\", \"E\"))\n\n\n[1] 17\n\n\nR code\nMRCA(tree, c(\"G\", \"H\"))\n\n\n[1] 21\n\n\nR code\n# MRCA(tree, tip=c(\"C\", \"E\"))\n# MRCA(tree, tip=c(\"G\", \"H\"))\n\n\n\n\nLabeling clades\nWe can use geom_cladelabel() to add another geom layer to annotate a selected clade with a bar indicating the clade with a corresponding label. You select the clades using the internal node number for the node that connects all the taxa in that clade.\nLet’s annotate the clade with the most recent common ancestor between taxa C and E (internal node 17). Let’s make the annotation red. See ?geom_cladelabel help for more.\n\n\nR code\nggtree(tree) + \n  geom_cladelabel(node=17, label=\"Some random clade\", color=\"red\")\n\n\nLet’s add back in the tip labels. Notice how now the clade label is too close to the tip labels. Let’s add an offset to adjust the position. You might have to fiddle with this number to get it looking right.\n\n\nR code\nggtree(tree) + \n  geom_tiplab() + \n  geom_cladelabel(node=17, label=\"Some random clade\", \n                  color=\"red2\", offset=.8)\n\n\nNow let’s add another label for the clade connecting taxa G and H (internal node 21).\n\n\nR code\nggtree(tree) + \n  geom_tiplab() + \n  geom_cladelabel(node=17, label=\"Some random clade\", \n                  color=\"red2\", offset=.8) + \n  geom_cladelabel(node=21, label=\"A different clade\", \n                  color=\"blue\", offset=.8)\n\n\nUh oh. Now we have two problems. First, the labels would look better if they were aligned. That’s simple. Pass align=TRUE to geom_cladelabel() (see ?geom_cladelabel help for more). But now, the labels are falling off the edge of the plot. That’s because geom_cladelabel() is just adding it this layer onto the end of the existing canvas that was originally layed out in the ggtree call. This default layout tried to optimize by plotting the entire tree over the entire region of the plot. Here’s how we’ll fix this.\n\nFirst create the generic layout of the plot with ggtree(tree).\nAdd some tip labels.\nAdd each clade label.\nRemember theme_tree2()? We used it way back to add a scale to the x-axis showing the genetic distance. This is the unit of the x-axis. We need to set the limits on the x-axis. Google around for something like “ggplot2 x axis limits” and you’ll wind up on this StackOverflow page that tells you exactly how to solve it – just add on a + xlim(..., ...) layer. Here let’s extend out the axis a bit further to the right.\nFinally, if we want, we can either comment out the theme_tree2() segment of the code, or we could just add another theme layer on top of the plot altogether, which will override the theme that was set before. theme_tree() doesn’t have the scale.\n\n\n\nR code\nggtree(tree) + \n  geom_tiplab() + \n  geom_cladelabel(node=17, label=\"Some random clade\", \n                  color=\"red2\", offset=.8, align=TRUE) + \n  geom_cladelabel(node=21, label=\"A different clade\", \n                  color=\"blue\", offset=.8, align=TRUE) + \n  theme_tree2() + \n  xlim(0, 70) + \n  theme_tree()\n\n\n\n\n\n\n\n\n\nAlternatively, we could highlight the entire clade with geom_hilight(). See the help for options to tweak.\n\n\nR code\nggtree(tree) + \n  geom_tiplab() + \n  geom_hilight(node=17, fill=\"gold\") + \n  geom_hilight(node=21, fill=\"purple\")\n\n\n\n\n\n\n\n\n\n\n\nConnecting taxa\nSome evolutionary events (e.g. reassortment, horizontal gene transfer) can be visualized with some simple annotations on a tree. The geom_taxalink() layer draws straight or curved lines between any of two nodes in the tree, allow it to show evolutionary events by connecting taxa.\n\n\nR code\nggtree(tree) + \n  geom_tiplab() + \n  geom_taxalink(\"E\", \"H\", color=\"blue3\") +\n  geom_taxalink(\"C\", \"G\", color=\"orange2\", curvature=-.9)"
  },
  {
    "objectID": "labs/lab8_ggtree.html#exercise-3",
    "href": "labs/lab8_ggtree.html#exercise-3",
    "title": "Lab 8 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Exercise 3",
    "text": "Exercise 3\nProduce the figure below.\n\nFirst, find what the MRCA is for taxa B+C, and taxa L+J. You can do this in one of two ways:\n\nEasiest: use MRCA(tree, tip=c(\"taxon1\", \"taxon2\")) for B/C and L/J separately.\nAlternatively: use ggtree(tree) + geom_text(aes(label=node), hjust=-.3) to see what the node labels are on the plot. You might also add tip labels here too.\n\nDraw the tree with ggtree(tree).\nAdd tip labels.\nHighlight these clades with separate colors.\nAdd a clade label to the larger superclade (node=17) that we saw before that includes A, B, C, D, and E. You’ll probably need an offset to get this looking right.\nLink taxa C to E, and G to J with a dashed gray line (hint: get the geom working first, then try changing the aesthetics. You’ll need linetype=2 somewhere in the geom_taxalink()).\nAdd a scale bar to the bottom by changing the theme.\nAdd a title.\nOptionally, go back to the original ggtree(tree, ...) call and change the layout to \"circular\"."
  },
  {
    "objectID": "labs/lab8_ggtree.html#advanced-tree-annotation",
    "href": "labs/lab8_ggtree.html#advanced-tree-annotation",
    "title": "Lab 8 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Advanced tree annotation",
    "text": "Advanced tree annotation\nLet’s use a previously published dataset from this paper:\nLiang et al. “Expansion of genotypic diversity and establishment of 2009 H1N1 pandemic-origin internal genes in pigs in China.” Journal of virology (2014): 88(18):10864-74.\nThis data was reanalyzed in the ggtree paper.\nThe subset of the data used here contains 76 H3 hemagglutinin gene sequences of a lineage containing both swine and human influenza A viruses. The sequence data set was re-analyzed by using BEAST (available at http://beast.bio.ed.ac.uk/). BEAST (Bayesian Evolutionary Analysis Sampling Trees) can give you rooted, time-measured phylogenies inferred using molecular clock models.\nFor this you’ll need the flu_tree_beast.tree output file from BEAST and the flu_aasequence.fasta FASTA file with the multiple sequence alignment. These are both available on the data downloads page. First let’s read in the tree with read.beast() (instead of the read.tree() we used before). Let’s add a scale bar with theme_tree2(). This gives you genetic distance. But, we have time measured here with molecular clock models. We’ve only estimated the relative time between branching events, so if we want to actually see dates on the x-axis, we need to supply the most recent sampling date to the ggtree() call. Do this by setting mrsd=\"YYYY-MM-DD\" inside ggtree().\nFinally, let’s add some tip labels. We’ll want to right-align them, and by default the dotted line is a little too thick. Let’s reduce the linesize a bit. Now, some of the labels might be falling off the margin. Set the xlim to limit the axis to show between 1990 and 2020. You could get MRCAs and node numbers and do all the annotations that we did before the same way here.\n\n\nR code\n# Read the data\ntree &lt;- read.beast(\"data/lab13/flu_tree_beast.tree\")\n# supply a most recent sampling date so you get the dates\n# and add a scale bar\nggtree(tree, mrsd=\"2013-01-01\") + \n  theme_tree2() \n# Finally, add tip labels and adjust axis\nggtree(tree, mrsd=\"2013-01-01\") + \n  theme_tree2() + \n  geom_tiplab(align=TRUE, linesize=.5) + \n  xlim(1990, 2020)\n\n\n\n\n\n\n\n\n\nFinally, let’s look at ?msaplot. This puts the multiple sequence alignment and the tree side-by-side. The function takes a tree object (produced with ggtree()) and the path to the FASTA multiple sequence alignment. You can do it with the entire MSA, or you could restrict to just a window. Want something interesting-looking, but maybe not all that useful? Try changing the coordinate system of the plot itself by passing + coord_polar(theta=\"y\") to the end of the command!\n\n\nR code\nmsaplot(p=ggtree(tree), fasta=\"data/lab13/flu_aasequence.fasta\", window=c(150, 175))"
  },
  {
    "objectID": "labs/lab8_ggtree.html#bonus",
    "href": "labs/lab8_ggtree.html#bonus",
    "title": "Lab 8 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Bonus!",
    "text": "Bonus!\nSee the ggtree vignettes for more details on how these work.\n\nMany trees\nggtree will let you plot many trees at once, and you can facet them the normal ggplot2 way. Let’s generate 3 replicates each of 4 random trees with 10, 25, 50, and 100 tips, plotting them all.\n\n\nR code\nset.seed(42)\ntrees &lt;- lapply(rep(c(10, 25, 50, 100), 3), rtree)\nclass(trees) &lt;- \"multiPhylo\"\nggtree(trees) + facet_wrap(~.id, scale=\"free\", ncol=4) + ggtitle(\"Many trees. Such phylogenetics. Wow.\")\n\n\n\n\n\n\n\n\n\n\n\nPlot tree with other data\nFor showing a phylogenetic tree alongside other panels with your own data, the facet_plot() function accepts a input data.frame and a geom function to draw the input data.\n\n\nR code\n# Generate a random tree with 30 tips\ntree &lt;- rtree(30)\n# Make the original plot\np &lt;- ggtree(tree)\n# generate some random values for each tip label in the data\nd1 &lt;- data.frame(id=tree$tip.label, val=rnorm(30, sd=3))\n# Make a second plot with the original, naming the new plot \"dot\", \n# using the data you just created, with a point geom.\np2 &lt;- facet_plot(p, panel=\"dot\", data=d1, geom=geom_point, aes(x=val), color='red3')\n# Make some more data with another random value.\nd2 &lt;- data.frame(id=tree$tip.label, value = abs(rnorm(30, mean=100, sd=50)))\n# Now add to that second plot, this time using the new d2 data above, \n# This time showing a bar segment, size 3, colored blue.\np3 &lt;- facet_plot(p2, panel='bar', data=d2, geom=geom_segment, \n           aes(x=0, xend=value, y=y, yend=y), size=3, color='blue4') \n# Show all three plots with a scale\np3 + theme_tree2()\n\n\n\n\n\n\n\n\n\n\n\nOverlay organism silouhettes\nphylopic.org hosts free silhouette images of animals, plants, and other life forms, all under Creative Commons or Public Domain. You can use ggtree to overlay a phylopic image on your plot at a node of your choosing.\n\n\nR code\n# get phylopic \n\nnewick &lt;- \"((Pongo_abelii,(Gorilla_gorilla_gorilla,(Pan_paniscus,Pan_troglodytes)Pan,Homo_sapiens)Homininae)Hominidae,Nomascus_leucogenys)Hominoidea;\"\n\ntree &lt;- read.tree(text=newick)\n\nd &lt;- ggimage::phylopic_uid(tree$tip.label)\nd$body_mass = c(52, 114, 47, 45, 58, 6)\n\np &lt;- ggtree(tree) %&lt;+% d + \n  geom_tiplab(aes(image=uid, colour=body_mass), geom=\"phylopic\", offset=2.5) +\n  geom_tiplab(aes(label=label), offset = .2) + xlim(NA, 7) +\n  scale_color_viridis_c()\np"
  },
  {
    "objectID": "labs/lab8_ggtree.html#tips",
    "href": "labs/lab8_ggtree.html#tips",
    "title": "Lab 8 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Tips",
    "text": "Tips\nIf you the tree is not fitting in the plot space you can modify the xlim\n\n\nR code\n# Add to your \nggtree(tree) +\n  xlim(0,2) # This worked from my rectangular trees\n  xlim(0.36) # This worked for my circular trees\n\n\nTo show the bootstrap values\n\n\nR code\nggtree(tree) +  \n  geom_nodelab()\n\n\nTo change text size and position\n\n\nR code\nggtree(tree) +\n  geom_tiplab(hjust=-.1, size = 3)\n\n\nTo change the size of the highlighted clade you can use extend\n\n\nR code\nggtree(tree) +\n  geom_hilight(node=34, fill=\"gold\", extend = 0.5) \n\n\nYou can order the layers when highlighting for certain effects"
  },
  {
    "objectID": "labs/lab8_ggtree.html#exercises",
    "href": "labs/lab8_ggtree.html#exercises",
    "title": "Lab 8 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Exercises",
    "text": "Exercises\nFor the lab turn in a report with the above examples. We will work with the NEON data next week."
  },
  {
    "objectID": "labs/lab9_ggtree.html",
    "href": "labs/lab9_ggtree.html",
    "title": "Lab 9 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "",
    "text": "This lab is a slighlty modified update of Stephen Turner’s ggtree tutorial. Many of the examples here were modified from the ggtree vignettes."
  },
  {
    "objectID": "labs/lab9_ggtree.html#acknowledgements",
    "href": "labs/lab9_ggtree.html#acknowledgements",
    "title": "Lab 9 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "",
    "text": "This lab is a slighlty modified update of Stephen Turner’s ggtree tutorial. Many of the examples here were modified from the ggtree vignettes."
  },
  {
    "objectID": "labs/lab9_ggtree.html#learning-objectives",
    "href": "labs/lab9_ggtree.html#learning-objectives",
    "title": "Lab 9 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nMultiple Sequence Alignment\nPhylogenetic Inference\nTree visualization using ggtree"
  },
  {
    "objectID": "labs/lab9_ggtree.html#sequence-alignment-background",
    "href": "labs/lab9_ggtree.html#sequence-alignment-background",
    "title": "Lab 9 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Sequence Alignment Background",
    "text": "Sequence Alignment Background\nSequence Alignment is one of the most fundamental operations of bioinformatics. Sequence alignments are used to decide if genes or proteins are related functionally, structurally and evolutionarily. It is also at the core of sequence search methods such as BLAST.\nThere are three principal methods of pair-wise sequence alignment\n\nDot matrix analysis.\nThe dynamic programming (or DP) algorithm.\nWord or k-tuple methods (such as those used in BLAST and FASTA - which we will discuss later)\n\n\nDot matrix analysis\nThis method involves constructing a matrix with one of the sequences to be compared running horizontally across the bottom, and the other running vertically along the left-hand side. Each entry of the matrix is a measure of similarity of those two residues on the horizontal and vertical sequence. The simplest scoring system, distinguishes only between identical (dots) and non-identical (blank) residues.\nThe dot plot displays graphically any possible sequence alignments as diagonals on the matrix. Insertions, deletions, direct repeats and inverted repeats can be visually detected on a dot plot. Dot plot can also detect regions of RNA that are self-complementary and thus might form secondary structure.\n\nRef:\nThis dot plot represents an alignment of two different strains of Helicobacter pylori. Forward matches are shown in red, while reverse matches are shown in green. This alignment clearly shows a major inversion event centered around the origin of replication.UM\n\n\nThe dynamic programming algorithm\n“Dynamic programming was formalized by mathematician Richard Bellman, who was working at RNAD corporation on optimal decision processes. He wanted to concoct an impressive name that would shield his work from US Secretary of Defense Charles Wilson, a man know to be hostile to mathematics research. His work involved times series and planning - thus ‘dynamic’ and ‘programming’ (note, nothing particularly to do with computer programming). Bellman especially liked ‘dynamic’ because”it’s impossible to use the work dynamic in the pejorative sense”; he figured dynamic programming was “something not even a Congressman could object to” (Bellman 1984).” (Eddy 2004)\nThe dynamic programming algorithm is at the heart of many bioinformatics programs including BLAST (sequence searching), FASTA (sequence searching), CLUSTALW (multiple sequence alignmnet), HMMER (profile searching), GENSCAN (gene finding), MFOLD (RNA folding), and PHYLIP (phylogenetic analysis). Dynamic programing was first used for global alignment of protein sequences by Needleman and Wunnsch (1970) and to find local alignments by Smith and Waterson (1981).\n  MLIGKVIGSVVSTRKN-LNVGKFMIVEPLK\n  |     |  |  ||   || ||||     |        Global Alignment\n  MVAARLI-VWANTRKTSLNLGKFMLAEIIK\n  \n              TRKN-LNVGKFM\n              ||   || ||||               Local Alignment\n              TRKTDLNLGKFML\n\n\n\nSubsitution penalties differ for particular nucleotides or aminoacids\nAmino acids are put together into proteins based on their three nucleotide codons, and most mutational events usually only change one nucleotide at time. For example, starting with the Alanine codon GCU and looking at all possible changes on a http://en.wikipedia.org/wiki/Genetic_code#RNA_codon_table\nGCU -&gt; UCU = Alanine (A) -&gt; Serine (S)\nGCU -&gt; ACU = Alanine (A) -&gt; Threonine (T)\nGCU -&gt; CCU = Alanine (A) -&gt; Proline (P)\nGCU -&gt; GUU = Alanine (A) -&gt; Valine (V)\nGCU -&gt; GAU = Alanine (A) -&gt; Aspartic Acid (D)\nGCU -&gt; GGU = Alanine (A) -&gt; Glycine (G)\nGCU -&gt; GCA = Alanine (A) -&gt; Alanine (A)\nGCU -&gt; GCC = Alanine (A) -&gt; Alanine (A)\nGCU -&gt; GCG = Alanine (A) -&gt; Alanine (A)\nThus, from the Alanine codon GCU with a single substitution we can only get to 6 of the 19 other amino acids and 33% of the changes are going to result in keeping Alanine at the site in the protein, the other six amino acids would result 11% of the time. Even if we look at all codons for Alanine, there are still 12 amino acids that can not be made by single substitution. Thus, the frequency of changing Alanine to another amino acid by a single nucleotide change would be\n       A   R   N   D   C   Q   E   G   H   I   L   K   M   F   P   S   T   W   Y   V\n    A .33  0   0  .06  0   0  .06 .11  0   0   0   0   0   0  .11 .11 .11  0   0  .11\n\n\n\nOrigin of protein substitution matrices and bioinformatics\nProtein substitution matrices were first developed by Margaret Dayhoff, one of the founders of the field of bioinformatics. She was particularly interested in the possibility of deducing the evolutionary relationships of organisms from protein sequences. Toward these ends she collected all the known protein sequences and, as a service to the scientific community, made them available to others in 1965 in a small book, the first Atlas of Protein Sequence and Structure.\nRef: http://en.wikipedia.org/wiki/Margaret_Oakley_Dayhoff\nFrom these sequences, she and her coworkers developed a model of protein evolution which resulted in the development of a set of widely used substitution matrices. These are frequently called Dayhoff or PAM (Percent Accepted Mutation) matrices and similar matrices are used in many areas of bioinformatics including: sequence similarity searches (e.g. BLAST), multiple sequence alignment tools (e.g ClustalW), phylogenetics and identifying functional regions of proteins.\n\n\nNatural selection governs which amino acid changes are observed\nJust like mutation mucks with the frequency matrix so does Natural Selection. Changes that occur between amino acids that have different biochemical properties are likely to affect the function of the protein. Therefore, a substitution is more likely to occur between amino acids with similar biochemical properties. In the above example with the Alanine codon a substitution that yields an amino acid change would result in mostly changes to other neutral amino acids, whereas the frequency of change to Aspartic Acid would probably be much lower. Amazingly, the genetic code has evolved to minimize changes between amino acids with different biochemical properties.\nAmino Acid          Side chain polarity Side chain acidity or basicity\n Alanine              nonpolar            neutral\n Arginine             polar                  strongly basic\n Asparagine           polar                  neutral\n Aspartic acid        polar                  acidic\n Cysteine             polar                  neutral\n Glutamic acid        polar                  acidic\n Glutamine            polar                  neutral\n Glycine              nonpolar               neutral\n Histidine            polar                  weakly basic\n Isoleucine           nonpolar               neutral\n Leucine              nonpolar               neutral\n Lysine               polar                  basic\n Methionine           nonpolar               neutral\n Phenylalanine        nonpolar               neutral\n Proline              nonpolar               neutral\n Serine               polar                  neutral\n Threonine            polar                  neutral\n Tryptophan           nonpolar               neutral\n Tyrosine             polar                  neutral\n Valine               nonpolar               neutral\n\n\n\nAccounting for multiple substitutions\nAs time goes on and sequence divergence gets larger it gets harder to account for multiple substitutions at the same amino acid position. PAM protein matrices are based on global alignments of closely related proteins. However, sequence changes over long evolutionary time scales are not well approximated by compounding small changes that occur over short time scales. For comparing more distantly related sequence other types of matrices are used. One of the most common is the The BLOSUM (BLOck SUbstitution Matrix) series of matrices created by Steve Henikoff and colleagues. Both the PAM and BLOSUM are not expressed as transformation frequencies but the probabilities of transformation are expressed by log-odds scores as shown below for a BLOSUM62 matrix.\n\nRef: http://en.wikipedia.org/wiki/Substitution_matrix"
  },
  {
    "objectID": "labs/lab9_ggtree.html#multiple-sequence-alignment",
    "href": "labs/lab9_ggtree.html#multiple-sequence-alignment",
    "title": "Lab 9 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Multiple Sequence Alignment",
    "text": "Multiple Sequence Alignment\nWhy is multiple seqence alignment difficult?\nIf sequences are all the same length, alignment is trivial:\n    KNMITGAAQMDGAILVVAATDGPMPQTREHVLLARQVEVP\n    KNMITGAAQMDGAILVVSATDGAMPQTKEHILLARQVGVP\n    KNMITGAAQMDGAILVVSATDGAMPQTKEHILLARQVGVP\n    KNMITGAAQMDGAILVVSAADGPMPQTREHILLARQVGVP\nThis sequence alignment is unambiguous because there is no length variation among the sequences. No indels are needed to make the alignment, and the ungapped sequences can simply be arranged together. However, if the sequenes are of various lengths, problem is more complex, potentially very complex:\n    RGSALKAVEAPNDPNHEA......YKPIQELLDAMDN.....YIPDPQRDVDKPFL\n    RGSALKALEGDAAYIEKVR..........ELMQAVDD.....NIPTPEREIDKPFL\n    RGSALKALE.....IEKVR..........ELMQAGDAAYVDDNIPTPEREIDKPFL\n    RGSALLALEEMHKNPKTKRGENEWVDKIWELLDAIDE.....YIPTPVRDVDKPFL\n    RGSALLALEQMHRNPKTRRGENEWVDKIWELLDAIDE.....YIPTPVRDVDKPFL\n    KGSALQALEALQANPKTARGEDKWVDRIWELLDAVDS.....YIPTPERATDKTFL\n    RGTARNALESPSKDIN....APEY.KCILELMNAVDE.....YIPTPQRAVDQPFL\n    KGSALQALE....NAE....DEEKTKCIWELLQAMDD.....YIPAPERDIDKPFL\n    KGSAFGAMS....NPE....DPESTKCVKELLESMDN.....YFDLPERDIDKPFL\n    RGSAFAAMS....KPD....DPAATKCLDELLDTMDK.....YFVIPERALDKPFL\nIn many cases the best position to place an indel is ambiguous. Ideally, one would know the phylogeny for the sequences; this would help infer the sequence of indels. Unfortunately one normally needs a multiple sequence alignment to make inferences about how the sequences are related. Most alignment algorithms make a quick approximation of phylogeny, and then base alignments on these. Sound circular? You are right and this is a challenging problem that is at the forefront of research in phylogenetics…the joint estimation of the alignment and phylogeny. For this class we will stick to the traditional method of first aligning sequences followed by phylogenetic analysis.\nProgressive alignment methods are efficient enough to implement on a large scale for many (100s to 1000s) sequences. Progressive alignment services are commonly available on publicly accessible web servers so users need not locally install the applications of interest. The most popular progressive alignment method has been the Clustal family Different portals or implementations can vary in user interface and make different parameters accessible to the user. Clustal Omega is used extensively for phylogenetic tree construction.\nThe basic steps in Clustal are:\n\nCalculate all possible pairwise alignments, record the score for each pair\nCalculate a guide tree based on the pairwise distances (algorithm: Neighbor Joining)\nFind the two most closely related sequences\nAlign these sequences (algorithm: Smith-Waterman).\n\nCalculate a consensus of this alignment\nReplace the two sequences with the consensus\nFind the two next-most closely related sequences (one of these could be a previously determined consensus sequence).\nIterate until all sequences have been aligned\n\nExpand the consensus sequences with the (gapped) original sequences\nReport the multiple sequence alignment\n\n\nSoftware for sequence alignment\nThere are many tools available for sequence alignment. The common tools are hosting at the European Bioinformatics Institute.\n\nPairwise sequence alignment\nMultiple sequence alignment\n\nThe most commonly used are Clustal, Muscle and MAFFT. MAFFT is commonly implemented for working with large data sets and where speed is important (e.g. web servers)"
  },
  {
    "objectID": "labs/lab9_ggtree.html#on-the-computer",
    "href": "labs/lab9_ggtree.html#on-the-computer",
    "title": "Lab 9 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "On the Computer",
    "text": "On the Computer\n\nPhylogenetic analysis\nCarl Woese sequenced isolated and sequenced ribosomal RNA to discover a new domain of life, the Archaea. His discovery was dependent on using phylogenetic methods to determine the relationship of his microbial sequence to other sequences in the database. Since his discovery, progress sequencing specfic genes from genomic DNA has greatly simplified the process of understanding microbial phylogenies. In this project you will be given a DNA sequence, then construct a multiple sequence alignment and do a phylogenetic analysis to show the relationships among the taxa represented by the DNA sequences.\nFrom a practical perspective, phylogenetic analysis is broken up into methods related to tree building and tree visualization. I have assembled a core group DNA sequences of small subunit ribosomal gene from our national DNA database Genbank to provide an example Tree_of_Life_Core_Sequences.txt. This file us in our Unity course directory /work/pi_bio678_umass_edu/data/lab9/ and on our course GitHub site directory contains data for the following species.\n\nThermoplasma volcanium\nHalalkalicoccus jeotgali\nCandidatus Korarchaeum cryptofilum\nNanoarchaeum equitans\nChlorobium chlorochromatii\nBurkholderia cenocepacia\nRhizobium leguminosarum\nEscherichia coli\nArchaeoglobus fulgidus\nMethanocaldococcus jannaschii\nPyrococcus abyssi\nOryza sativa nuclear\nOryza sativa mitochondrion\nOryza sativa chloroplast\nSaccharomyces cerevisiae nuclear\nHomo sapies nuclear\nDrosophila yakuba nuclear\nAmphidinium carterae nuclear\nThermotoga lettingae\nProchlorococcus marinus\nTrypanosoma cruzi\n\n\n\nNGPhylogeny.fr - A quick and easy working flow including multiple sequence alignment, phylogenetic analysis and tree visualization\nThis web server is great place to learn the basic workflow for phylogenetic analysis and I use it on occasion when I just need a quick see of the relationships among a small set of sequences. Doing a phylogenetic analysis involves (1) assembling a group of sequences to evaluate, (2) aligning the sequences so that a distance can be calculate between the sequences, (3) determining the relationships among the sequences, and (4) visualizing the relationships. Step 1 was accomplished above by getting representative archaeal, bacterial and eukaryotic sequences from GenBank. Steps 2-4 will be done on web server that has been set up to run these steps at the same time. Go to the site Robust Phylogenetic Analysis For The Non-Specialist. Scroll down and under Phylogenetic Analysis select “One Click”. Upload or paste in your DNA sequences from the file in Moodle. Then click submit. This will start the analysis process. The numbers of the tree represent statistical support for the relationship. The more robust the relationship the closer the value will be to 1. The scale at the bottom represents sequence distance (e.g. 0.2 is 20%).\nThe newick tree file was added to the /work/pi_bio678_umass_edu/data/lab9/ directory"
  },
  {
    "objectID": "labs/lab9_ggtree.html#phylogenetic-tree-visualization",
    "href": "labs/lab9_ggtree.html#phylogenetic-tree-visualization",
    "title": "Lab 9 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Phylogenetic tree visualization",
    "text": "Phylogenetic tree visualization\nThe tree visualizations on NGPhylogeny.fr are ok for some purposes, but often for publications more tree editing and visualization features are needed. There are a number of packages available to bring trees to like including Figtree, Dendroscope and iTOL. ETE Toolkit is a framework for analyzing and visualizing trees in Python. For a more complete list see Wikipedia.\nHistorically most phylogenetic analysis and tree visualization has been done with independent software tools. There has been a growing move towards using R for phylogenetic analyses. The package ape: Analysis of Phylogenetics and Evolution can access popular multiple sequence alignment and phylogenetic analysis methods. ggtree extends the ‘ggplot2’ for visualization and annotation of phylogenetic trees with their annotation data. The author, Guangchuang Yu, has recently published an R book Data Integration, Manipulation and Visualization of Phylogenetic Trees."
  },
  {
    "objectID": "labs/lab9_ggtree.html#the-ggtree-package",
    "href": "labs/lab9_ggtree.html#the-ggtree-package",
    "title": "Lab 9 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "The ggtree Package",
    "text": "The ggtree Package\nggtree is an R package that extends ggplot2 for visualizating and annotating phylogenetic trees. It is available from Bioconductor. Bioconductor is a project to provide tools for analyzing and annotating various kinds of genomic data. You can search and browse Bioconductor packages here.\n\nggtree Bioconductor page: bioconductor.org/packages/ggtree.\nggtree homepage: guangchuangyu.github.io/ggtree (contains more information about the package, more documentation, a gallery of beautiful images, and links to related resources).\nggtree publication: Yu, Guangchuang Data Integration, Manipulation and Visualization of Phylogenetic Trees\n\nBioconductor packages usually have great documentation in the form of vignettes. Take a look at the landing page for ggtree – about halfway down the page under the “Documentation” heading there are multiple walkthrough tutorials directed to different applications and functionalities of ggtree, chock full of runnable examples and explanations.\nJust like R packages from CRAN, you only need to install Bioconductor packages once, then load them every time you start a new R session. Load the tidyverse package first.\n\n\nR code\nlibrary(tidyverse)\nlibrary(ggtree)\n# added treeio and ggimage library\nlibrary(treeio)\nlibrary(rphylopic)\nlibrary(ggimage)\n\n\nA note on masked functions: Take a second and look through some of the output that you see when you load ggtree after tidyverse. When you first installed ggtree it may have taken a while, because ggtree depends on a number of other R packages. Each of these, in turn, may depend on other packages. These are all loaded into your working environment when you load ggtree. Also notice the lines that start with The following objects are masked from 'package:.... One example of this is the collapse() function from dplyr. When ggtree was loaded, it loaded it’s own function called collapse(). Now, if you wanted to use dplyr’s collapse function, you’ll have to call it explicitly using this kind of syntax: dplyr::collapse(). See this Q&A thread for more."
  },
  {
    "objectID": "labs/lab9_ggtree.html#tree-import",
    "href": "labs/lab9_ggtree.html#tree-import",
    "title": "Lab 9 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Tree Import",
    "text": "Tree Import\nFrom the ggtree landing page take a look at the Tree Data Import vignette. There are many different software packages for creating phylogenetic trees from different types of data, and there are many formats for storing the resulting phylogenetic trees they produce.\nMost tree viewer software (including R packages) focus on Newick and Nexus file formats, and other evolution analysis software might also contain supporting evidence within the file that are ready for annotating a phylogenetic tree. ggtree supports several file formats, including:\n\nNewick\nNexus\nPhylip\nJplace\nNew Hampshire eXtended format (NHX)\n\nand software output from:\n\nBEAST\nEPA\nHYPHY\nPAML\nPHYLDOG\npplacer\nr8s\nRAxML\nRevBayes\n\nThe ggtree package implement several parser functions, including:\n\nread.tree for reading Newick files.\nread.phylip for reading Phylip files.\nread.jplace for reading Jplace files.\nread.nhx for reading NHX files.\nread.beast for parsing output of BEAST\nread.codeml for parsing output of CODEML (rst and mlc files)\nread.codeml_mlc for parsing mlc file (output of CODEML)\nread.hyphy for parsing output of HYPHY\nread.jplace for parsing jplace file including output from EPA and pplacer\nread.nhx for parsing NHX file including output from PHYLODOG and RevBayes\nread.paml_rst for parsing rst file (output of BASEML and CODEML)\nread.r8s for parsing output of r8s\nread.raxml for parsing output of RAxML"
  },
  {
    "objectID": "labs/lab9_ggtree.html#basic-trees",
    "href": "labs/lab9_ggtree.html#basic-trees",
    "title": "Lab 9 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Basic trees",
    "text": "Basic trees\nLet’s first import our tree data. We’re going to work with a made-up phylogeny with 13 taxa tree_newick.nwk. Let’s load the libraries you’ll need if you haven’t already, and then import the tree using read.tree(). Displaying the object itself really isn’t useful. The output just tells you a little bit about the tree itself.\n\n\nR code\ntree &lt;- read.tree(\"data/lab13/tree_newick.nwk\")\ntree\n\n\nJust like with ggplot2 we created a basic canvas with ggplot(...) and added layers with +geom_???(), we can do the same here. The ggtree package gives us a geom_tree() function. Because ggtree is built on top of ggplot2, you get ggplot2’s default gray theme with white lines. You can override this with a theme from the ggtree package.\nBecause you’ll almost always want to add a tree geom and remove the default background and axes, the ggtree() function is essentially a shortcut for ggplot(...) + geom_tree() + theme_tree().\n\n\nR code\n# build a ggplot with a geom_tree\nggplot(tree) + geom_tree() + theme_tree()\n# This is convenient shorthand\nggtree(tree)\n\n\n\n\n\n\n\n\n\nThere’s also the treescale geom, which adds a scale bar, or alternatively, you can change the default ggtree() theme to theme_tree2(), which adds a scale on the x-axis. The horizontal dimension in this plot shows the amount of genetic change, and the branches and represent evolutionary lineages changing over time. The longer the branch in the horizonal dimension, the larger the amount of change, and the scale tells you this. The units of branch length are usually nucleotide substitutions per site – that is, the number of changes or substitutions divided by the length of the sequence (alternatively, it could represent the percent change, i.e., the number of changes per 100 bases).\n\n\nR code\n# add a scale\nggtree(tree) + geom_treescale()\n# or add the entire scale to the x axis with theme_tree2()\nggtree(tree) + theme_tree2()\n\n\n\n\n\n\n\n\n\nThe default is to plot a phylogram, where the x-axis shows the genetic change / evolutionary distance. If you want to disable scaling and produce a cladogram instead, set the branch.length=\"none\" option inside the ggtree() call. See ?ggtree for more.\n\n\nR code\nggtree(tree, branch.length=\"none\")\n\n\n\n\n\n\n\n\n\nThe ... option in the help for ?ggtree represents additional options that are further passed to ggplot(). You can use this to change aesthetics of the plot. Let’s draw a cladogram (no branch scaling) using thick blue dotted lines (note that I’m not mapping these aesthetics to features of the data with aes() – we’ll get to that later).\n\n\nR code\nggtree(tree, branch.length=\"none\", color=\"blue\", size=2, linetype=3)\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1\nLook at the help again for ?ggtree, specifically at the layout= option. By default, it produces a rectangular layout.\n\nCreate a slanted phylogenetic tree.\nCreate a circular phylogenetic tree.\nCreate a circular unscaled cladogram with thick red lines.\n\n\n\n\nOther tree geoms\nLet’s add additional layers. We can create a plot object, e.g., p, to store the basic layout of a ggplot, and add more layers to it as we desire. Let’s add node and tip points. Let’s finally label the tips.\n\n\nR code\n# create the basic plot\np &lt;- ggtree(tree)\n# add node points\np + geom_nodepoint()\n# add tip points\np + geom_tippoint()\n# Label the tips\np + geom_tiplab()\n\n\n\n\n\n\nExercise 2\nSimilar to how we change the aesthetics for the tree inside the ggtree() call, we can also change the aesthetics of the points themselves by passing graphical parameters inside the geom_nodepoint() or geom_tippoint() calls. Create a phylogeny with the following aesthetic characteristics:\n\ntips labeled in purple\npurple-colored diamond-shape tip points (hint: Google search “R point characters”)\nlarge semitransparent yellow node points (hint: alpha=)\nAdd a title with + ggtitle(...)"
  },
  {
    "objectID": "labs/lab9_ggtree.html#tree-annotation",
    "href": "labs/lab9_ggtree.html#tree-annotation",
    "title": "Lab 9 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Tree annotation",
    "text": "Tree annotation\nThe geom_tiplab() function adds some very rudimentary annotation. Let’s take annotation a bit further.\n\nInternal node number\nBefore we can go further we need to understand how ggtree is handling the tree structure internally. Some of the functions in ggtree for annotating clades need a parameter specifying the internal node number. To get the internal node number, user can use geom_text to display it, where the label is an aesthetic mapping to the “node variable” stored inside the tree object (think of this like the continent variable inside the gapminder object). We also supply the hjust option so that the labels aren’t sitting right on top of the nodes.\n\n\nR code\nggtree(tree) + geom_text(aes(label=node), hjust=-.3)\n\n\n\n\n\n\n\n\n\nAnother way to get the internal node number is using MRCA() function by providing a vector of taxa names (created using c(\"taxon1\", \"taxon2\")).. The function will return node number of input taxa’s most recent commond ancestor (MRCA). First, re-create the plot so you can choose which taxa you want to grab the MRCA from.\n\n\nR code\nggtree(tree) + geom_tiplab()\n\n\n\n\n\n\n\n\n\nLet’s grab the most recent common ancestor for taxa C+E, and taxa G+H. We can use MRCA() to get the internal node numbers. Go back to the node-labeled plot from before to confirm this.\n\n\nR code\nMRCA(tree, c(\"C\", \"E\"))\n\n\n[1] 17\n\n\nR code\nMRCA(tree, c(\"G\", \"H\"))\n\n\n[1] 21\n\n\nR code\n# MRCA(tree, tip=c(\"C\", \"E\"))\n# MRCA(tree, tip=c(\"G\", \"H\"))\n\n\n\n\nLabeling clades\nWe can use geom_cladelabel() to add another geom layer to annotate a selected clade with a bar indicating the clade with a corresponding label. You select the clades using the internal node number for the node that connects all the taxa in that clade.\nLet’s annotate the clade with the most recent common ancestor between taxa C and E (internal node 17). Let’s make the annotation red. See ?geom_cladelabel help for more.\n\n\nR code\nggtree(tree) + \n  geom_cladelabel(node=17, label=\"Some random clade\", color=\"red\")\n\n\nLet’s add back in the tip labels. Notice how now the clade label is too close to the tip labels. Let’s add an offset to adjust the position. You might have to fiddle with this number to get it looking right.\n\n\nR code\nggtree(tree) + \n  geom_tiplab() + \n  geom_cladelabel(node=17, label=\"Some random clade\", \n                  color=\"red2\", offset=.8)\n\n\nNow let’s add another label for the clade connecting taxa G and H (internal node 21).\n\n\nR code\nggtree(tree) + \n  geom_tiplab() + \n  geom_cladelabel(node=17, label=\"Some random clade\", \n                  color=\"red2\", offset=.8) + \n  geom_cladelabel(node=21, label=\"A different clade\", \n                  color=\"blue\", offset=.8)\n\n\nUh oh. Now we have two problems. First, the labels would look better if they were aligned. That’s simple. Pass align=TRUE to geom_cladelabel() (see ?geom_cladelabel help for more). But now, the labels are falling off the edge of the plot. That’s because geom_cladelabel() is just adding it this layer onto the end of the existing canvas that was originally layed out in the ggtree call. This default layout tried to optimize by plotting the entire tree over the entire region of the plot. Here’s how we’ll fix this.\n\nFirst create the generic layout of the plot with ggtree(tree).\nAdd some tip labels.\nAdd each clade label.\nRemember theme_tree2()? We used it way back to add a scale to the x-axis showing the genetic distance. This is the unit of the x-axis. We need to set the limits on the x-axis. Google around for something like “ggplot2 x axis limits” and you’ll wind up on this StackOverflow page that tells you exactly how to solve it – just add on a + xlim(..., ...) layer. Here let’s extend out the axis a bit further to the right.\nFinally, if we want, we can either comment out the theme_tree2() segment of the code, or we could just add another theme layer on top of the plot altogether, which will override the theme that was set before. theme_tree() doesn’t have the scale.\n\n\n\nR code\nggtree(tree) + \n  geom_tiplab() + \n  geom_cladelabel(node=17, label=\"Some random clade\", \n                  color=\"red2\", offset=.8, align=TRUE) + \n  geom_cladelabel(node=21, label=\"A different clade\", \n                  color=\"blue\", offset=.8, align=TRUE) + \n  theme_tree2() + \n  xlim(0, 70) + \n  theme_tree()\n\n\n\n\n\n\n\n\n\nAlternatively, we could highlight the entire clade with geom_hilight(). See the help for options to tweak.\n\n\nR code\nggtree(tree) + \n  geom_tiplab() + \n  geom_hilight(node=17, fill=\"gold\") + \n  geom_hilight(node=21, fill=\"purple\")\n\n\n\n\n\n\n\n\n\n\n\nConnecting taxa\nSome evolutionary events (e.g. reassortment, horizontal gene transfer) can be visualized with some simple annotations on a tree. The geom_taxalink() layer draws straight or curved lines between any of two nodes in the tree, allow it to show evolutionary events by connecting taxa.\n\n\nR code\nggtree(tree) + \n  geom_tiplab() + \n  geom_taxalink(\"E\", \"H\", color=\"blue3\") +\n  geom_taxalink(\"C\", \"G\", color=\"orange2\", curvature=-.9)"
  },
  {
    "objectID": "labs/lab9_ggtree.html#exercise-3",
    "href": "labs/lab9_ggtree.html#exercise-3",
    "title": "Lab 9 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Exercise 3",
    "text": "Exercise 3\nProduce the figure below.\n\nFirst, find what the MRCA is for taxa B+C, and taxa L+J. You can do this in one of two ways:\n\nEasiest: use MRCA(tree, tip=c(\"taxon1\", \"taxon2\")) for B/C and L/J separately.\nAlternatively: use ggtree(tree) + geom_text(aes(label=node), hjust=-.3) to see what the node labels are on the plot. You might also add tip labels here too.\n\nDraw the tree with ggtree(tree).\nAdd tip labels.\nHighlight these clades with separate colors.\nAdd a clade label to the larger superclade (node=17) that we saw before that includes A, B, C, D, and E. You’ll probably need an offset to get this looking right.\nLink taxa C to E, and G to J with a dashed gray line (hint: get the geom working first, then try changing the aesthetics. You’ll need linetype=2 somewhere in the geom_taxalink()).\nAdd a scale bar to the bottom by changing the theme.\nAdd a title.\nOptionally, go back to the original ggtree(tree, ...) call and change the layout to \"circular\"."
  },
  {
    "objectID": "labs/lab9_ggtree.html#advanced-tree-annotation",
    "href": "labs/lab9_ggtree.html#advanced-tree-annotation",
    "title": "Lab 9 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Advanced tree annotation",
    "text": "Advanced tree annotation\nLet’s use a previously published dataset from this paper:\nLiang et al. “Expansion of genotypic diversity and establishment of 2009 H1N1 pandemic-origin internal genes in pigs in China.” Journal of virology (2014): 88(18):10864-74.\nThis data was reanalyzed in the ggtree paper.\nThe subset of the data used here contains 76 H3 hemagglutinin gene sequences of a lineage containing both swine and human influenza A viruses. The sequence data set was re-analyzed by using BEAST (available at http://beast.bio.ed.ac.uk/). BEAST (Bayesian Evolutionary Analysis Sampling Trees) can give you rooted, time-measured phylogenies inferred using molecular clock models.\nFor this you’ll need the flu_tree_beast.tree output file from BEAST and the flu_aasequence.fasta FASTA file with the multiple sequence alignment. These are both available on the data downloads page. First let’s read in the tree with read.beast() (instead of the read.tree() we used before). Let’s add a scale bar with theme_tree2(). This gives you genetic distance. But, we have time measured here with molecular clock models. We’ve only estimated the relative time between branching events, so if we want to actually see dates on the x-axis, we need to supply the most recent sampling date to the ggtree() call. Do this by setting mrsd=\"YYYY-MM-DD\" inside ggtree().\nFinally, let’s add some tip labels. We’ll want to right-align them, and by default the dotted line is a little too thick. Let’s reduce the linesize a bit. Now, some of the labels might be falling off the margin. Set the xlim to limit the axis to show between 1990 and 2020. You could get MRCAs and node numbers and do all the annotations that we did before the same way here.\n\n\nR code\n# Read the data\ntree &lt;- read.beast(\"data/lab13/flu_tree_beast.tree\")\n# supply a most recent sampling date so you get the dates\n# and add a scale bar\nggtree(tree, mrsd=\"2013-01-01\") + \n  theme_tree2() \n# Finally, add tip labels and adjust axis\nggtree(tree, mrsd=\"2013-01-01\") + \n  theme_tree2() + \n  geom_tiplab(align=TRUE, linesize=.5) + \n  xlim(1990, 2020)\n\n\n\n\n\n\n\n\n\nFinally, let’s look at ?msaplot. This puts the multiple sequence alignment and the tree side-by-side. The function takes a tree object (produced with ggtree()) and the path to the FASTA multiple sequence alignment. You can do it with the entire MSA, or you could restrict to just a window. Want something interesting-looking, but maybe not all that useful? Try changing the coordinate system of the plot itself by passing + coord_polar(theta=\"y\") to the end of the command!\n\n\nR code\nmsaplot(p=ggtree(tree), fasta=\"data/lab13/flu_aasequence.fasta\", window=c(150, 175))"
  },
  {
    "objectID": "labs/lab9_ggtree.html#bonus",
    "href": "labs/lab9_ggtree.html#bonus",
    "title": "Lab 9 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Bonus!",
    "text": "Bonus!\nSee the ggtree vignettes for more details on how these work.\n\nMany trees\nggtree will let you plot many trees at once, and you can facet them the normal ggplot2 way. Let’s generate 3 replicates each of 4 random trees with 10, 25, 50, and 100 tips, plotting them all.\n\n\nR code\nset.seed(42)\ntrees &lt;- lapply(rep(c(10, 25, 50, 100), 3), rtree)\nclass(trees) &lt;- \"multiPhylo\"\nggtree(trees) + facet_wrap(~.id, scale=\"free\", ncol=4) + ggtitle(\"Many trees. Such phylogenetics. Wow.\")\n\n\n\n\n\n\n\n\n\n\n\nPlot tree with other data\nFor showing a phylogenetic tree alongside other panels with your own data, the facet_plot() function accepts a input data.frame and a geom function to draw the input data.\n\n\nR code\n# Generate a random tree with 30 tips\ntree &lt;- rtree(30)\n# Make the original plot\np &lt;- ggtree(tree)\n# generate some random values for each tip label in the data\nd1 &lt;- data.frame(id=tree$tip.label, val=rnorm(30, sd=3))\n# Make a second plot with the original, naming the new plot \"dot\", \n# using the data you just created, with a point geom.\np2 &lt;- facet_plot(p, panel=\"dot\", data=d1, geom=geom_point, aes(x=val), color='red3')\n# Make some more data with another random value.\nd2 &lt;- data.frame(id=tree$tip.label, value = abs(rnorm(30, mean=100, sd=50)))\n# Now add to that second plot, this time using the new d2 data above, \n# This time showing a bar segment, size 3, colored blue.\np3 &lt;- facet_plot(p2, panel='bar', data=d2, geom=geom_segment, \n           aes(x=0, xend=value, y=y, yend=y), size=3, color='blue4') \n# Show all three plots with a scale\np3 + theme_tree2()\n\n\n\n\n\n\n\n\n\n\n\nOverlay organism silouhettes\nphylopic.org hosts free silhouette images of animals, plants, and other life forms, all under Creative Commons or Public Domain. You can use ggtree to overlay a phylopic image on your plot at a node of your choosing.\n\n\nR code\n# get phylopic \n\nnewick &lt;- \"((Pongo_abelii,(Gorilla_gorilla_gorilla,(Pan_paniscus,Pan_troglodytes)Pan,Homo_sapiens)Homininae)Hominidae,Nomascus_leucogenys)Hominoidea;\"\n\ntree &lt;- read.tree(text=newick)\n\nd &lt;- ggimage::phylopic_uid(tree$tip.label)\nd$body_mass = c(52, 114, 47, 45, 58, 6)\n\np &lt;- ggtree(tree) %&lt;+% d + \n  geom_tiplab(aes(image=uid, colour=body_mass), geom=\"phylopic\", offset=2.5) +\n  geom_tiplab(aes(label=label), offset = .2) + xlim(NA, 7) +\n  scale_color_viridis_c()\np"
  },
  {
    "objectID": "labs/lab9_ggtree.html#tips",
    "href": "labs/lab9_ggtree.html#tips",
    "title": "Lab 9 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Tips",
    "text": "Tips\nIf you the tree is not fitting in the plot space you can modify the xlim\n\n\nR code\n# Add to your \nggtree(tree) +\n  xlim(0,2) # This worked from my rectangular trees\n  xlim(0.36) # This worked for my circular trees\n\n\nTo show the bootstrap values\n\n\nR code\nggtree(tree) +  \n  geom_nodelab()\n\n\nTo change text size and position\n\n\nR code\nggtree(tree) +\n  geom_tiplab(hjust=-.1, size = 3)\n\n\nTo change the size of the highlighted clade you can use extend\n\n\nR code\nggtree(tree) +\n  geom_hilight(node=34, fill=\"gold\", extend = 0.5) \n\n\nYou can order the layers when highlighting for certain effects"
  },
  {
    "objectID": "labs/lab9_ggtree.html#exercises",
    "href": "labs/lab9_ggtree.html#exercises",
    "title": "Lab 9 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Exercises",
    "text": "Exercises\nFor the lab turn in a report with the above examples. We will work with the NEON data next week."
  },
  {
    "objectID": "labs/lab9_ggtree.html#bioconductor",
    "href": "labs/lab9_ggtree.html#bioconductor",
    "title": "Lab 9 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Bioconductor",
    "text": "Bioconductor\nBioconductor is an open source, open development software project for genomic data analysis. There are thousands of software packages available and an active user community. R/Bioconductor is one of the primary mechanisms for publishing new genomic data analysis tools. We will use Bioconductor in the this and the following weeks labs.\n\nInstalling Bioconductor, ggtree and Biostrings\nBioconductor is already installed on Unity.\nInstall Bioconductor locally on your computer or Unity by typing in the R console.\n\n\nR code\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(version = \"3.19\")\n\n\nYou may get an error message that BioConductor is already installed.\nIndividual packages that are not part of the BioConductor core such as ggtree and Biostrings can be installed using\n\n\nR code\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n\nBiocManager::install(\"ggtree\")\n\n\n\n\nR code\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n\nBiocManager::install(\"Biostrings\")\n\n\n\n\nThe ggtree Package\nggtree is an R package that extends ggplot2 for visualizating and annotating phylogenetic trees. It is available from Bioconductor. Bioconductor is a project to provide tools for analyzing and annotating various kinds of genomic data. You can search and browse Bioconductor packages here.\n\nggtree Bioconductor page: bioconductor.org/packages/ggtree.\nggtree homepage: guangchuangyu.github.io/ggtree (contains more information about the package, more documentation, a gallery of beautiful images, and links to related resources).\nggtree publication: Yu, Guangchuang Data Integration, Manipulation and Visualization of Phylogenetic Trees\n\nBioconductor packages usually have great documentation in the form of vignettes. Take a look at the landing page for ggtree – about halfway down the page under the “Documentation” heading there are multiple walkthrough tutorials directed to different applications and functionalities of ggtree, chock full of runnable examples and explanations.\nJust like R packages from CRAN, you only need to install Bioconductor packages once, then load them every time you start a new R session. Load the tidyverse package first.\n\n\nR code\n# You may need to install the treeio, rphylopic and ggimage packages\nlibrary(tidyverse)\nlibrary(ggtree)\nlibrary(treeio)\nlibrary(rphylopic)\nlibrary(ggimage)\n\n\nA note on masked functions: Take a second and look through some of the output that you see when you load ggtree after tidyverse. When you first installed ggtree it may have taken a while, because ggtree depends on a number of other R packages. Each of these, in turn, may depend on other packages. These are all loaded into your working environment when you load ggtree. Also notice the lines that start with The following objects are masked from 'package:.... One example of this is the collapse() function from dplyr. When ggtree was loaded, it loaded it’s own function called collapse(). Now, if you wanted to use dplyr’s collapse function, you’ll have to call it explicitly using this kind of syntax: dplyr::collapse(). See this Q&A thread for more."
  },
  {
    "objectID": "labs/lab9_ggtree.html#connecting-to-jupyterlab-on-unity",
    "href": "labs/lab9_ggtree.html#connecting-to-jupyterlab-on-unity",
    "title": "Lab 9 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "Connecting to JupyterLab on Unity",
    "text": "Connecting to JupyterLab on Unity\nIn addition to RStudio, today we are going to connect to Unity using the JupyterLab interface\n\nOnce you open JupyterLab you can navigate to our course site by clicking on the folder icon just to the left of your /home\n\nThan click on work &gt; pi_bio678_umass_edu &gt; data &gt; lab9\n\nYou can now open these files by clicking on them. These are the files we will work with today.\n***You will need to change the path to the files in the examples below by adding /work/pi_bio678_umass_edu/data/lab9/"
  },
  {
    "objectID": "labs/lab9_ggtree.html#ggtree",
    "href": "labs/lab9_ggtree.html#ggtree",
    "title": "Lab 9 - Sequence Alignment, Phylogenetic Analysis and Tree Visualization",
    "section": "ggtree",
    "text": "ggtree\n\nTree Import\nFrom the ggtree landing page take a look at the Tree Data Import vignette. There are many different software packages for creating phylogenetic trees from different types of data, and there are many formats for storing the resulting phylogenetic trees they produce.\nMost tree viewer software (including R packages) focus on Newick and Nexus file formats, and other evolution analysis software might also contain supporting evidence within the file that are ready for annotating a phylogenetic tree. ggtree supports several file formats, including:\n\nNewick\nNexus\nPhylip\nJplace\nNew Hampshire eXtended format (NHX)\n\nand software output from:\n\nBEAST\nEPA\nHYPHY\nPAML\nPHYLDOG\npplacer\nr8s\nRAxML\nRevBayes\n\nThe ggtree package implement several parser functions, including:\n\nread.tree for reading Newick files.\nread.phylip for reading Phylip files.\nread.jplace for reading Jplace files.\nread.nhx for reading NHX files.\nread.beast for parsing output of BEAST\nread.codeml for parsing output of CODEML (rst and mlc files)\nread.codeml_mlc for parsing mlc file (output of CODEML)\nread.hyphy for parsing output of HYPHY\nread.jplace for parsing jplace file including output from EPA and pplacer\nread.nhx for parsing NHX file including output from PHYLODOG and RevBayes\nread.paml_rst for parsing rst file (output of BASEML and CODEML)\nread.r8s for parsing output of r8s\nread.raxml for parsing output of RAxML\n\n\n\nBasic trees\nLet’s first import our tree data. We’re going to work with a made-up phylogeny with 13 taxa tree_newick.nwk. Let’s load the libraries you’ll need if you haven’t already, and then import the tree using read.tree(). Displaying the object itself really isn’t useful. The output just tells you a little bit about the tree itself.\n\n\nR code\ntree &lt;- read.tree(\"data/lab9/tree_newick.nwk\")\n# tree &lt;- read.tree(\"/work/pi_bio678_umass_edu/data/lab9/tree_newick.nwk\")\ntree\n\n\nJust like with ggplot2 we created a basic canvas with ggplot(...) and added layers with +geom_???(), we can do the same here. The ggtree package gives us a geom_tree() function. Because ggtree is built on top of ggplot2, you get ggplot2’s default gray theme with white lines. You can override this with a theme from the ggtree package.\nBecause you’ll almost always want to add a tree geom and remove the default background and axes, the ggtree() function is essentially a shortcut for ggplot(...) + geom_tree() + theme_tree().\n\n\nR code\n# build a ggplot with a geom_tree\nggplot(tree) + geom_tree() + theme_tree()\n# This is convenient shorthand\nggtree(tree)\n\n\n\n\n\n\n\n\n\nThere’s also the treescale geom, which adds a scale bar, or alternatively, you can change the default ggtree() theme to theme_tree2(), which adds a scale on the x-axis. The horizontal dimension in this plot shows the amount of genetic change, and the branches and represent evolutionary lineages changing over time. The longer the branch in the horizonal dimension, the larger the amount of change, and the scale tells you this. The units of branch length are usually nucleotide substitutions per site – that is, the number of changes or substitutions divided by the length of the sequence (alternatively, it could represent the percent change, i.e., the number of changes per 100 bases).\n\n\nR code\n# add a scale\nggtree(tree) + geom_treescale()\n# or add the entire scale to the x axis with theme_tree2()\nggtree(tree) + theme_tree2()\n\n\n\n\n\n\n\n\n\nThe default is to plot a phylogram, where the x-axis shows the genetic change / evolutionary distance. If you want to disable scaling and produce a cladogram instead, set the branch.length=\"none\" option inside the ggtree() call. See ?ggtree for more.\n\n\nR code\nggtree(tree, branch.length=\"none\")\n\n\n\n\n\n\n\n\n\nThe ... option in the help for ?ggtree represents additional options that are further passed to ggplot(). You can use this to change aesthetics of the plot. Let’s draw a cladogram (no branch scaling) using thick blue dotted lines (note that I’m not mapping these aesthetics to features of the data with aes() – we’ll get to that later).\n\n\nR code\nggtree(tree, branch.length=\"none\", color=\"blue\", size=2, linetype=3)\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1\nLook at the help again for ?ggtree, specifically at the layout= option. By default, it produces a rectangular layout.\n\nCreate a slanted phylogenetic tree.\nCreate a circular phylogenetic tree.\nCreate a circular unscaled cladogram with thick red lines.\n\n\n\n\nOther tree geoms\nLet’s add additional layers. We can create a plot object, e.g., p, to store the basic layout of a ggplot, and add more layers to it as we desire. Let’s add node and tip points. Let’s finally label the tips.\n\n\nR code\n# create the basic plot\np &lt;- ggtree(tree)\n# add node points\np + geom_nodepoint()\n# add tip points\np + geom_tippoint()\n# Label the tips\np + geom_tiplab()\n\n\n\n\n\n\nExercise 2\nSimilar to how we change the aesthetics for the tree inside the ggtree() call, we can also change the aesthetics of the points themselves by passing graphical parameters inside the geom_nodepoint() or geom_tippoint() calls. Create a phylogeny with the following aesthetic characteristics:\n\ntips labeled in purple\npurple-colored diamond-shape tip points (hint: Google search “R point characters”)\nlarge semitransparent yellow node points (hint: alpha=)\nAdd a title with + ggtitle(...)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTree annotation\nThe geom_tiplab() function adds some very rudimentary annotation. Let’s take annotation a bit further.\n\nInternal node number\nBefore we can go further we need to understand how ggtree is handling the tree structure internally. Some of the functions in ggtree for annotating clades need a parameter specifying the internal node number. To get the internal node number, user can use geom_text to display it, where the label is an aesthetic mapping to the “node variable” stored inside the tree object (think of this like the continent variable inside the gapminder object). We also supply the hjust option so that the labels aren’t sitting right on top of the nodes.\n\n\nR code\nggtree(tree) + geom_text(aes(label=node), hjust=-.3)\n\n\n\n\n\n\n\n\n\nAnother way to get the internal node number is using MRCA() function by providing a vector of taxa names (created using c(\"taxon1\", \"taxon2\")).. The function will return node number of input taxa’s most recent commond ancestor (MRCA). First, re-create the plot so you can choose which taxa you want to grab the MRCA from.\n\n\nR code\nggtree(tree) + geom_tiplab()\n\n\n\n\n\n\n\n\n\nLet’s grab the most recent common ancestor for taxa C+E, and taxa G+H. We can use MRCA() to get the internal node numbers. Go back to the node-labeled plot from before to confirm this.\n\n\nR code\nMRCA(tree, c(\"C\", \"E\"))\n\n\n[1] 17\n\n\nR code\nMRCA(tree, c(\"G\", \"H\"))\n\n\n[1] 21\n\n\nR code\n# MRCA(tree, tip=c(\"C\", \"E\"))\n# MRCA(tree, tip=c(\"G\", \"H\"))\n\n\n\n\n\nLabeling clades\nWe can use geom_cladelabel() to add another geom layer to annotate a selected clade with a bar indicating the clade with a corresponding label. You select the clades using the internal node number for the node that connects all the taxa in that clade.\nLet’s annotate the clade with the most recent common ancestor between taxa C and E (internal node 17). Let’s make the annotation red. See ?geom_cladelabel help for more.\n\n\nR code\nggtree(tree) + \n  geom_cladelabel(node=17, label=\"Some random clade\", color=\"red\")\n\n\nLet’s add back in the tip labels. Notice how now the clade label is too close to the tip labels. Let’s add an offset to adjust the position. You might have to fiddle with this number to get it looking right.\n\n\nR code\nggtree(tree) + \n  geom_tiplab() + \n  geom_cladelabel(node=17, label=\"Some random clade\", \n                  color=\"red2\", offset=.8)\n\n\nNow let’s add another label for the clade connecting taxa G and H (internal node 21).\n\n\nR code\nggtree(tree) + \n  geom_tiplab() + \n  geom_cladelabel(node=17, label=\"Some random clade\", \n                  color=\"red2\", offset=.8) + \n  geom_cladelabel(node=21, label=\"A different clade\", \n                  color=\"blue\", offset=.8)\n\n\nUh oh. Now we have two problems. First, the labels would look better if they were aligned. That’s simple. Pass align=TRUE to geom_cladelabel() (see ?geom_cladelabel help for more). But now, the labels are falling off the edge of the plot. That’s because geom_cladelabel() is just adding it this layer onto the end of the existing canvas that was originally layed out in the ggtree call. This default layout tried to optimize by plotting the entire tree over the entire region of the plot. Here’s how we’ll fix this.\n\nFirst create the generic layout of the plot with ggtree(tree).\nAdd some tip labels.\nAdd each clade label.\nRemember theme_tree2()? We used it way back to add a scale to the x-axis showing the genetic distance. This is the unit of the x-axis. We need to set the limits on the x-axis. Google around for something like “ggplot2 x axis limits” and you’ll wind up on this StackOverflow page that tells you exactly how to solve it – just add on a + xlim(..., ...) layer. Here let’s extend out the axis a bit further to the right.\nFinally, if we want, we can either comment out the theme_tree2() segment of the code, or we could just add another theme layer on top of the plot altogether, which will override the theme that was set before. theme_tree() doesn’t have the scale.\n\n\n\nR code\nggtree(tree) + \n  geom_tiplab() + \n  geom_cladelabel(node=17, label=\"Some random clade\", \n                  color=\"red2\", offset=.8, align=TRUE) + \n  geom_cladelabel(node=21, label=\"A different clade\", \n                  color=\"blue\", offset=.8, align=TRUE) + \n  theme_tree2() + \n  xlim(0, 70) + \n  theme_tree()\n\n\n\n\n\n\n\n\n\nAlternatively, we could highlight the entire clade with geom_hilight(). See the help for options to tweak.\n\n\nR code\nggtree(tree) + \n  geom_tiplab() + \n  geom_hilight(node=17, fill=\"gold\") + \n  geom_hilight(node=21, fill=\"purple\")\n\n\n\n\n\n\n\n\n\n\n\nConnecting taxa\nSome evolutionary events (e.g. reassortment, horizontal gene transfer) can be visualized with some simple annotations on a tree. The geom_taxalink() layer draws straight or curved lines between any of two nodes in the tree, allow it to show evolutionary events by connecting taxa.\n\n\nR code\nggtree(tree) + \n  geom_tiplab() + \n  geom_taxalink(\"E\", \"H\", color=\"blue3\") +\n  geom_taxalink(\"C\", \"G\", color=\"orange2\", curvature=-.9)\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3\nProduce the figure below.\n\nFirst, find what the MRCA is for taxa B+C, and taxa L+J. You can do this in one of two ways:\n\nEasiest: use MRCA(tree, tip=c(\"taxon1\", \"taxon2\")) for B/C and L/J separately.\nAlternatively: use ggtree(tree) + geom_text(aes(label=node), hjust=-.3) to see what the node labels are on the plot. You might also add tip labels here too.\n\nDraw the tree with ggtree(tree).\nAdd tip labels.\nHighlight these clades with separate colors.\nAdd a clade label to the larger superclade (node=17) that we saw before that includes A, B, C, D, and E. You’ll probably need an offset to get this looking right.\nLink taxa C to E, and G to J with a dashed gray line (hint: get the geom working first, then try changing the aesthetics. You’ll need linetype=2 somewhere in the geom_taxalink()).\nAdd a scale bar to the bottom by changing the theme.\nAdd a title.\nOptionally, go back to the original ggtree(tree, ...) call and change the layout to \"circular\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced tree annotation\nLet’s use a previously published dataset from this paper:\nLiang et al. “Expansion of genotypic diversity and establishment of 2009 H1N1 pandemic-origin internal genes in pigs in China.” Journal of virology (2014): 88(18):10864-74.\nThis data was reanalyzed in the ggtree paper.\nThe subset of the data used here contains 76 H3 hemagglutinin gene sequences of a lineage containing both swine and human influenza A viruses. The sequence data set was re-analyzed by using BEAST (available at http://beast.bio.ed.ac.uk/). BEAST (Bayesian Evolutionary Analysis Sampling Trees) can give you rooted, time-measured phylogenies inferred using molecular clock models.\nFor this you’ll need the flu_tree_beast.tree output file from BEAST and the flu_aasequence.fasta FASTA file with the multiple sequence alignment. These are both available on the data downloads page. First let’s read in the tree with read.beast() (instead of the read.tree() we used before). Let’s add a scale bar with theme_tree2(). This gives you genetic distance. But, we have time measured here with molecular clock models. We’ve only estimated the relative time between branching events, so if we want to actually see dates on the x-axis, we need to supply the most recent sampling date to the ggtree() call. Do this by setting mrsd=\"YYYY-MM-DD\" inside ggtree().\nFinally, let’s add some tip labels. We’ll want to right-align them, and by default the dotted line is a little too thick. Let’s reduce the linesize a bit. Now, some of the labels might be falling off the margin. Set the xlim to limit the axis to show between 1990 and 2020. You could get MRCAs and node numbers and do all the annotations that we did before the same way here.\n\n\nR code\n# Read the data\ntree &lt;- read.beast(\"data/lab9/flu_tree_beast.tree\")\n# tree &lt;- read.tree(\"/work/pi_bio678_umass_edu/data/lab9/flu_tree_beast.tree\")\n# supply a most recent sampling date so you get the dates\n# and add a scale bar\nggtree(tree, mrsd=\"2013-01-01\") + \n  theme_tree2() \n# Finally, add tip labels and adjust axis\nggtree(tree, mrsd=\"2013-01-01\") + \n  theme_tree2() + \n  geom_tiplab(align=TRUE, linesize=.5) + \n  xlim(1990, 2020)\n\n\n\n\n\n\n\n\n\nFinally, let’s look at ?msaplot. This puts the multiple sequence alignment and the tree side-by-side. The function takes a tree object (produced with ggtree()) and the path to the FASTA multiple sequence alignment. You can do it with the entire MSA, or you could restrict to just a window. Want something interesting-looking, but maybe not all that useful? Try changing the coordinate system of the plot itself by passing + coord_polar(theta=\"y\") to the end of the command!\n\n\nR code\n# msaplot(p=ggtree(tree), fasta=\"/work/pi_bio678_umass_edu/data/lab9/flu_aasequence.fasta\", window=c(150, 175))\n\nmsaplot(p=ggtree(tree), fasta=\"data/lab9/flu_aasequence.fasta\", window=c(150, 175))\n\n\n\n\n\n\n\n\n\n\n\nBonus!\nSee the ggtree vignettes for more details on how these work.\n\n\nMany trees\nggtree will let you plot many trees at once, and you can facet them the normal ggplot2 way. Let’s generate 3 replicates each of 4 random trees with 10, 25, 50, and 100 tips, plotting them all.\n\n\nR code\nset.seed(42)\ntrees &lt;- lapply(rep(c(10, 25, 50, 100), 3), rtree)\nclass(trees) &lt;- \"multiPhylo\"\nggtree(trees) + facet_wrap(~.id, scale=\"free\", ncol=4) + ggtitle(\"Many trees. Such phylogenetics. Wow.\")\n\n\n\n\n\n\n\n\n\n\n\nPlotting trees with other data\nFor showing a phylogenetic tree alongside other panels with your own data, the facet_plot() function accepts a input data.frame and a geom function to draw the input data.\n\n\nR code\n# Generate a random tree with 30 tips\ntree &lt;- rtree(30)\n# Make the original plot\np &lt;- ggtree(tree)\n# generate some random values for each tip label in the data\nd1 &lt;- data.frame(id=tree$tip.label, val=rnorm(30, sd=3))\n# Make a second plot with the original, naming the new plot \"dot\", \n# using the data you just created, with a point geom.\np2 &lt;- facet_plot(p, panel=\"dot\", data=d1, geom=geom_point, aes(x=val), color='red3')\n# Make some more data with another random value.\nd2 &lt;- data.frame(id=tree$tip.label, value = abs(rnorm(30, mean=100, sd=50)))\n# Now add to that second plot, this time using the new d2 data above, \n# This time showing a bar segment, size 3, colored blue.\np3 &lt;- facet_plot(p2, panel='bar', data=d2, geom=geom_segment, \n           aes(x=0, xend=value, y=y, yend=y), size=3, color='blue4') \n# Show all three plots with a scale\np3 + theme_tree2()\n\n\n\n\n\n\n\n\n\n\n\nOverlay organism silouhettes\nphylopic.org hosts free silhouette images of animals, plants, and other life forms, all under Creative Commons or Public Domain. You can use ggtree to overlay a phylopic image on your plot at a node of your choosing.\n\n\nR code\n# get phylopic \n\nnewick &lt;- \"((Pongo_abelii,(Gorilla_gorilla_gorilla,(Pan_paniscus,Pan_troglodytes)Pan,Homo_sapiens)Homininae)Hominidae,Nomascus_leucogenys)Hominoidea;\"\n\ntree &lt;- read.tree(text=newick)\n\nd &lt;- ggimage::phylopic_uid(tree$tip.label)\nd$body_mass = c(52, 114, 47, 45, 58, 6)\n\np &lt;- ggtree(tree) %&lt;+% d + \n  geom_tiplab(aes(image=uid, colour=body_mass), geom=\"phylopic\", offset=2.5) +\n  geom_tiplab(aes(label=label), offset = .2) + xlim(NA, 7) +\n  scale_color_viridis_c()\np  \n\n\n\n\n\n\n\n\n\n\n\nTips\nIf you the tree is not fitting in the plot space you can modify the xlim\n\n\nR code\n# Add to your \nggtree(tree) +\n  xlim(0,2) # This worked from my rectangular trees\n  xlim(0.36) # This worked for my circular trees\n\n\nTo show the bootstrap values\n\n\nR code\nggtree(tree) +  \n  geom_nodelab()\n\n\nTo change text size and position\n\n\nR code\nggtree(tree) +\n  geom_tiplab(hjust=-.1, size = 3)\n\n\nTo change the size of the highlighted clade you can use extend\n\n\nR code\nggtree(tree) +\n  geom_hilight(node=9, fill=\"gold\", extend = 0.5) \n\n\nYou can order the layers when highlighting for certain effects"
  },
  {
    "objectID": "labs/lab3s_github.html#introduction-to-the-shell-and-terminal",
    "href": "labs/lab3s_github.html#introduction-to-the-shell-and-terminal",
    "title": "Lab S4 - Working on the command line",
    "section": "Introduction to the Shell and Terminal",
    "text": "Introduction to the Shell and Terminal\nThe terminal is a text-based interface that allows users to interact with their computer by typing commands. It’s a powerful tool for controlling the system, running programs, and managing files.\nThe shell is the program that interprets these commands. When you open a terminal window, you’re typically interacting with a shell—commonly bash (Bourne Again SHell), though others like zsh, fish, or sh may be used.\n\nKey Concepts:\n\nPrompt: The line where you type commands, often ending in $ for regular users or # for root.\nCommands: Instructions you give to the shell, like ls (list files), cd (change directory), or mkdir (make directory).\nArguments: Extra information passed to commands, e.g., ls -l lists files in long format.\nPipes and Redirection: You can chain commands (|) or redirect output (&gt; or &lt;) to files.\nScripts: You can write shell scripts—text files with a series of commands—to automate tasks.\n\n\n\nWhy Use the Shell?\n\nEfficiency: Perform tasks faster than with a graphical interface.\nAutomation: Easily automate repetitive tasks.\nRemote Access: Manage systems over SSH.\nControl: Fine-grained control over system behavior and configuration."
  },
  {
    "objectID": "labs/lab3s_github.html#accessing-the-shell-and-terminal-on-unity",
    "href": "labs/lab3s_github.html#accessing-the-shell-and-terminal-on-unity",
    "title": "Lab S4 - Working on the command line",
    "section": "Accessing the Shell and Terminal on Unity",
    "text": "Accessing the Shell and Terminal on Unity\nOn Microsoft’s Windows and Apple’s OSX there are terminal programs available, but user’s in those communities rarely use the terminal to run shell commands.\nThere are many ways to access the Shell and Terminal on Unity.\n\nYou can access the shell directly without launching a new job.\n\n\n\n\nIn RStudio the terminal is available in the bottom left quadrant\n\n 3. Using JypterLab"
  },
  {
    "objectID": "labs/lab3s_github.html#shell-tutorial",
    "href": "labs/lab3s_github.html#shell-tutorial",
    "title": "Lab S4 - Working on the command line",
    "section": "Shell Tutorial",
    "text": "Shell Tutorial\n\nGetting started\n\nOn Unity under Interactive Apps Open a JupyterLab session\nIn the Launcher start a terminal session.\n\nTry This:\n\nType: echo “Hello, world!”\nThis prints text to the screen.\n\n\n\nNavigating the Filesystem\nKey Commands:\n\npwd – Print current directory.\nls – List files.\ncd – Change directory.\n\nTry This:\n$ pwd\n\n$ ls\n\n$ cd \\~        \\# Go to home directory\n\n$ cd /        \\# Go to root directory\n\n$ cd -        \\# Go back to previous directory\n\n$ cd pathogen         \\# To move into a directory\n\n$ cd ..         \\# To go up one directory\nTips:\n\nUse ls -l for detailed listing.\nUse ls -a to show hidden files.\n\n\n\nWorking with Files and Directories\nKey Commands:\n\ntouch – Create a file.\nmkdir – Make a directory.\nrm – Remove a file.\nrmdir – Remove a directory.\ncp – Copy files.\nmv – Move or rename files.\n\nTry This:\n$ mkdir test_folder\n\n$ cd test_folder\n\n$ touch file1.txt\n\n$ ls\n\n$ cp file1.txt file2.txt\n\n$ mv file2.txt renamed.txt\n\n$ rm file1.txt\n\n$ cd ..\n\n$ rmdir test_folder\n\n**Caution:** `rm` deletes files permanently. Use with care.\n\n\n\nViewing and Editing Files\nKey Commands: - cat – View file contents. - less – Scroll through file. - nano – Simple text editor.\nTry This:\n$ touch test.txt\n\n$ echo \"This is a test file.\" &gt; test.txt\n\n$ cat test.txt\n\n## To exit type `q`\n\n$ less test.txt\n\n## To exit type `q`\n\n$ nano test.txt\n\n## To exit `type`ctr x`\nTip: Press Ctrl+X to exit nano.\n\n\nUseful Tools and Shortcuts\nKey Concepts:\n\nman – Manual pages.\nhistory – View command history.\nclear – Clear terminal screen.\nTab completion – Auto-complete file names.\nArrow keys – Navigate command history.\n\nTry This:\n$ man ls\n\n$ history\n\n$ clear\nBonus:\n\nUse grep to search inside files:\n\n$ grep \"test\" test.txt"
  },
  {
    "objectID": "labs/lab3s_github.html#exercises",
    "href": "labs/lab3s_github.html#exercises",
    "title": "Lab S4 - Working on the command line",
    "section": "Exercises",
    "text": "Exercises\nComplete the above commands. Open a text file and save as myhistory.txt. Type history into the shell as above. Copy and paste the history into the myhistory.txt file. Download the file to your computer and then upload the file to Canvas."
  },
  {
    "objectID": "labs/lab4s_shell.html",
    "href": "labs/lab4s_shell.html",
    "title": "Lab S4 - Working on the command line in Terminal with Shell",
    "section": "",
    "text": "In this lesson, you will learn:\n\nLearn basic shell commands to navigate, manage files, and perform simple tasks in the terminal."
  },
  {
    "objectID": "labs/lab4s_shell.html#learning-objectives",
    "href": "labs/lab4s_shell.html#learning-objectives",
    "title": "Lab S4 - Working on the command line in Terminal with Shell",
    "section": "",
    "text": "In this lesson, you will learn:\n\nLearn basic shell commands to navigate, manage files, and perform simple tasks in the terminal."
  },
  {
    "objectID": "labs/lab4s_shell.html#introduction-to-the-shell-and-terminal",
    "href": "labs/lab4s_shell.html#introduction-to-the-shell-and-terminal",
    "title": "Lab S4 - Working on the command line in Terminal with Shell",
    "section": "Introduction to the Shell and Terminal",
    "text": "Introduction to the Shell and Terminal\nThe terminal is a text-based interface that allows users to interact with their computer by typing commands. It’s a powerful tool for controlling the system, running programs, and managing files.\nThe shell is the program that interprets these commands. When you open a terminal window, you’re typically interacting with a shell—commonly bash (Bourne Again SHell), though others like zsh, fish, or sh may be used.\n\nKey Concepts:\n\nPrompt: The line where you type commands, often ending in $ for regular users or # for root.\nCommands: Instructions you give to the shell, like ls (list files), cd (change directory), or mkdir (make directory).\nArguments: Extra information passed to commands, e.g., ls -l lists files in long format.\nPipes and Redirection: You can chain commands (|) or redirect output (&gt; or &lt;) to files.\nScripts: You can write shell scripts—text files with a series of commands—to automate tasks.\n\n\n\nWhy Use the Shell?\n\nEfficiency: Perform tasks faster than with a graphical interface.\nAutomation: Easily automate repetitive tasks.\nRemote Access: Manage systems over SSH.\nControl: Fine-grained control over system behavior and configuration."
  },
  {
    "objectID": "labs/lab4s_shell.html#accessing-the-shell-and-terminal-on-unity",
    "href": "labs/lab4s_shell.html#accessing-the-shell-and-terminal-on-unity",
    "title": "Lab S4 - Working on the command line in Terminal with Shell",
    "section": "Accessing the Shell and Terminal on Unity",
    "text": "Accessing the Shell and Terminal on Unity\nOn Microsoft’s Windows and Apple’s OSX there are terminal programs available, but user’s in those communities rarely use the terminal to run shell commands.\nThere are many ways to access the Shell and Terminal on Unity.\n\nYou can access the shell directly without launching a new job.\n\n\n\n\nIn RStudio the terminal is available in the bottom left quadrant\n\n 3. Using JypterLab"
  },
  {
    "objectID": "labs/lab4s_shell.html#shell-tutorial",
    "href": "labs/lab4s_shell.html#shell-tutorial",
    "title": "Lab S4 - Working on the command line in Terminal with Shell",
    "section": "Shell Tutorial",
    "text": "Shell Tutorial\n\nGetting started\n\nOn Unity under Interactive Apps Open a JupyterLab session\nIn the Launcher start a terminal session.\n\nTry This:\n\nType: echo “Hello, world!”\nThis prints text to the screen.\n\n\n\nNavigating the Filesystem\nKey Commands:\n\npwd – Print current directory.\nls – List files.\ncd – Change directory.\n\nTry This:\n$ pwd\n\n$ ls\n\n$ cd \\~        \\# Go to home directory\n\n$ cd /        \\# Go to root directory\n\n$ cd -        \\# Go back to previous directory\n\n$ cd pathogen         \\# To move into a directory\n\n$ cd ..         \\# To go up one directory\nTips:\n\nUse ls -l for detailed listing.\nUse ls -a to show hidden files.\n\n\n\nWorking with Files and Directories\nKey Commands:\n\ntouch – Create a file.\nmkdir – Make a directory.\nrm – Remove a file.\nrmdir – Remove a directory.\ncp – Copy files.\nmv – Move or rename files.\n\nTry This:\n$ mkdir test_folder\n\n$ cd test_folder\n\n$ touch file1.txt\n\n$ ls\n\n$ cp file1.txt file2.txt\n\n$ mv file2.txt renamed.txt\n\n$ rm file1.txt\n\n$ cd ..\n\n$ rmdir test_folder\n\n**Caution:** `rm` deletes files permanently. Use with care.\n\n\n\nViewing and Editing Files\nKey Commands: - cat – View file contents. - less – Scroll through file. - nano – Simple text editor.\nTry This:\n$ touch test.txt\n\n$ echo \"This is a test file.\" &gt; test.txt\n\n$ cat test.txt\n\n## To exit type `q`\n\n$ less test.txt\n\n## To exit type `q`\n\n$ nano test.txt\n\n## To exit `type`ctr x`\nTip: Press Ctrl+X to exit nano.\n\n\nUseful Tools and Shortcuts\nKey Concepts:\n\nman – Manual pages.\nhistory – View command history.\nclear – Clear terminal screen.\nTab completion – Auto-complete file names.\nArrow keys – Navigate command history.\n\nTry This:\n$ man ls\n\n$ history\n\n$ clear\nBonus:\n\nUse grep to search inside files:\n\n$ grep \"test\" test.txt"
  },
  {
    "objectID": "labs/lab4s_shell.html#exercises",
    "href": "labs/lab4s_shell.html#exercises",
    "title": "Lab S4 - Working on the command line in Terminal with Shell",
    "section": "Exercises",
    "text": "Exercises\nComplete the above commands. Open a text file and save as myhistory.txt. Type history into the shell as above. Copy and paste the history into the myhistory.txt file. Download the file to your computer and then upload the file to Canvas."
  },
  {
    "objectID": "labs/lab10_regex.html",
    "href": "labs/lab10_regex.html",
    "title": "Lab 10 : Detecting Patterns with Regular Expressions",
    "section": "",
    "text": "How to use regular expressions in R"
  },
  {
    "objectID": "labs/lab10_regex.html#learning-objectives",
    "href": "labs/lab10_regex.html#learning-objectives",
    "title": "Lab 10 : Detecting Patterns with Regular Expressions",
    "section": "",
    "text": "How to use regular expressions in R"
  },
  {
    "objectID": "labs/lab10_regex.html#load-libaries",
    "href": "labs/lab10_regex.html#load-libaries",
    "title": "Lab 10 : Detecting Patterns with Regular Expressions",
    "section": "Load libaries",
    "text": "Load libaries\n\n\nR code\nlibrary(tidyverse)"
  },
  {
    "objectID": "labs/lab10_regex.html#what-are-regular-expressions",
    "href": "labs/lab10_regex.html#what-are-regular-expressions",
    "title": "Lab 10 : Detecting Patterns with Regular Expressions",
    "section": "What Are Regular Expressions?",
    "text": "What Are Regular Expressions?\nRegular expressions are sequences of characters that define search patterns. In R, they are commonly used for:\n\nSearching text\nExtracting substrings\nReplacing patterns\nValidating formats (e.g., emails, dates)\n\n\nKey Functions in R for Regex\nR provides several base functions that support regular expressions:\n\n1. grep() / grepl()\n\ngrep() returns indices of matches.\ngrepl() returns a logical vector indicating matches.\n\n\n\nR code\ngrepl(\"cat\", c(\"cat\", \"dog\", \"catalog\"))\n\n\n[1]  TRUE FALSE  TRUE\n\n\n\n\n2. sub() / gsub()\n\nsub() replaces the first match.\ngsub() replaces all matches.\n\n\n\nR code\ngsub(\"dog\", \"cat\", \"dog and dog\")\n\n\n[1] \"cat and cat\"\n\n\n\n\n3. regexpr() / gregexpr()\n\nReturn the position of the first/all matches and their lengths.\n\n\n\n4. regmatches()\n\nExtracts matched substrings based on regexpr() or gregexpr().\n\n\n\n\nRegex Syntax Basics\nHere are some commonly used regex symbols:\n\n\n\n\n\n\n\n\nSymbol\nMeaning\nExample\n\n\n\n\n.\nAny character except newline\n\"a.b\" matches “acb”, “a1b”\n\n\n^\nStart of string\n\"^cat\" matches “catfish”\n\n\n$\nEnd of string\n\"cat$\" matches “bobcat”\n\n\n*\n0 or more repetitions\n\"ca*t\" matches “ct”, “cat”, “caaaat”\n\n\n+\n1 or more repetitions\n\"ca+t\" matches “cat”, “caaaat”\n\n\n?\n0 or 1 repetition\n\"ca?t\" matches “ct”, “cat”\n\n\n[]\nCharacter class\n\"[cd]og\" matches “dog”, “cog”\n\n\n|\nOR\n\"cat|dog\" matches either\n\n\n()\nGrouping\n\"(cat|dog)s?\" matches “cat”, “cats”, “dog”, “dogs”\n\n\n\\\\\nEscape special characters\n\"\\\\.\" matches a literal dot\n\n\n\n\n\nExample Use Case\n\n\nR code\ntext &lt;- c(\"apple\", \"banana\", \"cherry\", \"date\")\ngrep(\"a\", text, value = TRUE)  # Matches strings containing 'a'\n\n\n[1] \"apple\"  \"banana\" \"date\"  \n\n\nR code\ngrep(\"^a\", text, value = TRUE)  # Matches strings starting with 'a'\n\n\n[1] \"apple\"\n\n\nR code\ngrep(\"a$\", text, value = TRUE)  # Matches strings ending with 'a'\n\n\n[1] \"banana\""
  },
  {
    "objectID": "labs/lab10_regex.html#key-functions-in-r-for-regex",
    "href": "labs/lab10_regex.html#key-functions-in-r-for-regex",
    "title": "Lab 10 : Detecting Patters with Regular Expressions",
    "section": "Key Functions in R for Regex",
    "text": "Key Functions in R for Regex\nR provides several base functions that support regular expressions:\n\n1. grep() / grepl()\n\ngrep() returns indices of matches.\ngrepl() returns a logical vector indicating matches.\n\n\n\nR code\ngrepl(\"cat\", c(\"cat\", \"dog\", \"catalog\"))\n\n\n[1]  TRUE FALSE  TRUE\n\n\n\n\n2. sub() / gsub()\n\nsub() replaces the first match.\ngsub() replaces all matches.\n\n\n\nR code\ngsub(\"dog\", \"cat\", \"dog and dog\")\n\n\n[1] \"cat and cat\"\n\n\n\n\n3. regexpr() / gregexpr()\n\nReturn the position of the first/all matches and their lengths.\n\n\n\n4. regmatches()\n\nExtracts matched substrings based on regexpr() or gregexpr()."
  },
  {
    "objectID": "labs/lab10_regex.html#regex-syntax-basics",
    "href": "labs/lab10_regex.html#regex-syntax-basics",
    "title": "Lab 10 : Detecting Patters with Regular Expressions",
    "section": "Regex Syntax Basics",
    "text": "Regex Syntax Basics\nHere are some commonly used regex symbols:\n\n\n\n\n\n\n\n\nSymbol\nMeaning\nExample\n\n\n\n\n.\nAny character except newline\n\"a.b\" matches “acb”, “a1b”\n\n\n^\nStart of string\n\"^cat\" matches “catfish”\n\n\n$\nEnd of string\n\"cat$\" matches “bobcat”\n\n\n*\n0 or more repetitions\n\"ca*t\" matches “ct”, “cat”, “caaaat”\n\n\n+\n1 or more repetitions\n\"ca+t\" matches “cat”, “caaaat”\n\n\n?\n0 or 1 repetition\n\"ca?t\" matches “ct”, “cat”\n\n\n[]\nCharacter class\n\"[cd]og\" matches “dog”, “cog”\n\n\n|\nOR\n\"cat|dog\" matches either\n\n\n()\nGrouping\n\"(cat|dog)s?\" matches “cat”, “cats”, “dog”, “dogs”\n\n\n\\\\\nEscape special characters\n\"\\\\.\" matches a literal dot"
  },
  {
    "objectID": "labs/lab10_regex.html#example-use-case",
    "href": "labs/lab10_regex.html#example-use-case",
    "title": "Lab 10 : Detecting Patters with Regular Expressions",
    "section": "Example Use Case",
    "text": "Example Use Case\n\n\nR code\ntext &lt;- c(\"apple\", \"banana\", \"cherry\", \"date\")\ngrep(\"a\", text, value = TRUE)  # Matches strings containing 'a'\n\n\n[1] \"apple\"  \"banana\" \"date\"  \n\n\nR code\ngrep(\"^a\", text, value = TRUE)  # Matches strings starting with 'a'\n\n\n[1] \"apple\"\n\n\nR code\ngrep(\"a$\", text, value = TRUE)  # Matches strings ending with 'a'\n\n\n[1] \"banana\""
  },
  {
    "objectID": "labs/lab10_regex.html#introduction-to-stringr",
    "href": "labs/lab10_regex.html#introduction-to-stringr",
    "title": "Lab 10 : Detecting Patterns with Regular Expressions",
    "section": "Introduction to stringr",
    "text": "Introduction to stringr\nThe base R regular expressions are great to use if you already use regular expressions in another programming language. Starting out the stringr package, part of the tidyverse, provides a cohesive and consistent set of functions for string manipulation. It simplifies working with regular expressions by offering:\n\nConsistent function names (str_*)\nPredictable argument order (string first, pattern second)\nBuilt-in support for vectorized operations\nBetter integration with tidyverse workflows\n\n\nWhy Use stringr rather than base R regex functions?\n\nCleaner syntax: Easier to read and write than base R regex functions.\nTidyverse-friendly: Works well with dplyr, purrr, and other packages.\nConsistent behavior: Avoids quirks of base R functions like grep() and sub().\n\n\n\nCommon stringr Functions for Regex\nHere are some of the most useful functions when working with regular expressions:\n\n1. str_detect()\nChecks if a pattern exists in a string.\n\n\nR code\nstr_detect(c(\"apple\", \"banana\", \"cherry\"), \"^a\")\n\n\n[1]  TRUE FALSE FALSE\n\n\n\n\n2. str_replace() / str_replace_all()\nReplaces the first or all occurrences of a pattern.\n\n\nR code\nstr_replace(\"cat and dog\", \"dog\", \"mouse\")\n\n\n[1] \"cat and mouse\"\n\n\n\n\nR code\nstr_replace_all(\"cat and dog and dog\", \"dog\", \"mouse\")\n\n\n[1] \"cat and mouse and mouse\"\n\n\n\n\n3. str_extract() / str_extract_all()\nExtracts the first or all matches of a pattern.\n\n\nR code\nstr_extract(\"My email is test@example.com\", \"[a-z0-9._%+-]+@[a-z0-9.-]+\\\\.[a-z]{2,}\")\n\n\n[1] \"test@example.com\"\n\n\n\n\n4. str_match() / str_match_all()\nExtracts matched groups using parentheses.\n\n\nR code\nstr_match(\"Name: John\", \"Name: (\\\\w+)\")\n\n\n     [,1]         [,2]  \n[1,] \"Name: John\" \"John\"\n\n\n\n\n5. str_split()\nSplits strings based on a pattern.\n\n\nR code\nstr_split(\"apple,banana,cherry\", \",\")\n\n\n[[1]]\n[1] \"apple\"  \"banana\" \"cherry\"\n\n\n\n\n\nRegex Integration\nAll stringr functions accept regular expressions by default. You can use:\n\n^, $, ., *, +, ?, [], (), | — standard regex symbols\nEscaped characters like \\\\d, \\\\s, \\\\w for digits, whitespace, and word characters\n\n\n\nstringi\nstringr is built on top of the stringi package. stringr is useful when you’re learning because it exposes a minimal set of functions, which have been carefully picked to handle the most common string manipulation functions. stringi, on the other hand, is designed to be comprehensive. It contains almost every function you might ever need: stringi has 250 functions to stringr’s 49."
  },
  {
    "objectID": "labs/lab10_regex.html#why-use-stringr-rather-than-base-r-regex-functions",
    "href": "labs/lab10_regex.html#why-use-stringr-rather-than-base-r-regex-functions",
    "title": "Lab 10 : Detecting Patters with Regular Expressions",
    "section": "Why Use stringr rather than base R regex functions?",
    "text": "Why Use stringr rather than base R regex functions?\n\nCleaner syntax: Easier to read and write than base R regex functions.\nTidyverse-friendly: Works well with dplyr, purrr, and other packages.\nConsistent behavior: Avoids quirks of base R functions like grep() and sub()."
  },
  {
    "objectID": "labs/lab10_regex.html#common-stringr-functions-for-regex",
    "href": "labs/lab10_regex.html#common-stringr-functions-for-regex",
    "title": "Lab 10 : Detecting Patters with Regular Expressions",
    "section": "Common stringr Functions for Regex",
    "text": "Common stringr Functions for Regex\nHere are some of the most useful functions when working with regular expressions:\n\n1. str_detect()\nChecks if a pattern exists in a string.\n\n\nR code\nstr_detect(c(\"apple\", \"banana\", \"cherry\"), \"^a\")\n\n\n[1]  TRUE FALSE FALSE\n\n\n\n\n2. str_replace() / str_replace_all()\nReplaces the first or all occurrences of a pattern.\n\n\nR code\nstr_replace(\"cat and dog\", \"dog\", \"mouse\")\n\n\n[1] \"cat and mouse\"\n\n\n\n\nR code\nstr_replace_all(\"cat and dog and dog\", \"dog\", \"mouse\")\n\n\n[1] \"cat and mouse and mouse\"\n\n\n\n\n3. str_extract() / str_extract_all()\nExtracts the first or all matches of a pattern.\n\n\nR code\nstr_extract(\"My email is test@example.com\", \"[a-z0-9._%+-]+@[a-z0-9.-]+\\\\.[a-z]{2,}\")\n\n\n[1] \"test@example.com\"\n\n\n\n\n4. str_match() / str_match_all()\nExtracts matched groups using parentheses.\n\n\nR code\nstr_match(\"Name: John\", \"Name: (\\\\w+)\")\n\n\n     [,1]         [,2]  \n[1,] \"Name: John\" \"John\"\n\n\n\n\n5. str_split()\nSplits strings based on a pattern.\n\n\nR code\nstr_split(\"apple,banana,cherry\", \",\")\n\n\n[[1]]\n[1] \"apple\"  \"banana\" \"cherry\""
  },
  {
    "objectID": "labs/lab10_regex.html#regex-integration",
    "href": "labs/lab10_regex.html#regex-integration",
    "title": "Lab 10 : Detecting Patters with Regular Expressions",
    "section": "Regex Integration",
    "text": "Regex Integration\nAll stringr functions accept regular expressions by default. You can use:\n\n^, $, ., *, +, ?, [], (), | — standard regex symbols\nEscaped characters like \\\\d, \\\\s, \\\\w for digits, whitespace, and word characters"
  },
  {
    "objectID": "labs/lab10_regex.html#exercises",
    "href": "labs/lab10_regex.html#exercises",
    "title": "Lab 10 : Detecting Patterns with Regular Expressions",
    "section": "Exercises",
    "text": "Exercises\nR for Data Science Chapter 15.\nToday we will walk through Chapter 15 Regular expressions in R for Data Science. As we did last week, by putting the examples and exercises in our own Quarto Markdown file, we can create own personal path through the Chapter.\n\nWhat to upload to Canvas\nAfter you Render the qmd file to an html file, export the file to your computer and upload it to Canvas."
  },
  {
    "objectID": "labs/lab5s_jobs.html",
    "href": "labs/lab5s_jobs.html",
    "title": "Lab S5 - Running bioinformatics software interactively and in batches",
    "section": "",
    "text": "Running bioinformatics software interactively on the command line\nSubmitting batch jobs through SLURM\n\nAs Unity users we can not install software. This is done by the Unity admins.\nOpen a terminal in JupyterLab or otherwise."
  },
  {
    "objectID": "labs/lab5s_jobs.html#learning-objectives",
    "href": "labs/lab5s_jobs.html#learning-objectives",
    "title": "Lab S5 - Running bioinformatics software interactively and in batches",
    "section": "",
    "text": "Running bioinformatics software interactively on the command line\nSubmitting batch jobs through SLURM\n\nAs Unity users we can not install software. This is done by the Unity admins.\nOpen a terminal in JupyterLab or otherwise."
  },
  {
    "objectID": "labs/lab5s_jobs.html#environment-modules",
    "href": "labs/lab5s_jobs.html#environment-modules",
    "title": "Lab S5 - Running bioinformatics software interactively and in batches",
    "section": "Environment modules",
    "text": "Environment modules\nMost of the bioinformatics software we use on Unity needs to be loaded using environmental modules. Environment modules are a convenient and efficient way to use non-standard and version-specific software in Unity.\nOpen up a terminal using JupyterLab or otherwise\n\nCheck If Modules Are Installed\nRun:\nmodule --version\nSince modules are install on Unity will get a version number, you’re good to go. If not, you may need to load the module system manually or ask your system administrator.\n\n\nList Available Modules\nTo see all available modules:\nmodule available\nYou can also use\nmodule avail\nmodule av\nml av\nThis will show a list of software packages and versions you can load.\nSome modules are hidden. They can be shown with module --show_hidden avail. Some modules can only be found after their parent modules have been loaded. To find version numbers and parent modules, you can use the command: $ module –show_hidden spider  or our web interface: https://ood.unity.rc.umass.edu/pun/sys/module-explorer\n\n\nLoad a Module\nTo load a module (e.g., R 4.4.0):\nmodule load  r/4.4.0  \nThis updates your environment variables (like PATH, LD_LIBRARY_PATH, etc.) so you can use that version of R. However, it does not start R. To do this simply type\nR\nThis will start R version 4.4.0 (2024-04-24) – “Puppy Cup”\nNow you can work in the R console. The R console is also available in RStudio in the bottom left corner.\n\n\nR code\nx &lt;- 1\ny &lt;- 2\nx + y\n\n\nTo quit the R console\n\n\nR code\nquit()\n\n\n\n\nRunning an R script from the command line\nOpen a new file using nano or in JupyterLab Select File &gt; New &gt; Text File\nnano addition.R\nAdd the R code (do not use the code chunk format, bust the commands you ran above)\nx &lt;- 1\ny &lt;- 2\nx + y\nUse ctr x to save and exit nano\nNow on the terminal run the R code (make sure you loaded the r module above)\nRscript addition.R\n\n\nSee Loaded Modules\nTo list currently loaded modules:\nmodule list\n\n\nUnload a Module\nTo remove a module from your environment:\nmodule unload r/4.4.0 \n\n\nPurge All Modules\nTo unload all currently loaded modules:\nmodule purge\n\n\nBonus Tips\n\nSearch for all versions of a module:\nmodule spider r\nGet help on a module:\nmodule help r/4.4.0"
  },
  {
    "objectID": "labs/lab5s_jobs.html#conda-environments",
    "href": "labs/lab5s_jobs.html#conda-environments",
    "title": "Lab S5 - Working on the command line in Terminal with Shell",
    "section": "Conda environments",
    "text": "Conda environments\nThe conda package manager allows users to compile software easily and without administrator privileges. Conda creates environments for a set of compatible software, and you can activate the environments as needed.\nRelevant Documentation:\nConda environments"
  },
  {
    "objectID": "labs/lab_renv.html",
    "href": "labs/lab_renv.html",
    "title": "Lab S4 - Working on the command line in Terminal with Shell",
    "section": "",
    "text": "Example of R run on the command line"
  },
  {
    "objectID": "labs/lab_renv.html#learning-objectives",
    "href": "labs/lab_renv.html#learning-objectives",
    "title": "Lab S4 - Working on the command line in Terminal with Shell",
    "section": "",
    "text": "Example of R run on the command line"
  },
  {
    "objectID": "labs/lab5s_jobs.html#submitting-jobs-with-sbatch-and-slurm",
    "href": "labs/lab5s_jobs.html#submitting-jobs-with-sbatch-and-slurm",
    "title": "Lab S5 - Running bioinformatics software interactively and in batches",
    "section": "Submitting Jobs with sbatch and SLURM",
    "text": "Submitting Jobs with sbatch and SLURM\nA batch job refers to a task or a series of tasks that can be executed without user intervention. These jobs are submitted to a job scheduler, which manages resources and executes them when the required resources (such as CPUs, memory, etc.) become available. Unity uses Slurm (Simple Linux Utility for Resource Management), a popular open-source job scheduler used in many supercomputing clusters and high-performance computing (HPC) setups.\nsbatch is a command within Slurm that is used to submit batch jobs. sbatch is a non-blocking command, meaning there is no circumstance where running the command will cause it to hold. If the resources requested in the batch job are unavailable, the job will be placed into a queue and will start to run once resources become available.\nHere are the steps\nOpen a new file using nano or in JupyterLab Select File &gt; New &gt; Text File\nnano sbatch_addition.sh\n\nCreate a Job Script\nA job script is a Bash script with SLURM directives at the top. Here’s a basic example:\n#!/bin/bash\n#SBATCH -J sbatch_addition.sh  # Name of the file\n#SBATCH -c 2  # Number of cpus\n#SBATCH --mem=4G # Requested Memor\n#SBATCH -t 01:00:00  # Job time limit\n#SBATCH -o slurm-%j.out  # %j = job ID\n\n# Load modules\nmodule load r/4.4.0\n\n# Run your command\nRscript addition.R\nSave this as sbatch_addition.sh.\n\n\nMaking the shell script executable\nTo be able to run sbatch_addition.sh it the file permissions must be changed. This is done in the terminal using chmod (for more info see https://cets.seas.upenn.edu/answers/chmod.html).\nchmod +x sbatch_addition.sh\n\n\nSubmit the Job\nUse sbatch to submit the job:\nsbatch sbatch_addition.sh\nSLURM will return a job ID, e.g.:\nSubmitted batch job 123456\nWhen the job is complete you will see a new file in your directory with your ID slurm-123456.txt\nWe could also include in our addition.R file code to print to a file\n\n\nMonitor the Job\nCheck job status:\nsqueue -u $USER\nCancel a job:\nscancel &lt;job_id&gt;\n\n\nCommon SLURM Directives\n\n\n\nDirective\nDescription\n\n\n\n\n--job-name\nName of the job\n\n\n--output / --error\nOutput and error file names\n\n\n--time\nMax runtime\n\n\n--partition\nWhich queue/partition to use\n\n\n--ntasks\nNumber of tasks (often 1 for serial jobs)\n\n\n--cpus-per-task\nNumber of CPU cores per task\n\n\n--mem\nMemory per node"
  },
  {
    "objectID": "labs/lab5s_jobs.html#exercises",
    "href": "labs/lab5s_jobs.html#exercises",
    "title": "Lab S5 - Running bioinformatics software interactively and in batches",
    "section": "Exercises",
    "text": "Exercises\nLike last week complete the above commands. Open a text file and save as myhistory_lab5s.txt. Type history into the terminal. Copy and paste the history into the myhistory.txt file. Download the file to your computer and then upload the file to Canvas."
  },
  {
    "objectID": "labs/lab11_joins.html",
    "href": "labs/lab11_joins.html",
    "title": "Lab 11 : Joining Data Frames",
    "section": "",
    "text": "Keys\nBasic joins\nFiltering joins\nNon-equi joins"
  },
  {
    "objectID": "labs/lab11_joins.html#learning-objectives",
    "href": "labs/lab11_joins.html#learning-objectives",
    "title": "Lab 11 : Joining Data Frames",
    "section": "",
    "text": "Keys\nBasic joins\nFiltering joins\nNon-equi joins"
  },
  {
    "objectID": "labs/lab11_joins.html#exercises",
    "href": "labs/lab11_joins.html#exercises",
    "title": "Lab 11 : Joining Data Frames",
    "section": "Exercises",
    "text": "Exercises\nToday in preparation for joining data frames with NEON data we will walk through Chapter 19 Joins in R for Data Science. As we did earlier, by putting the examples and exercises in our own Quarto Markdown file, we can create own personal path through the Chapter.\n\nWhat to upload to Canvas\nAfter you Render the qmd file to an html file, export the file to your computer and upload it to Canvas."
  },
  {
    "objectID": "labs/lab11_joins.html#overview",
    "href": "labs/lab11_joins.html#overview",
    "title": "Lab 11 : Joining Data Frames",
    "section": "Overview",
    "text": "Overview\n\nLoad the tidyverse\n\n\nR code\nlibrary(tidyverse)\n\n\n\n\nCommon Join Functions\ndplyr offers several types of joins:\n\ninner_join(x, y, by): Keeps only rows with matching keys in both data frames.\nleft_join(x, y, by): Keeps all rows from x and adds matching data from y.\nright_join(x, y, by): Keeps all rows from y and adds matching data from x.\nfull_join(x, y, by): Keeps all rows from both data frames.\nsemi_join(x, y, by): Keeps rows from x that have a match in y (no columns from y).\nanti_join(x, y, by): Keeps rows from x that do not have a match in y.\n\n\n\nExample Dat\n\n\nR code\nspecies &lt;- tibble(id = c(1, 2, 3),\n              name = c(\"Species1\", \"Species2\", \"Species3\"))\n\nabundance &lt;- tibble(id = c(2, 3, 4),\n              abundance = c(85, 90, 95))\n\n\n\n\nPerform Joins\n\nLeft Join\n\n\nR code\nleft_join(species, abundance, by = \"id\")\n\n\n# A tibble: 3 × 3\n     id name     abundance\n  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;\n1     1 Species1        NA\n2     2 Species2        85\n3     3 Species3        90\n\n\nResult: All rows from species, scores added where available.\n\n\nInner Join\n\n\nR code\ninner_join(species, abundance, by = \"id\")\n\n\n# A tibble: 2 × 3\n     id name     abundance\n  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;\n1     2 Species2        85\n2     3 Species3        90\n\n\nResult: Only rows with id 2 and 3.\n\n\nFull Join\n\n\nR code\nfull_join(species, abundance, by = \"id\")\n\n\n# A tibble: 4 × 3\n     id name     abundance\n  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;\n1     1 Species1        NA\n2     2 Species2        85\n3     3 Species3        90\n4     4 &lt;NA&gt;            95\n\n\nResult: All rows from both, with NA where no match.\n\n\n\nNotes\n\nIf column names differ, use by = c(\"col_in_x\" = \"col_in_y\").\nJoins preserve the order of the first data frame."
  },
  {
    "objectID": "methods/Access_NEON_Data_for_Metagenomics_soilchem.html",
    "href": "methods/Access_NEON_Data_for_Metagenomics_soilchem.html",
    "title": "Access NEON Data for Metagenomics",
    "section": "",
    "text": "From Access NEON Data for Metagenomics See * Update on the changing NEON microbial data * Soil microbe metagenome sequences\nInstall packages\nR code\ninstall.packages(\"neonUtilities\")\ninstall.packages(\"neonOS\")\nR code\nlibrary(neonUtilities)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(viridis)\nR code\n# soilTrialSites = c(\"BONA\",\"DEJU\",\"HEAL\",\"TOOL\",\"BARR\")\n# soilTrialSites = c(\"HARV\")\n\n\nsoilChem &lt;- loadByProduct(\n  dpID='DP1.10086.001',\n  startdate = \"2023-01\",\n  enddate = \"2023-12\",\n  check.size = FALSE,\n#  site = soilTrialSites,\n  package='expanded')\nThis is collection of data frames called an R Data Object. Because it takes a while to download we will save it. This will also ensure that we are using this version of the soil chemistry data for all future analysis ()\nR code\nsaveRDS(soilChem, file = \"../data/NEON_metadata/soilChem.rds\")\nTo load the data\nR code\nsoilChem &lt;- readRDS(\"../data/NEON_metadata/soilChem.rds\")\nR code\nView(soilChem$sls_metagenomicsPooling)"
  },
  {
    "objectID": "methods/Access_NEON_Data_for_Metagenomics_soilchem.html#additional-examples-for-accessing-data",
    "href": "methods/Access_NEON_Data_for_Metagenomics_soilchem.html#additional-examples-for-accessing-data",
    "title": "Access NEON Data for Metagenomics",
    "section": "Additional examples for accessing data",
    "text": "Additional examples for accessing data\nHere are some other examples to help you get started with NEON metagenomic data\n\nMerging pH measurements\nThe example above showing how to merge data works for many straightforward measurements, for which calculating the mean is logical. For pH measures, however, this won’t work. Since pH is a logarithmic scale, averaging them will not work. Fortunately, the R package respirometry includes the function mean_pH for averaging pH measurements. This function first converts each pH measure to hydrogen ion concentration [H+], averages the measures, then converts back to the logarithmic scale.\nBelow we show an example with the existing data. First we will create a new table with pH measurements, only keeping the samples from our metagenomic set:\n\n\nR code\nsoilpH_Example &lt;- soilChem$sls_soilpH %&gt;%\n  dplyr::filter(sampleID %in% combinedTab$sampleID) %&gt;%\n  dplyr::select(sampleID,soilInWaterpH,soilInCaClpH)\n\n# now join with the existing table\ncombinedTab_pH &lt;- left_join(combinedTab,soilpH_Example, by = \"sampleID\")\n\n# and the final\nhead(combinedTab_pH)\n\n\n# A tibble: 6 × 5\n  sampleID              genomicsSampleID soilMoisture soilInWaterpH soilInCaClpH\n  &lt;chr&gt;                 &lt;chr&gt;                   &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1 HARV_034-O-23-202307… HARV_034-O-2023…         3.41          3.45         2.7 \n2 HARV_034-O-41-202307… HARV_034-O-2023…         2.98          3.28         2.6 \n3 HARV_034-O-39-202307… HARV_034-O-2023…         2.70          3.34         2.71\n4 HARV_033-O-39-202307… HARV_033-O-2023…         3.48          3.85         3.04\n5 HARV_033-O-21-202307… HARV_033-O-2023…         2.76          3.99         3.29\n6 HARV_033-O-41-202307… HARV_033-O-2023…         3.31          3.63         2.84\n\n\nNow, we can apply the same kind of tidyverse approach as the previous example, only using the mean_pH function:\n\n\nR code\nlibrary(respirometry)\n\ngenome_groups_pH &lt;- combinedTab_pH %&gt;%\n  group_by(genomicsSampleID) %&gt;%\n  summarize_at(c(\"soilInWaterpH\",\"soilInCaClpH\"), mean_pH) \n\nhead(genome_groups_pH)\n\n\n# A tibble: 6 × 3\n  genomicsSampleID         soilInWaterpH soilInCaClpH\n  &lt;chr&gt;                            &lt;dbl&gt;        &lt;dbl&gt;\n1 BONA_001-O-20230710-COMP          3.99         3.45\n2 BONA_002-O-20230711-COMP          4.35         3.82\n3 BONA_004-O-20230712-COMP          3.69         3.10\n4 BONA_006-O-20230710-COMP          4.05         3.48\n5 BONA_009-O-20230711-COMP          4.08         3.50\n6 BONA_013-O-20230710-COMP          3.97         3.42\n\n\nOne thing to note with the previous command is that all the other chemical data was lost when you ran the command. In this example we use two summarize_at commands to apply different functions to the two types of variables, and then left_join will combine them:\n\n\nR code\ngenome_groups_all &lt;- combinedTab_pH %&gt;%\n  group_by(genomicsSampleID) %&gt;%\n  {left_join(\n    summarize_at(.,vars(\"soilMoisture\"), mean),\n    summarize_at(.,vars(\"soilInWaterpH\",\"soilInCaClpH\"), mean_pH)\n  )}\nhead(genome_groups_all)\n\n\n# A tibble: 6 × 4\n  genomicsSampleID         soilMoisture soilInWaterpH soilInCaClpH\n  &lt;chr&gt;                           &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1 BONA_001-O-20230710-COMP         1.55          3.99         3.45\n2 BONA_002-O-20230711-COMP         1.14          4.35         3.82\n3 BONA_004-O-20230712-COMP         3.55          3.69         3.10\n4 BONA_006-O-20230710-COMP         2.54          4.05         3.48\n5 BONA_009-O-20230711-COMP         1.93          4.08         3.50\n6 BONA_013-O-20230710-COMP         2.45          3.97         3.42"
  },
  {
    "objectID": "labs/lab12_soilchem.html",
    "href": "labs/lab12_soilchem.html",
    "title": "Lab 12 - Accessing NEON Metadata for Metagenomics",
    "section": "",
    "text": "This lab was designed based on the tutorial Access NEON Data for Metagenomics by Kaye Shek and Hugh Cross. Some of the text blocks are repeated verbatim. See"
  },
  {
    "objectID": "labs/lab12_soilchem.html#learning-objectives",
    "href": "labs/lab12_soilchem.html#learning-objectives",
    "title": "Lab 12 - Accessing NEON Metadata for Metagenomics",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nStoring Multiple Data Frames in a Single Object in R\nHow to save and read RDS files\nAccessing data from a national repository\nJoining data frames"
  },
  {
    "objectID": "labs/lab12_soilchem.html#fixing-lab-9-and-future-labs",
    "href": "labs/lab12_soilchem.html#fixing-lab-9-and-future-labs",
    "title": "Lab 12 - Accessing NEON Metadata for Metagenomics",
    "section": "Fixing lab 9 and future labs",
    "text": "Fixing lab 9 and future labs\nThe conflict in lab 9 was the result of RStudio and Quarto using a different order of preferred R library paths. To resolve this.\nAfter you start RStudio on Unity type in the R console\n .libPaths()\nThis is the result in my Unity directory is\n[1] \"/home/jlb_umass_edu/R/x86_64-pc-linux-gnu-library/4.5\"\n[2] \"/usr/local/lib/R/site-library\"                        \n[3] \"/usr/local/lib/R/library\"  \nIn your quarto .qmd file, after the YAML block add the following R code chunk which includes your personal libPath. For example\n.libPaths(c(\"/home/jlb_umass_edu/R/x86_64-pc-linux-gnu-library/4.5\", \"/usr/local/lib/R/site-library\", \"/usr/local/lib/R/library\"))\nDo this for lab 9 and all future labs"
  },
  {
    "objectID": "labs/lab12_soilchem.html#why-store-multiple-data-frames-in-a-single-object-in-r",
    "href": "labs/lab12_soilchem.html#why-store-multiple-data-frames-in-a-single-object-in-r",
    "title": "Lab 12 - Accessing NEON Metadata for Metagenomics",
    "section": "Why Store Multiple Data Frames in a Single Object in R?",
    "text": "Why Store Multiple Data Frames in a Single Object in R?\nWhen working with R, it’s common to handle multiple datasets—whether they represent different time periods, experimental conditions, or subsets of a larger dataset. Instead of scattering these data frames across your environment, storing them in a single object offers several practical advantages. Here’s why this approach is considered a best practice.\n\n1. Better Organization and Readability\nManaging numerous standalone data frames can quickly become messy. By grouping them into a single object—typically a list or a named list—you create a structured container that keeps related data together. This makes your workspace cleaner and your code easier to read.\n\n\n2. Simplifies Iterative Operations\nLists allow you to apply functions across all data frames without repetitive code. Functions like lapply(), purrr::map(), or dplyr pipelines make it easy to automate transformations, summaries, and visualizations.\n\n\n3. Dynamic Handling of Data\nIf you’re importing multiple files or working with an unknown number of datasets, storing them in a single object enables programmatic handling. You can loop through files, read them into data frames, and store them in a list without hardcoding names.\n\n\n4. Facilitates Automation and Reproducibility\nA single object can be saved using saveRDS() or save(), ensuring all related data frames are preserved together. This makes sharing and reproducing analyses straightforward.\n\n\n5. Supports Complex Workflows\nLists can hold not only data frames but also metadata, models, and other objects. This flexibility is useful for hierarchical data structures or multi-step analyses where related components need to stay grouped.\n\n\n6. Reduces Risk of Errors\nWhen data frames are stored in a single object, you avoid naming conflicts and accidental overwriting. It also makes it easier to track which datasets belong to a project."
  },
  {
    "objectID": "labs/lab12_soilchem.html#rds-file-format",
    "href": "labs/lab12_soilchem.html#rds-file-format",
    "title": "Lab 12 - Accessing NEON Metadata for Metagenomics",
    "section": "RDS file format",
    "text": "RDS file format\nRDS files are a format used in R for saving single R objects, allowing for easy storage and retrieval of data. You can save an object using the saveRDS() function and load it back with readRDS(), which helps preserve the object’s structure and type."
  },
  {
    "objectID": "labs/lab12_soilchem.html#understanding-rds-files-in-r",
    "href": "labs/lab12_soilchem.html#understanding-rds-files-in-r",
    "title": "Lab 12 - Accessing NEON Metadata for Metagenomics",
    "section": "Understanding RDS Files in R",
    "text": "Understanding RDS Files in R\nRDS files are a specific format used in R for saving single R objects. This format is beneficial for preserving the structure and attributes of the data, making it ideal for data serialization, the process of translating a data structure or object state into a format that can be stored or transmitted and reconstructed later.\n\nSaving R Objects as RDS Files\nTo save an R object as an RDS file, you can use the saveRDS() function. Here’s how it works:\n\nSyntax:\n\nsaveRDS(object, file = \"filename.rds\")\n\nExample:\n\nsaveRDS(mtcars, \"mtcars.rds\")\nThis command saves the mtcars dataset into a file named mtcars.rds.\n\n\nReading RDS Files\nTo read an RDS file back into R, use the readRDS() function:\n\nSyntax:\n\nobject &lt;- readRDS(file = \"filename.rds\")\n\nExample:\n\nmy_data &lt;- readRDS(\"mtcars.rds\")\nThis command restores the mtcars dataset from the RDS file into the variable my_data. We will use a RDS file to save the NEON data\n\n\nKey Features of RDS File\n\nSingle Object Storage: RDS files are designed to store a single R object, unlike other formats that can store multiple objects.\nCompression: RDS files can be compressed, which helps save disk space.\nPreservation of Structure: When saving data in RDS format, the data structure (like data frames, lists, etc.) and attributes are preserved.\n\n\n\nComparison with Other Formats\n\n\n\n\n\n\n\n\nFeature\nRDS Files\nCSV Files\n\n\n\n\nStores\nSingle R object\nTabular data\n\n\nStructure Preservation\nYes\nNo\n\n\nCompression\nYes\nNo\n\n\nRead/Write Functions\nsaveRDS(), readRDS()\nwrite.csv(), read.csv()\n\n\n\nUsing RDS files is a great way to save and load R objects efficiently while maintaining their integrity."
  },
  {
    "objectID": "labs/lab12_soilchem.html#accessing-neon-data",
    "href": "labs/lab12_soilchem.html#accessing-neon-data",
    "title": "Lab 12 - Accessing NEON Metadata for Metagenomics",
    "section": "Accessing NEON data",
    "text": "Accessing NEON data\nYou do not need to run the code in this section. It is for completeness and reproducibility purposes\n\nInstall NEON packages\n\n\nR code\ninstall.packages(\"neonUtilities\")\ninstall.packages(\"neonOS\")\n\n\n\n\nLoad libraries\n\n\nR code\nlibrary(neonUtilities)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(viridis)\n\n\n\n\nDownload the NEON soil chemistry data\n\n\nR code\nsoilChem &lt;- loadByProduct(\n  dpID='DP1.10086.001',\n  startdate = \"2023-01\",\n  enddate = \"2023-12\",\n  check.size = FALSE,\n  package='expanded')\n\n\nThis is collection of data frames called a list or list of data frames. Because it takes a while to download we will save it. This will also ensure that we are using this version of the soil chemistry data for all future analysis ()\n\n\nR code\n## This is the path for working on Unity\n# saveRDS(\"/work/pi_bio678_umass_edu/data_NEON/soilChem.rds\")\n\n## This is the path for Jeff's personal computer\nsaveRDS(soilChem, file = \"../data/NEON_metadata/soilChem.rds\")"
  },
  {
    "objectID": "labs/lab12_soilchem.html#on-the-computer",
    "href": "labs/lab12_soilchem.html#on-the-computer",
    "title": "Lab 12 - Accessing NEON Metadata for Metagenomics",
    "section": "On the computer",
    "text": "On the computer\nStart by loading in the R data object that was saved above (you do not need to run the above code)\n\nLoad the libraries\n\n\nR code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(viridis)\n\n\n\n\nRead in the NEON soil chemistry data\n\n\nR code\n## This is the path for working on Unity\n# soilChem &lt;- readRDS(\"/work/pi_bio678_umass_edu/data_NEON/soilChem.rds\")\n\n## This is the path for Jeff's personal computer\nsoilChem &lt;- readRDS(\"../data/NEON_metadata/soilChem.rds\")\n\n\nIn the your Environment window in the upper right corner of RStudio click on the soilChem data.\n This is a list of the data frames in this object. You can click on the tabs to see the columns in each data frame.\n\n\nWrangling sample data for metagenomic samples\nNow that you are familiar with downloading and viewing the soilChem data, we can start to focus on linking the data to the metagenomic samples. For soil samples, the DNA samples for metagenomics are combined from up to 3 individual soil samples. This is why you will not see any of the chemistry data we just viewed corresponding to any metagenomic DNA sample directly. We need to access the list of samples that were combined to create a soil composite sample, so we can link these data directly. For this we look at the sls_metagenomicsPooling table.\nIn this table soilChem$sls_metagenomicsPooling the list for each composite DNA sample is in the column genomicsPooledIDList. For example, if we want to look at the list of samples used for the composite sample ‘HARV_013-O-20230704-COMP’ (sample is in the genomicsSampleID field), we can pull up the list for the 11th sample in the table:\n\n\nR code\nsoilChem$sls_metagenomicsPooling[5,'genomicsSampleID']\n\n\n[1] \"HARV_013-O-20230704-COMP\"\n\n\nR code\nsoilChem$sls_metagenomicsPooling[5,'collectDate']\n\n\n[1] \"2023-07-04 19:03:00 UTC\"\n\n\nR code\nsoilChem$sls_metagenomicsPooling$genomicsPooledIDList[5]\n\n\n[1] \"HARV_013-O-39-20230704|HARV_013-O-41-20230704|HARV_013-O-23-20230704\"\n\n\nFrom the list you can see that there are three samples for the composite sample (there can be anywhere from 1-3 samples for each composite), all from plot HARV_013 collected on 7/04/2023, and all from the organic soil horizon (O). Now we can observe the chemical measurements for those specific samples. We will use tidyverse to help get the fields from the table. Let’s take the first one as an example:\n\n\nR code\n# View(soilChem$sls_soilChemistry # No soil chemistry data for HARV\nsoilChem$sls_soilMoisture |&gt; \n  filter(sampleID == \"HARV_013-O-39-20230704\") |&gt; \n  select(sampleID, soilMoisture)\n\n\nWe would like to be able to get these measurements for all the metagenomic subsamples. For this, we will first have to get the list for each composite sample and create a new table.\n\n\nR code\n# split up the pooled list into new columns\ngenomicSamples &lt;- soilChem$sls_metagenomicsPooling |&gt;\n  tidyr::separate(genomicsPooledIDList, into=c(\"first\",\"second\",\"third\"),sep=\"\\\\|\",fill=\"right\") |&gt;\n  dplyr::select(genomicsSampleID,first,second,third)\n\nhead(genomicSamples)\n\n\n          genomicsSampleID                  first                 second\n1 HARV_034-O-20230703-COMP HARV_034-O-23-20230703 HARV_034-O-41-20230703\n2 HARV_033-O-20230703-COMP HARV_033-O-39-20230703 HARV_033-O-21-20230703\n3 HARV_035-O-20230704-COMP HARV_035-O-23-20230704 HARV_035-O-41-20230704\n4 HARV_037-O-20230704-COMP HARV_037-O-41-20230704 HARV_037-O-21-20230704\n5 HARV_013-O-20230704-COMP HARV_013-O-39-20230704 HARV_013-O-41-20230704\n6 HARV_001-O-20230705-COMP HARV_001-O-41-20230705 HARV_001-O-21-20230705\n                   third\n1 HARV_034-O-39-20230703\n2 HARV_033-O-41-20230703\n3 HARV_035-O-21-20230704\n4 HARV_037-O-23-20230704\n5 HARV_013-O-23-20230704\n6 HARV_001-O-23-20230705\n\n\nNow we will adjust the table so that each sampleID is a row, with the genomicsSampleID listed for each sample:\n\n\nR code\ngenSampleExample &lt;- genomicSamples |&gt; \n  tidyr::pivot_longer(cols=c(\"first\",\"second\",\"third\"),values_to = \"sampleID\") |&gt;\n  dplyr::select(sampleID,genomicsSampleID) |&gt;\n  drop_na()\n\n\nNow that you have all samples for each metagenomic sample listed, you can easily combine this table with other tables, using the sampleID. As an example we will go back to the soil chemistry data:\n\n\nR code\nchemEx &lt;- soilChem$sls_soilMoisture |&gt;\n  dplyr::select(sampleID,soilMoisture)\n\n## now combine the tables \ncombinedTab &lt;- left_join(genSampleExample,chemEx, by = \"sampleID\") |&gt; drop_na()\n\n## Show the data table\nhead(combinedTab)\n\n\n# A tibble: 6 × 3\n  sampleID               genomicsSampleID         soilMoisture\n  &lt;chr&gt;                  &lt;chr&gt;                           &lt;dbl&gt;\n1 HARV_034-O-23-20230703 HARV_034-O-20230703-COMP         3.41\n2 HARV_034-O-41-20230703 HARV_034-O-20230703-COMP         2.98\n3 HARV_034-O-39-20230703 HARV_034-O-20230703-COMP         2.70\n4 HARV_033-O-39-20230703 HARV_033-O-20230703-COMP         3.48\n5 HARV_033-O-21-20230703 HARV_033-O-20230703-COMP         2.76\n6 HARV_033-O-41-20230703 HARV_033-O-20230703-COMP         3.31\n\n\n\n\nMerging metadata around composite samples\nWe now have a table that includes the genetic subsamples and their corresponding biogeochemical measurements. However, if we want to compare the biogeochemical data directly with the metagenomic genomic samples, you may want to merge the rows in the table back to a single row for each composite sample. This means we will need to average the chemical measurements across the subsamples for each composite sample. Care must be taken, however, when averaging across certain measurements (see in particular the example for averaging pH, below). Here is a basic example for our table:\n\n\nR code\ngenome_groups_mean &lt;- combinedTab %&gt;%\n  group_by(genomicsSampleID) %&gt;%\n  summarize_at(c(\"soilMoisture\"), mean)\n\nhead(genome_groups_mean)\n\n\n# A tibble: 6 × 2\n  genomicsSampleID         soilMoisture\n  &lt;chr&gt;                           &lt;dbl&gt;\n1 BONA_001-O-20230710-COMP         1.55\n2 BONA_002-O-20230711-COMP         1.14\n3 BONA_004-O-20230712-COMP         3.55\n4 BONA_006-O-20230710-COMP         2.54\n5 BONA_009-O-20230711-COMP         1.93\n6 BONA_013-O-20230710-COMP         2.45"
  },
  {
    "objectID": "labs/lab12_soilchem.html#additional-examples-for-accessing-data",
    "href": "labs/lab12_soilchem.html#additional-examples-for-accessing-data",
    "title": "Lab 12 - Accessing NEON Metadata for Metagenomics",
    "section": "Additional examples for accessing data",
    "text": "Additional examples for accessing data\nHere are some other examples to help you get started with NEON metagenomic data\n\nMerging pH measurements\nThe example above showing how to merge data works for many straightforward measurements, for which calculating the mean is logical. For pH measures, however, this won’t work. Since pH is a logarithmic scale, averaging them will not work. Fortunately, the R package respirometry includes the function mean_pH for averaging pH measurements. This function first converts each pH measure to hydrogen ion concentration [H+], averages the measures, then converts back to the logarithmic scale.\nBelow we show an example with the existing data. First we will create a new table with pH measurements, only keeping the samples from our metagenomic set:\n\n\nR code\nsoilpH_Example &lt;- soilChem$sls_soilpH %&gt;%\n  dplyr::filter(sampleID %in% combinedTab$sampleID) %&gt;%\n  dplyr::select(sampleID,soilInWaterpH,soilInCaClpH)\n\n# now join with the existing table\ncombinedTab_pH &lt;- left_join(combinedTab,soilpH_Example, by = \"sampleID\")\n\n# and the final\nhead(combinedTab_pH)\n\n\n# A tibble: 6 × 5\n  sampleID              genomicsSampleID soilMoisture soilInWaterpH soilInCaClpH\n  &lt;chr&gt;                 &lt;chr&gt;                   &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1 HARV_034-O-23-202307… HARV_034-O-2023…         3.41          3.45         2.7 \n2 HARV_034-O-41-202307… HARV_034-O-2023…         2.98          3.28         2.6 \n3 HARV_034-O-39-202307… HARV_034-O-2023…         2.70          3.34         2.71\n4 HARV_033-O-39-202307… HARV_033-O-2023…         3.48          3.85         3.04\n5 HARV_033-O-21-202307… HARV_033-O-2023…         2.76          3.99         3.29\n6 HARV_033-O-41-202307… HARV_033-O-2023…         3.31          3.63         2.84\n\n\nNow, we can apply the same kind of tidyverse approach as the previous example, only using the mean_pH function:\n\n\nR code\n# Remember to install the respirometry package\nlibrary(respirometry)\n\ngenome_groups_pH &lt;- combinedTab_pH %&gt;%\n  group_by(genomicsSampleID) %&gt;%\n  summarize_at(c(\"soilInWaterpH\",\"soilInCaClpH\"), mean_pH) \n\nhead(genome_groups_pH)\n\n\n# A tibble: 6 × 3\n  genomicsSampleID         soilInWaterpH soilInCaClpH\n  &lt;chr&gt;                            &lt;dbl&gt;        &lt;dbl&gt;\n1 BONA_001-O-20230710-COMP          3.99         3.45\n2 BONA_002-O-20230711-COMP          4.35         3.82\n3 BONA_004-O-20230712-COMP          3.69         3.10\n4 BONA_006-O-20230710-COMP          4.05         3.48\n5 BONA_009-O-20230711-COMP          4.08         3.50\n6 BONA_013-O-20230710-COMP          3.97         3.42\n\n\nOne thing to note with the previous command is that all the other chemical data was lost when you ran the command. In this example we use two summarize_at commands to apply different functions to the two types of variables, and then left_join will combine them:\n\n\nR code\ngenome_groups_all_mean &lt;- combinedTab_pH %&gt;%\n  group_by(genomicsSampleID) %&gt;%\n  {left_join(\n    summarize_at(.,vars(\"soilMoisture\"), mean),\n    summarize_at(.,vars(\"soilInWaterpH\",\"soilInCaClpH\"), mean_pH)\n  )}\n\nhead(genome_groups_all_mean)\n\n\n# A tibble: 6 × 4\n  genomicsSampleID         soilMoisture soilInWaterpH soilInCaClpH\n  &lt;chr&gt;                           &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1 BONA_001-O-20230710-COMP         1.55          3.99         3.45\n2 BONA_002-O-20230711-COMP         1.14          4.35         3.82\n3 BONA_004-O-20230712-COMP         3.55          3.69         3.10\n4 BONA_006-O-20230710-COMP         2.54          4.05         3.48\n5 BONA_009-O-20230711-COMP         1.93          4.08         3.50\n6 BONA_013-O-20230710-COMP         2.45          3.97         3.42"
  },
  {
    "objectID": "labs/lab12_soilchem.html#rds-files",
    "href": "labs/lab12_soilchem.html#rds-files",
    "title": "Lab 12 - Accessing NEON Metadata for Metagenomics",
    "section": "RDS Files",
    "text": "RDS Files\nRDS files are a specific format used in R for saving single R objects. This format is beneficial for preserving the structure and attributes of the data, making it ideal for data serialization, the process of translating a data structure or object state into a format that can be stored or transmitted and reconstructed later.\n\nSaving R Objects as RDS Files\nTo save an R object as an RDS file, you can use the saveRDS() function. Here’s how it works:\n\nSyntax:\n\nsaveRDS(object, file = \"filename.rds\")\n\nExample:\n\nsaveRDS(mtcars, \"mtcars.rds\")\nThis command saves the mtcars dataset into a file named mtcars.rds.\n\n\nReading RDS Files\nTo read an RDS file back into R, use the readRDS() function:\n\nSyntax:\n\nobject &lt;- readRDS(file = \"filename.rds\")\n\nExample:\n\nmy_data &lt;- readRDS(\"mtcars.rds\")\nThis command restores the mtcars dataset from the RDS file into the variable my_data. We will use a RDS file to save the NEON data\n\n\nKey Features of RDS File\n\nSingle Object Storage: RDS files are designed to store a single R object, unlike other formats that can store multiple objects.\nCompression: RDS files can be compressed, which helps save disk space.\nPreservation of Structure: When saving data in RDS format, the data structure (like data frames, lists, etc.) and attributes are preserved.\n\n\n\nComparison with Other Formats\n\n\n\n\n\n\n\n\nFeature\nRDS Files\nCSV Files\n\n\n\n\nStores\nSingle R object\nTabular data\n\n\nStructure Preservation\nYes\nNo\n\n\nCompression\nYes\nNo\n\n\nRead/Write Functions\nsaveRDS(), readRDS()\nwrite.csv(), read.csv()\n\n\n\nUsing RDS files is a great way to save and load R objects efficiently while maintaining their integrity."
  },
  {
    "objectID": "labs/lab12_soilchem.html#exercises",
    "href": "labs/lab12_soilchem.html#exercises",
    "title": "Lab 12 - Accessing NEON Metadata for Metagenomics",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1\nThe dataframe sls_soilCoreCollection contains alot of useful information for analyzing the NEON MAGs. Create a new dataframe coreCollectionSubset with just the following columns\n\nsampleID\ndecimalLatitude\ndecimalLongitude\nelevation\nsoilTemp\n\n\n\nExercise 2\nFollowing the above examples with chemEx join genSampleExample and coreCollectionSubset.\n\n\nExercise 3\nGet the mean for each mean for each metagenome genomicsSampleID for each column in your data frame from Ex. 2.\n\n\nExercise 4\nJoin the data frame from Ex. 3 with the soil moisture and pH measurments in genome_groups_all_mean.\n\n\nExercise 5\nUsing the genomicsSampleID column, make a new column with the siteID (e.g. HARV). There is a similar example in lab 6 where we parsed the NEON MAG data.\n\n\nExercise 6\nMake a box plot with the soilpH by siteID\n\n\nExercise 7\nCalculate the mean soilTemp and soilMoisture for each siteID. Make a graph of mean soilTemp vs soilMoisture\n\n\nExercise 8\nJoin your data frame from Ex 4 with the NEON_MAGs_soil data frame from lab 6.\n\n\nExercise 9\nMake a table of the MAGs found in soils with a pH &lt; 4.0."
  },
  {
    "objectID": "labs/lab6s_neon_multi_key.html",
    "href": "labs/lab6s_neon_multi_key.html",
    "title": "Lab S6 - AI assisted analysis of the NEON MAG taxonomy, soil chemistry and location data",
    "section": "",
    "text": "What can we do using an AI prompt?"
  },
  {
    "objectID": "labs/lab6s_neon_multi_key.html#learning-objectives",
    "href": "labs/lab6s_neon_multi_key.html#learning-objectives",
    "title": "Lab S6 - AI assisted analysis of the NEON MAG taxonomy, soil chemistry and location data",
    "section": "",
    "text": "What can we do using an AI prompt?"
  },
  {
    "objectID": "labs/lab6s_neon_multi_key.html#on-the-computer",
    "href": "labs/lab6s_neon_multi_key.html#on-the-computer",
    "title": "Lab S6 - AI assisted analysis of the NEON MAG taxonomy, soil chemistry and location data",
    "section": "On the computer",
    "text": "On the computer\n\nLoad the libraries\n\n\nR code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(viridis)\nlibrary(janitor)\nlibrary(plotly)\nlibrary(respirometry)\n\n\n\nReading and cleaning freshwater and soil MAGs\n\n\nR code\n# This is the location used for Github\nNEON_MAGs_prelim &lt;- read_tsv(\"../data/NEON_metadata/exported_img_bins_Gs0166454_NEON.tsv\") |&gt; \n# This is the location used for the class data directory on Unity\n# NEON_MAGs_prelim &lt;- read_tsv(\"/work/pi_bio678_umass_edu/data_NEON/exported_img_bins_Gs0166454_NEON.tsv\") |&gt;\n  \n  clean_names() |&gt; \n  \n  # Add a new column community corresponding to different communities names in the genome_name\n  mutate(community = case_when(\n    str_detect(genome_name, \"Freshwater sediment microbial communities\") ~ \"Freshwater sediment microbial communitie\",\n    str_detect(genome_name, \"Freshwater biofilm microbial communities\") ~ \"Freshwater biofilm microbial communities\",\n    str_detect(genome_name, \"Freshwater microbial communities\") ~ \"Freshwater microbial communities\",\n    str_detect(genome_name, \"Soil microbial communities\") ~ \"Soil microbial communities\",\n    TRUE ~ NA_character_\n  )) |&gt; \n  \n  # Create a column type that is either Freshwater or Soil\n  mutate(type = case_when(\n    str_detect(genome_name, \"Freshwater sediment microbial communities\") ~ \"Freshwater\",\n    str_detect(genome_name, \"Freshwater biofilm microbial communities\") ~ \"Freshwater\",\n    str_detect(genome_name, \"Freshwater microbial communities\") ~ \"Freshwater\",\n    str_detect(genome_name, \"Soil microbial communities\") ~ \"Soil\",\n    TRUE ~ NA_character_\n  )) |&gt; \n  \n  # Get rid of the communities strings\n  mutate_at(\"genome_name\", str_replace, \"Freshwater sediment microbial communities from \", \"\") |&gt; \n  mutate_at(\"genome_name\", str_replace, \"Freshwater biofilm microbial communities from\", \"\") |&gt; \n  mutate_at(\"genome_name\", str_replace, \"Freshwater microbial communities from \", \"\") |&gt; \n  mutate_at(\"genome_name\", str_replace, \"Soil microbial communities from \", \"\") |&gt; \n\n  # separate site from sample name \n  separate(genome_name, c(\"site\",\"sample_name\"), \" - \") |&gt;   \n  \n  # Deal with these unknow fields in the sample name by creating a new column and removing them from the sample name\n  mutate(sample_unknown = case_when(\n    str_detect(sample_name, \".SS.\") ~ \"SS\",\n    str_detect(sample_name, \".C0.\") ~ \"C0\",\n    str_detect(sample_name, \".C1.\") ~ \"C1\",\n    str_detect(sample_name, \".C2.\") ~ \"C2\",\n    TRUE ~ NA_character_\n  )) |&gt; \n\n# These fields are all associated with \"Freshwater microbial communities from...   \n# SS - near stream sensor\n# C0 - non-stratified lake/river surface near buoy\n# C1 - stratified lake surface/epilimnion near buoy\n# C2 - stratified lake hypolimnion near buoy\n# EPIPSAMMON  - biofilm on sand/silt\n# EPILITHON - biofilm on rocks/cobbles\n# EPIPHYTON - biofilm that grows on the stems and leaves of aquatic plants\n  \n# These fields are all associated with \"Freshwater biofilm microbial communities from\n# EPILITHON - biofilm on rocks/cobbles\n  \n# These fields are all associated with \"Freshwater sediment microbial communities from \n# EPIPSAMMON - biofilm on sand/silt\n  \n  mutate_at(\"sample_name\", str_replace, \".SS\", \"\") |&gt; \n  mutate_at(\"sample_name\", str_replace, \".C0\", \"\") |&gt; \n  mutate_at(\"sample_name\", str_replace, \".C1\", \"\") |&gt; \n  mutate_at(\"sample_name\", str_replace, \".C2\", \"\") |&gt; \n  \n\n  # Get rid of the the common strings at the end of sample names\n  mutate_at(\"sample_name\", str_replace, \"-GEN-DNA1\", \"\") |&gt;  \n  mutate_at(\"sample_name\", str_replace, \"-COMP-DNA1\", \"\") |&gt;  \n  mutate_at(\"sample_name\", str_replace, \"-COMP-DNA2\", \"\") |&gt;  \n  mutate_at(\"sample_name\", str_replace, \".DNA-DNA1\", \"\") |&gt;\n  mutate_at(\"sample_name\", str_replace, \"_v2\", \"\") |&gt;\n  mutate_at(\"sample_name\", str_replace, \" \\\\(version 2\\\\)\", \"\") |&gt;\n  mutate_at(\"sample_name\", str_replace, \" \\\\(version 3\\\\)\", \"\") |&gt;\n  \n# Separate out the taxonomy groups\n  separate(gtdb_taxonomy_lineage, c(\"domain\", \"phylum\", \"class\", \"order\", \"family\", \"genus\"), \"; \", remove = FALSE)\n\n\n\n\nR code\n# This block is for separating out the freshwater sample names\nNEON_MAGs_freshwater &lt;- NEON_MAGs_prelim |&gt;\n  filter(type == \"Freshwater\") |&gt;\n  # separate the Sample Name into Site ID and plot info\n  separate(sample_name, c(\"site_ID\",\"date\", \"layer\", \"subplot\"), \"\\\\.\", remove = FALSE) |&gt;\n  mutate(quadrant = NA_character_)\n\n\n\n\nR code\n# This block is for separating out the soil sample names\nNEON_MAGs_soil &lt;- NEON_MAGs_prelim |&gt;\n  filter(type == \"Soil\") |&gt;\n  # separate the Sample Name into Site ID and plot info\n  separate(sample_name, c(\"site_ID\",\"subplot.layer.date\"), \"_\", remove = FALSE,) |&gt; \n  # some sample names have 3 fields while others have a fourth field for the quadrant. This code create a field for the quadrant when present and adds na for samples from combined cores.\n  extract(\n    subplot.layer.date,\n    into = c(\"subplot\", \"layer\", \"quadrant\", \"date\"),\n    regex = \"^([^-]+)-([^-]+)(?:-([^-]+))?-([^-]+)$\",\n    remove = FALSE\n  ) |&gt;\n  mutate(quadrant = na_if(quadrant, \"\")) |&gt;\n  select(-subplot.layer.date)\n\n\n\n\nR code\n# This combines the soil and freshwater data frames\nNEON_MAGs &lt;-bind_rows(NEON_MAGs_freshwater, NEON_MAGs_soil) |&gt;\n  # This changes the format of the data column\n  mutate(date = ymd(date))\n\n\n\n\nmySoilChem\n\n\nR code\n# Read in the soilChem data object\n## This is the path for working on Unity\n# soilChem &lt;- readRDS(\"/work/pi_bio678_umass_edu/data_NEON/soilChem.rds\")\n\n## This is the path for Jeff's personal computer\nsoilChem &lt;- readRDS(\"../data/NEON_metadata/soilChem.rds\")\n\n\n\n\nR code\n# split up the pooled list into new columns\ngenomicSamples &lt;- soilChem$sls_metagenomicsPooling |&gt;\n  tidyr::separate(genomicsPooledIDList, into=c(\"first\",\"second\",\"third\"),sep=\"\\\\|\",fill=\"right\") |&gt;\n  dplyr::select(genomicsSampleID,first,second,third)\n\nhead(genomicSamples)\n\n\n          genomicsSampleID                  first                 second\n1 HARV_034-O-20230703-COMP HARV_034-O-23-20230703 HARV_034-O-41-20230703\n2 HARV_033-O-20230703-COMP HARV_033-O-39-20230703 HARV_033-O-21-20230703\n3 HARV_035-O-20230704-COMP HARV_035-O-23-20230704 HARV_035-O-41-20230704\n4 HARV_037-O-20230704-COMP HARV_037-O-41-20230704 HARV_037-O-21-20230704\n5 HARV_013-O-20230704-COMP HARV_013-O-39-20230704 HARV_013-O-41-20230704\n6 HARV_001-O-20230705-COMP HARV_001-O-41-20230705 HARV_001-O-21-20230705\n                   third\n1 HARV_034-O-39-20230703\n2 HARV_033-O-41-20230703\n3 HARV_035-O-21-20230704\n4 HARV_037-O-23-20230704\n5 HARV_013-O-23-20230704\n6 HARV_001-O-23-20230705\n\n\nNow we will adjust the table so that each sampleID is a row, with the genomicsSampleID listed for each sample:\n\n\nR code\ngenSampleExample &lt;- genomicSamples |&gt; \n  tidyr::pivot_longer(cols=c(\"first\",\"second\",\"third\"),values_to = \"sampleID\") |&gt;\n  dplyr::select(sampleID,genomicsSampleID) |&gt;\n  drop_na()\n\n\n\n\nR code\nchemEx &lt;- soilChem$sls_soilMoisture |&gt;\n  dplyr::select(sampleID,soilMoisture)\n\n## now combine the tables \ncombinedTab_soilMoisture &lt;- left_join(genSampleExample,chemEx, by = \"sampleID\") |&gt; drop_na()\n\n\n\n\nR code\ngenome_groups_mean &lt;- combinedTab_soilMoisture %&gt;%\n  group_by(genomicsSampleID) %&gt;%\n  summarize_at(c(\"soilMoisture\"), mean)\n\n\n\n\nR code\nsoilpH_Example &lt;- soilChem$sls_soilpH %&gt;%\n  dplyr::filter(sampleID %in% combinedTab_soilMoisture$sampleID) %&gt;%\n  dplyr::select(sampleID,soilInWaterpH,soilInCaClpH)\n\n# now join with the existing table\ncombinedTab_pH &lt;- left_join(combinedTab_soilMoisture, soilpH_Example, by = \"sampleID\")\n\n\n\n\nR code\n# Remember to install the respirometry package\n\ngenome_groups_pH &lt;- combinedTab_pH %&gt;%\n  group_by(genomicsSampleID) %&gt;%\n  summarize_at(c(\"soilInWaterpH\",\"soilInCaClpH\"), mean_pH) \n\n\n\n\nR code\ngenome_groups_all_mean &lt;- combinedTab_pH %&gt;%\n  group_by(genomicsSampleID) %&gt;%\n  {left_join(\n    summarize_at(.,vars(\"soilMoisture\"), mean),\n    summarize_at(.,vars(\"soilInWaterpH\",\"soilInCaClpH\"), mean_pH)\n  )}\n\n\n\n\nR code\ngenSampleExample &lt;- genomicSamples |&gt; \n  tidyr::pivot_longer(cols=c(\"first\",\"second\",\"third\"),values_to = \"sampleID\") |&gt;\n  dplyr::select(sampleID,genomicsSampleID) |&gt;\n  drop_na()\n\n\n\n\nR code\nsoilCoreCollection &lt;- soilChem$sls_soilCoreCollection |&gt;\n  dplyr::select(sampleID, decimalLatitude, decimalLongitude, elevation, soilTemp)\n\n## now combine the tables \ncombinedTab_soilCoreCollection &lt;- left_join(genSampleExample,soilCoreCollection, by = \"sampleID\") |&gt; drop_na()\n\n\n\n\nR code\n# now join with the existing table\ncombinedTab &lt;- combinedTab_pH |&gt; \n  select(- genomicsSampleID) |&gt; \n  left_join(combinedTab_soilCoreCollection, by = \"sampleID\")\n\n\n\n\nR code\nmySoilChem_mean &lt;- combinedTab %&gt;%\n  group_by(genomicsSampleID) %&gt;%\n  {left_join(\n    summarize_at(.,vars(\"soilMoisture\", \"decimalLatitude\", \"decimalLongitude\", \"elevation\", \"soilTemp\"), mean),\n    summarize_at(.,vars(\"soilInWaterpH\",\"soilInCaClpH\"), mean_pH)\n  )}\n\n\n\n\nJoin Soil NEON MAGs and mySoilChem_mean\n\n\nR code\nNEON_soilMAGs_soilChem &lt;- mySoilChem_mean |&gt; \n  mutate_at(\"genomicsSampleID\", str_replace, \"-COMP\", \"\") |&gt;  \n  left_join(NEON_MAGs_soil, c(\"genomicsSampleID\" = \"sample_name\")) |&gt; \n  select(- sample_unknown)"
  },
  {
    "objectID": "labs/lab6s_neon_multi_key.html#exercises",
    "href": "labs/lab6s_neon_multi_key.html#exercises",
    "title": "Lab S6 - AI assisted analysis of the NEON MAG taxonomy, soil chemistry and location data",
    "section": "Exercises",
    "text": "Exercises\nTurn in typical lab report with your code and results.\n\nExercise 1 - Preparing your NEON_soilMAGs_soilChem file\nWe are going to start with the data frame that you created in Lab 12 ex8 by joining the genome_groups_all_means and NEON_MAGs_soil dataframes. For consistency I am going to call this NEON_soilMAGs_soilChem. Write this data frame to a file\n\n\nR code\nwrite.csv(NEON_soilMAGs_soilChem,file = \"NEON_soilMAGs_soilChem.csv\")\n\n\n\n\nExercise 2 - Upload this file to co-pilot (you may need to download to your computer first).\nGive co-pilot a prompt such as - This is a csv file of metagenome assembled genomes, with their taxonomy, geographic location, soil chemistry and other metadata. What type of analysis can I do with this dataset in R?\n\n\nExercise 3\nThis is an open ended lab. Spend at the class time today try to do either Taxonomic Composition or Environmental Correlations. We will return to this next Monday after the break."
  },
  {
    "objectID": "labs/lab6s_neon_multi.html#on-the-computer",
    "href": "labs/lab6s_neon_multi.html#on-the-computer",
    "title": "Lab S6 - AI assisted analysis of the NEON MAG taxonomy, soil chemistry and location data - Part I",
    "section": "On the computer",
    "text": "On the computer\n\nLoad the libraries\n\n\nR code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(viridis)\nlibrary(janitor)\nlibrary(plotly)\nlibrary(respirometry)"
  },
  {
    "objectID": "labs/lab6s_neon_multi.html#exercises",
    "href": "labs/lab6s_neon_multi.html#exercises",
    "title": "Lab S6 - AI assisted analysis of the NEON MAG taxonomy, soil chemistry and location data - Part I",
    "section": "Exercises",
    "text": "Exercises\nTurn in typical lab report with your code and results.\n\nExercise 1 - Preparing your NEON_soilMAGs_soilChem file\nWe are going to start with the data frame that you created in Lab 12 ex8 by joining the genome_groups_all_means and NEON_MAGs_soil dataframes. For consistency I am going to call this NEON_soilMAGs_soilChem. Write this data frame to a file\n\n\nR code\nwrite.csv(NEON_soilMAGs_soilChem,file = \"NEON_soilMAGs_soilChem.csv\")\n\n\n\n\nExercise 2 - Upload this file to co-pilot (you may need to download to your computer first).\nGive co-pilot a prompt such as - This is a csv file of metagenome assembled genomes, with their taxonomy, geographic location, soil chemistry and other metadata. What type of analysis can I do with this dataset in R?\n\n\nExercise 3\nThis is an open ended lab. Spend at the class time today try to do either Taxonomic Composition or Environmental Correlations. We will return to this next Monday after the break."
  },
  {
    "objectID": "labs/lab7s_neon_ai_assist.html#on-the-computer",
    "href": "labs/lab7s_neon_ai_assist.html#on-the-computer",
    "title": "Lab S7 - AI assisted analysis of the NEON MAG taxonomy, soil chemistry and location data - Part II",
    "section": "On the computer",
    "text": "On the computer\n\nLoad the libraries\n\n\nR code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(viridis)\nlibrary(janitor)\nlibrary(plotly)\nlibrary(respirometry)"
  },
  {
    "objectID": "labs/lab7s_neon_ai_assist.html#exercises",
    "href": "labs/lab7s_neon_ai_assist.html#exercises",
    "title": "Lab S7 - AI assisted analysis of the NEON MAG taxonomy, soil chemistry and location data - Part II",
    "section": "Exercises",
    "text": "Exercises\nToday we will build on the self-motivated analysis of the NEON MAGs and their associated metadata (including the soil chemistry data). As with last week upload your NEON_soilMAGs_soilChem.csv file into co-pilot. Then ask for suggestions in the types of analysis you can do. This week pick a different category than last week. Some possibilies are\n\nCommunity Composition & Diversity\n\n\nTaxonomic profiling: Summarize relative abundances of phyla, classes, or genera across sites.\nAlpha diversity: Compute metrics like Shannon, Simpson, and richness for each site.\nBeta diversity: Compare community composition between sites using Bray-Curtis or UniFrac distances.\nOrdination: Perform PCA, NMDS, or PCoA to visualize differences in microbial communities.\n\n\nEnvironmental Correlations\n\n\nSoil chemistry vs. taxonomy: Correlate pH, moisture, temperature, and elevation with microbial diversity or specific taxa.\nGradient analysis: Identify taxa enriched along environmental gradients (e.g., moisture or pH).\nCanonical Correspondence Analysis (CCA): Link community composition to environmental variables.\n\n\nBiogeography & Spatial Patterns\n\n\nMapping taxa distributions: Visualize where specific taxa or functional groups occur across latitude/longitude.\nSpatial autocorrelation: Test if microbial communities are structured by geography.\nDistance-decay analysis: Examine how community similarity decreases with geographic distance.\n\n\nCo-occurrence & Network Analysis\n\n\nMicrobial interaction networks: Build co-occurrence networks to identify potential ecological interactions.\nModule detection: Identify clusters of taxa that respond similarly to environmental factors.\n\nAsk co-pilot for guidance and code in doing your analysis. As with last week this is an open ended lab. The expectations are that you spend class time exploring one of these areas and submit a lab report with your code."
  },
  {
    "objectID": "labs/NEON_MAGs_Community_Analysis_By_SiteID.html",
    "href": "labs/NEON_MAGs_Community_Analysis_By_SiteID.html",
    "title": "Comprehensive Community Composition & Diversity Analysis of NEON MAGs by SiteID",
    "section": "",
    "text": "This tutorial aggregates data by siteID instead of sampleID and includes: - Taxonomic profiling - Alpha and beta diversity - Ordination (PCoA, NMDS) - Constrained ordination (CCA, RDA) with soil chemistry - Color-coded biplots and interactive plots - Variance explained and significance tests\n\n\nExplanation: Aggregate MAG counts and soil chemistry by siteID.\n\n\nR code\ndf &lt;- read_csv(\"NEON_soilMAGs_soilChem.csv\")\n\ndf &lt;- df %&gt;%\n  filter(!is.na(genomicsSampleID)) %&gt;%\n  mutate(siteID = site_ID) %&gt;%\n  select(siteID, phylum, soilMoisture, soilTemp, soilInWaterpH, soilInCaClpH)\n\n\n\n\n\nExplanation: Summarize phylum-level counts per site.\n\n\nR code\ntax_profile &lt;- df %&gt;%\n  group_by(siteID, phylum) %&gt;%\n  summarise(count = n(), .groups = \"drop\") %&gt;%\n  mutate(rel_abundance = count / sum(count))\n\nggplot(tax_profile, aes(x = siteID, y = rel_abundance, fill = phylum)) +\n  geom_bar(stat = \"identity\") +\n  theme_minimal() +\n  labs(title = \"Taxonomic Profile by Site\", y = \"Relative Abundance\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), legend.position = \"right\")\n\n\n\n\n\n\n\n\n\n\n\n\nExplanation: Compute diversity metrics at site level.\n\n\nR code\nabundance_matrix &lt;- tax_profile %&gt;%\n  pivot_wider(names_from = phylum, values_from = count, values_fill = 0)\nabundance_matrix &lt;- abundance_matrix %&gt;% distinct(siteID, .keep_all = TRUE)\nabundance_matrix &lt;- abundance_matrix %&gt;% filter(!is.na(siteID))\nrn &lt;- abundance_matrix$siteID\nabundance_matrix &lt;- abundance_matrix %&gt;% select(-siteID)\nabundance_matrix &lt;- as.data.frame(lapply(abundance_matrix, as.numeric))\nrownames(abundance_matrix) &lt;- rn\nabundance_matrix &lt;- abundance_matrix[rowSums(abundance_matrix) &gt; 0, ]\n\nshannon &lt;- diversity(abundance_matrix, index = \"shannon\")\nsimpson &lt;- diversity(abundance_matrix, index = \"simpson\")\nrichness &lt;- specnumber(abundance_matrix)\nalpha_div &lt;- tibble(siteID = rownames(abundance_matrix), Shannon = shannon, Simpson = simpson, Richness = richness)\nalpha_div\n\n\n# A tibble: 27 × 4\n   siteID Shannon  Simpson Richness\n   &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;int&gt;\n 1 BONA   0.00173 0.000359        2\n 2 CLBJ   0.00173 0.000359        2\n 3 CPER   0.00173 0.000359        2\n 4 DCFS   0.00173 0.000359        2\n 5 DEJU   0.694   0.500           3\n 6 DSNY   0.00173 0.000359        2\n 7 GUAN   0.00173 0.000359        2\n 8 HARV   0.00173 0.000359        2\n 9 KONZ   0.00173 0.000359        2\n10 LENO   0.00173 0.000359        2\n# ℹ 17 more rows\n\n\n\n\n\nExplanation: Bray-Curtis dissimilarity between sites.\n\n\nR code\nbray_dist &lt;- vegdist(abundance_matrix, method = \"bray\")\nas.matrix(bray_dist)[1:5, 1:5]\n\n\n           BONA       CLBJ      CPER      DCFS      DEJU\nBONA 0.00000000 0.06930693 0.7419355 0.4210526 0.6223501\nCLBJ 0.06930693 0.00000000 0.7090909 0.3623188 0.6630573\nCPER 0.74193548 0.70909091 0.0000000 0.4666667 0.9333276\nDCFS 0.42105263 0.36231884 0.4666667 0.0000000 0.8267575\nDEJU 0.62235014 0.66305730 0.9333276 0.8267575 0.0000000\n\n\n\n\n\nExplanation: Visualize site-level differences.\n\n\nR code\npcoa_coords &lt;- cmdscale(bray_dist, k = 2)\npcoa_df &lt;- as_tibble(pcoa_coords) %&gt;% mutate(siteID = rownames(pcoa_coords))\n\nggplot(pcoa_df, aes(x = V1, y = V2, label = siteID)) + geom_point() + geom_text(vjust = -0.5) + labs(title = \"PCoA (Bray-Curtis)\")\n\n\n\n\n\n\n\n\n\nR code\nnmds &lt;- metaMDS(abundance_matrix, distance = \"bray\", k = 2)\n\n\nSquare root transformation\nWisconsin double standardization\nRun 0 stress 1.219798e-16 \nRun 1 stress 0 \n... New best solution\n... Procrustes: rmse 0.08431368  max resid 0.2164455 \nRun 2 stress 0.005368253 \nRun 3 stress 3.036322e-05 \n... Procrustes: rmse 0.1075216  max resid 0.1751653 \nRun 4 stress 9.836936e-05 \n... Procrustes: rmse 0.1207031  max resid 0.1831196 \nRun 5 stress 0 \n... Procrustes: rmse 0.1444153  max resid 0.3680746 \nRun 6 stress 8.45748e-05 \n... Procrustes: rmse 0.1232783  max resid 0.3293707 \nRun 7 stress 9.932887e-05 \n... Procrustes: rmse 0.1059297  max resid 0.1652366 \nRun 8 stress 0 \n... Procrustes: rmse 0.1599825  max resid 0.3953014 \nRun 9 stress 0 \n... Procrustes: rmse 0.1540484  max resid 0.4326244 \nRun 10 stress 0.004081708 \nRun 11 stress 0 \n... Procrustes: rmse 0.09780662  max resid 0.1555036 \nRun 12 stress 0 \n... Procrustes: rmse 0.1569152  max resid 0.4531087 \nRun 13 stress 0 \n... Procrustes: rmse 0.1328415  max resid 0.3239635 \nRun 14 stress 0 \n... Procrustes: rmse 0.1280671  max resid 0.2942607 \nRun 15 stress 0.007794782 \nRun 16 stress 0 \n... Procrustes: rmse 0.1082688  max resid 0.2118214 \nRun 17 stress 0 \n... Procrustes: rmse 0.09806647  max resid 0.1613522 \nRun 18 stress 0 \n... Procrustes: rmse 0.08747574  max resid 0.1432149 \nRun 19 stress 0 \n... Procrustes: rmse 0.1318983  max resid 0.2894873 \nRun 20 stress 0 \n... Procrustes: rmse 0.1281027  max resid 0.2842546 \n*** Best solution was not repeated -- monoMDS stopping criteria:\n     3: no. of iterations &gt;= maxit\n    17: stress &lt; smin\n\n\nR code\nnmds_df &lt;- as_tibble(nmds$points) %&gt;% mutate(siteID = rownames(nmds$points))\n\nggplot(nmds_df, aes(x = MDS1, y = MDS2, label = siteID)) + geom_point() + geom_text(vjust = -0.5) + labs(title = \"NMDS (Bray-Curtis)\")\n\n\n\n\n\n\n\n\n\n\n\n\nExplanation: Aggregate soil chemistry by site and visualize environmental influence.\n\n\nR code\nsoil_data &lt;- df %&gt;% group_by(siteID) %&gt;% summarise(across(c(soilMoisture, soilTemp, soilInWaterpH, soilInCaClpH), mean, na.rm = TRUE))\nsoil_data &lt;- soil_data %&gt;% filter(siteID %in% rownames(abundance_matrix))\nsoil_data &lt;- column_to_rownames(soil_data, \"siteID\")\nsoil_data &lt;- as.data.frame(lapply(soil_data, as.numeric))\nrownames(soil_data) &lt;- rownames(abundance_matrix)\n\nvariable_types &lt;- tibble(variable = c(\"soilMoisture\", \"soilTemp\", \"soilInWaterpH\", \"soilInCaClpH\"),\n                         type = c(\"Moisture\", \"Temperature\", \"pH\", \"pH\"))\n\n# CCA\ncca_model &lt;- cca(abundance_matrix ~ ., data = soil_data)\ncca_var &lt;- summary(cca_model)$concont$importance[2,]\ncca_sites &lt;- scores(cca_model, display = \"sites\")\ncca_env &lt;- scores(cca_model, display = \"bp\")\ncca_df &lt;- as.data.frame(cca_sites)\ncca_df$siteID &lt;- rownames(cca_df)\ncca_env_df &lt;- as.data.frame(cca_env)\ncca_env_df$variable &lt;- rownames(cca_env_df)\ncca_env_df &lt;- left_join(cca_env_df, variable_types, by = \"variable\")\n\np_cca &lt;- ggplot() +\n  geom_point(data = cca_df, aes(x = CCA1, y = CCA2, text = siteID), color = \"blue\") +\n  geom_segment(data = cca_env_df, aes(x = 0, y = 0, xend = CCA1, yend = CCA2, color = type, text = variable),\n               arrow = arrow(length = unit(0.3, \"cm\"))) +\n  geom_text(data = cca_env_df, aes(x = CCA1, y = CCA2, label = variable), vjust = -0.5) +\n  scale_color_manual(values = c(\"Moisture\" = \"blue\", \"Temperature\" = \"orange\", \"pH\" = \"green\")) +\n  labs(title = paste(\"CCA Interactive Biplot (Variance Explained: CCA1 =\", round(cca_var[1]*100, 1), \"%, CCA2 =\", round(cca_var[2]*100, 1), \"% )\"),\n       color = \"Variable Type\") + theme_minimal()\n\nggplotly(p_cca, tooltip = c(\"text\"))\n\n\n\n\n\n\nR code\n# RDA\nrda_model &lt;- rda(abundance_matrix ~ ., data = soil_data)\nrda_var &lt;- summary(rda_model)$cont$importance[2,]\nrda_sites &lt;- scores(rda_model, display = \"sites\")\nrda_env &lt;- scores(rda_model, display = \"bp\")\nrda_df &lt;- as.data.frame(rda_sites)\nrda_df$siteID &lt;- rownames(rda_df)\nrda_env_df &lt;- as.data.frame(rda_env)\nrda_env_df$variable &lt;- rownames(rda_env_df)\nrda_env_df &lt;- left_join(rda_env_df, variable_types, by = \"variable\")\n\np_rda &lt;- ggplot() +\n  geom_point(data = rda_df, aes(x = RDA1, y = RDA2, text = siteID), color = \"darkgreen\") +\n  geom_segment(data = rda_env_df, aes(x = 0, y = 0, xend = RDA1, yend = RDA2, color = type, text = variable),\n               arrow = arrow(length = unit(0.3, \"cm\"))) +\n  geom_text(data = rda_env_df, aes(x = RDA1, y = RDA2, label = variable), vjust = -0.5) +\n  scale_color_manual(values = c(\"Moisture\" = \"blue\", \"Temperature\" = \"orange\", \"pH\" = \"green\")) +\n  labs(title = paste(\"RDA Interactive Biplot (Variance Explained: RDA1 =\", round(rda_var[1]*100, 1), \"%, RDA2 =\", round(rda_var[2]*100, 1), \"% )\"),\n       color = \"Variable Type\") + theme_minimal()\n\nggplotly(p_rda, tooltip = c(\"text\"))"
  },
  {
    "objectID": "labs/NEON_MAGs_Community_Analysis_By_SiteID.html#step-1-load-and-prepare-data",
    "href": "labs/NEON_MAGs_Community_Analysis_By_SiteID.html#step-1-load-and-prepare-data",
    "title": "Comprehensive Community Composition & Diversity Analysis of NEON MAGs by SiteID",
    "section": "",
    "text": "Explanation: Aggregate MAG counts and soil chemistry by siteID.\n\n\nR code\ndf &lt;- read_csv(\"NEON_soilMAGs_soilChem.csv\")\n\ndf &lt;- df %&gt;%\n  filter(!is.na(genomicsSampleID)) %&gt;%\n  mutate(siteID = site_ID) %&gt;%\n  select(siteID, phylum, soilMoisture, soilTemp, soilInWaterpH, soilInCaClpH)"
  },
  {
    "objectID": "labs/NEON_MAGs_Community_Analysis_By_SiteID.html#step-2-taxonomic-profiling-by-site",
    "href": "labs/NEON_MAGs_Community_Analysis_By_SiteID.html#step-2-taxonomic-profiling-by-site",
    "title": "Comprehensive Community Composition & Diversity Analysis of NEON MAGs by SiteID",
    "section": "",
    "text": "Explanation: Summarize phylum-level counts per site.\n\n\nR code\ntax_profile &lt;- df %&gt;%\n  group_by(siteID, phylum) %&gt;%\n  summarise(count = n(), .groups = \"drop\") %&gt;%\n  mutate(rel_abundance = count / sum(count))\n\nggplot(tax_profile, aes(x = siteID, y = rel_abundance, fill = phylum)) +\n  geom_bar(stat = \"identity\") +\n  theme_minimal() +\n  labs(title = \"Taxonomic Profile by Site\", y = \"Relative Abundance\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), legend.position = \"right\")"
  },
  {
    "objectID": "labs/NEON_MAGs_Community_Analysis_By_SiteID.html#step-3-alpha-diversity",
    "href": "labs/NEON_MAGs_Community_Analysis_By_SiteID.html#step-3-alpha-diversity",
    "title": "Comprehensive Community Composition & Diversity Analysis of NEON MAGs by SiteID",
    "section": "",
    "text": "Explanation: Compute diversity metrics at site level.\n\n\nR code\nabundance_matrix &lt;- tax_profile %&gt;%\n  pivot_wider(names_from = phylum, values_from = count, values_fill = 0)\nabundance_matrix &lt;- abundance_matrix %&gt;% distinct(siteID, .keep_all = TRUE)\nabundance_matrix &lt;- abundance_matrix %&gt;% filter(!is.na(siteID))\nrn &lt;- abundance_matrix$siteID\nabundance_matrix &lt;- abundance_matrix %&gt;% select(-siteID)\nabundance_matrix &lt;- as.data.frame(lapply(abundance_matrix, as.numeric))\nrownames(abundance_matrix) &lt;- rn\nabundance_matrix &lt;- abundance_matrix[rowSums(abundance_matrix) &gt; 0, ]\n\nshannon &lt;- diversity(abundance_matrix, index = \"shannon\")\nsimpson &lt;- diversity(abundance_matrix, index = \"simpson\")\nrichness &lt;- specnumber(abundance_matrix)\nalpha_div &lt;- tibble(siteID = rownames(abundance_matrix), Shannon = shannon, Simpson = simpson, Richness = richness)\nalpha_div\n\n\n# A tibble: 27 × 4\n   siteID Shannon  Simpson Richness\n   &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;int&gt;\n 1 BONA   0.00173 0.000359        2\n 2 CLBJ   0.00173 0.000359        2\n 3 CPER   0.00173 0.000359        2\n 4 DCFS   0.00173 0.000359        2\n 5 DEJU   0.694   0.500           3\n 6 DSNY   0.00173 0.000359        2\n 7 GUAN   0.00173 0.000359        2\n 8 HARV   0.00173 0.000359        2\n 9 KONZ   0.00173 0.000359        2\n10 LENO   0.00173 0.000359        2\n# ℹ 17 more rows"
  },
  {
    "objectID": "labs/NEON_MAGs_Community_Analysis_By_SiteID.html#step-4-beta-diversity",
    "href": "labs/NEON_MAGs_Community_Analysis_By_SiteID.html#step-4-beta-diversity",
    "title": "Comprehensive Community Composition & Diversity Analysis of NEON MAGs by SiteID",
    "section": "",
    "text": "Explanation: Bray-Curtis dissimilarity between sites.\n\n\nR code\nbray_dist &lt;- vegdist(abundance_matrix, method = \"bray\")\nas.matrix(bray_dist)[1:5, 1:5]\n\n\n           BONA       CLBJ      CPER      DCFS      DEJU\nBONA 0.00000000 0.06930693 0.7419355 0.4210526 0.6223501\nCLBJ 0.06930693 0.00000000 0.7090909 0.3623188 0.6630573\nCPER 0.74193548 0.70909091 0.0000000 0.4666667 0.9333276\nDCFS 0.42105263 0.36231884 0.4666667 0.0000000 0.8267575\nDEJU 0.62235014 0.66305730 0.9333276 0.8267575 0.0000000"
  },
  {
    "objectID": "labs/NEON_MAGs_Community_Analysis_By_SiteID.html#step-5-ordination-pcoa-and-nmds",
    "href": "labs/NEON_MAGs_Community_Analysis_By_SiteID.html#step-5-ordination-pcoa-and-nmds",
    "title": "Comprehensive Community Composition & Diversity Analysis of NEON MAGs by SiteID",
    "section": "",
    "text": "Explanation: Visualize site-level differences.\n\n\nR code\npcoa_coords &lt;- cmdscale(bray_dist, k = 2)\npcoa_df &lt;- as_tibble(pcoa_coords) %&gt;% mutate(siteID = rownames(pcoa_coords))\n\nggplot(pcoa_df, aes(x = V1, y = V2, label = siteID)) + geom_point() + geom_text(vjust = -0.5) + labs(title = \"PCoA (Bray-Curtis)\")\n\n\n\n\n\n\n\n\n\nR code\nnmds &lt;- metaMDS(abundance_matrix, distance = \"bray\", k = 2)\n\n\nSquare root transformation\nWisconsin double standardization\nRun 0 stress 1.219798e-16 \nRun 1 stress 0 \n... New best solution\n... Procrustes: rmse 0.08431368  max resid 0.2164455 \nRun 2 stress 0.005368253 \nRun 3 stress 3.036322e-05 \n... Procrustes: rmse 0.1075216  max resid 0.1751653 \nRun 4 stress 9.836936e-05 \n... Procrustes: rmse 0.1207031  max resid 0.1831196 \nRun 5 stress 0 \n... Procrustes: rmse 0.1444153  max resid 0.3680746 \nRun 6 stress 8.45748e-05 \n... Procrustes: rmse 0.1232783  max resid 0.3293707 \nRun 7 stress 9.932887e-05 \n... Procrustes: rmse 0.1059297  max resid 0.1652366 \nRun 8 stress 0 \n... Procrustes: rmse 0.1599825  max resid 0.3953014 \nRun 9 stress 0 \n... Procrustes: rmse 0.1540484  max resid 0.4326244 \nRun 10 stress 0.004081708 \nRun 11 stress 0 \n... Procrustes: rmse 0.09780662  max resid 0.1555036 \nRun 12 stress 0 \n... Procrustes: rmse 0.1569152  max resid 0.4531087 \nRun 13 stress 0 \n... Procrustes: rmse 0.1328415  max resid 0.3239635 \nRun 14 stress 0 \n... Procrustes: rmse 0.1280671  max resid 0.2942607 \nRun 15 stress 0.007794782 \nRun 16 stress 0 \n... Procrustes: rmse 0.1082688  max resid 0.2118214 \nRun 17 stress 0 \n... Procrustes: rmse 0.09806647  max resid 0.1613522 \nRun 18 stress 0 \n... Procrustes: rmse 0.08747574  max resid 0.1432149 \nRun 19 stress 0 \n... Procrustes: rmse 0.1318983  max resid 0.2894873 \nRun 20 stress 0 \n... Procrustes: rmse 0.1281027  max resid 0.2842546 \n*** Best solution was not repeated -- monoMDS stopping criteria:\n     3: no. of iterations &gt;= maxit\n    17: stress &lt; smin\n\n\nR code\nnmds_df &lt;- as_tibble(nmds$points) %&gt;% mutate(siteID = rownames(nmds$points))\n\nggplot(nmds_df, aes(x = MDS1, y = MDS2, label = siteID)) + geom_point() + geom_text(vjust = -0.5) + labs(title = \"NMDS (Bray-Curtis)\")"
  },
  {
    "objectID": "labs/NEON_MAGs_Community_Analysis_By_SiteID.html#step-6-soil-chemistry-integration-for-cca-and-rda-with-interactive-color-coded-biplots",
    "href": "labs/NEON_MAGs_Community_Analysis_By_SiteID.html#step-6-soil-chemistry-integration-for-cca-and-rda-with-interactive-color-coded-biplots",
    "title": "Comprehensive Community Composition & Diversity Analysis of NEON MAGs by SiteID",
    "section": "",
    "text": "Explanation: Aggregate soil chemistry by site and visualize environmental influence.\n\n\nR code\nsoil_data &lt;- df %&gt;% group_by(siteID) %&gt;% summarise(across(c(soilMoisture, soilTemp, soilInWaterpH, soilInCaClpH), mean, na.rm = TRUE))\nsoil_data &lt;- soil_data %&gt;% filter(siteID %in% rownames(abundance_matrix))\nsoil_data &lt;- column_to_rownames(soil_data, \"siteID\")\nsoil_data &lt;- as.data.frame(lapply(soil_data, as.numeric))\nrownames(soil_data) &lt;- rownames(abundance_matrix)\n\nvariable_types &lt;- tibble(variable = c(\"soilMoisture\", \"soilTemp\", \"soilInWaterpH\", \"soilInCaClpH\"),\n                         type = c(\"Moisture\", \"Temperature\", \"pH\", \"pH\"))\n\n# CCA\ncca_model &lt;- cca(abundance_matrix ~ ., data = soil_data)\ncca_var &lt;- summary(cca_model)$concont$importance[2,]\ncca_sites &lt;- scores(cca_model, display = \"sites\")\ncca_env &lt;- scores(cca_model, display = \"bp\")\ncca_df &lt;- as.data.frame(cca_sites)\ncca_df$siteID &lt;- rownames(cca_df)\ncca_env_df &lt;- as.data.frame(cca_env)\ncca_env_df$variable &lt;- rownames(cca_env_df)\ncca_env_df &lt;- left_join(cca_env_df, variable_types, by = \"variable\")\n\np_cca &lt;- ggplot() +\n  geom_point(data = cca_df, aes(x = CCA1, y = CCA2, text = siteID), color = \"blue\") +\n  geom_segment(data = cca_env_df, aes(x = 0, y = 0, xend = CCA1, yend = CCA2, color = type, text = variable),\n               arrow = arrow(length = unit(0.3, \"cm\"))) +\n  geom_text(data = cca_env_df, aes(x = CCA1, y = CCA2, label = variable), vjust = -0.5) +\n  scale_color_manual(values = c(\"Moisture\" = \"blue\", \"Temperature\" = \"orange\", \"pH\" = \"green\")) +\n  labs(title = paste(\"CCA Interactive Biplot (Variance Explained: CCA1 =\", round(cca_var[1]*100, 1), \"%, CCA2 =\", round(cca_var[2]*100, 1), \"% )\"),\n       color = \"Variable Type\") + theme_minimal()\n\nggplotly(p_cca, tooltip = c(\"text\"))\n\n\n\n\n\n\nR code\n# RDA\nrda_model &lt;- rda(abundance_matrix ~ ., data = soil_data)\nrda_var &lt;- summary(rda_model)$cont$importance[2,]\nrda_sites &lt;- scores(rda_model, display = \"sites\")\nrda_env &lt;- scores(rda_model, display = \"bp\")\nrda_df &lt;- as.data.frame(rda_sites)\nrda_df$siteID &lt;- rownames(rda_df)\nrda_env_df &lt;- as.data.frame(rda_env)\nrda_env_df$variable &lt;- rownames(rda_env_df)\nrda_env_df &lt;- left_join(rda_env_df, variable_types, by = \"variable\")\n\np_rda &lt;- ggplot() +\n  geom_point(data = rda_df, aes(x = RDA1, y = RDA2, text = siteID), color = \"darkgreen\") +\n  geom_segment(data = rda_env_df, aes(x = 0, y = 0, xend = RDA1, yend = RDA2, color = type, text = variable),\n               arrow = arrow(length = unit(0.3, \"cm\"))) +\n  geom_text(data = rda_env_df, aes(x = RDA1, y = RDA2, label = variable), vjust = -0.5) +\n  scale_color_manual(values = c(\"Moisture\" = \"blue\", \"Temperature\" = \"orange\", \"pH\" = \"green\")) +\n  labs(title = paste(\"RDA Interactive Biplot (Variance Explained: RDA1 =\", round(rda_var[1]*100, 1), \"%, RDA2 =\", round(rda_var[2]*100, 1), \"% )\"),\n       color = \"Variable Type\") + theme_minimal()\n\nggplotly(p_rda, tooltip = c(\"text\"))"
  },
  {
    "objectID": "labs/microbial_network_analysis_comprehensive.html",
    "href": "labs/microbial_network_analysis_comprehensive.html",
    "title": "Comprehensive Microbial Co-occurrence and Network Analysis",
    "section": "",
    "text": "This comprehensive report includes: - Data preprocessing and normalization. - Static network visualization. - Interactive network visualization with taxonomy tooltips and module color coding. - Module detection visualization. - Interactive soil chemistry correlation heatmap.\n\n\n\n\nR code\nlibrary(tidyverse)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(tidygraph)\nlibrary(readr)\nlibrary(visNetwork)\nlibrary(plotly)"
  },
  {
    "objectID": "labs/microbial_network_analysis_comprehensive.html#required-packages",
    "href": "labs/microbial_network_analysis_comprehensive.html#required-packages",
    "title": "Comprehensive Microbial Co-occurrence and Network Analysis",
    "section": "",
    "text": "R code\nlibrary(tidyverse)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(tidygraph)\nlibrary(readr)\nlibrary(visNetwork)\nlibrary(plotly)"
  },
  {
    "objectID": "labs/NEON_MAGs_Environmental_Correlations.html",
    "href": "labs/NEON_MAGs_Environmental_Correlations.html",
    "title": "Environmental Correlations in Microbial Communities (NEON MAGs)",
    "section": "",
    "text": "This tutorial explores environmental correlations in microbial communities using NEON MAGs data: - Correlate soil chemistry (pH, moisture, temperature, elevation) with microbial diversity and taxa. - Perform gradient analysis to identify taxa enriched along environmental gradients. - Use Canonical Correspondence Analysis (CCA) to link community composition to environmental variables.\n\n\n\n\nR code\ndf &lt;- read_csv(\"NEON_soilMAGs_soilChem.csv\")\n\n# Select relevant columns\ndf &lt;- df %&gt;%\n  filter(!is.na(genomicsSampleID)) %&gt;%\n  mutate(siteID = site_ID,\n         sampleID = genomicsSampleID) %&gt;%\n  select(sampleID, siteID, phylum, soilMoisture, soilTemp, soilInWaterpH, soilInCaClpH, elevation)\n\n\n\n\n\n\n\nR code\ntax_profile &lt;- df %&gt;%\n  group_by(sampleID, phylum) %&gt;%\n  summarise(count = n(), .groups = \"drop\") %&gt;%\n  pivot_wider(names_from = phylum, values_from = count, values_fill = 0)\n\n# Merge with environmental data\nenv_data &lt;- df %&gt;% select(sampleID, soilMoisture, soilTemp, soilInWaterpH, soilInCaClpH, elevation) %&gt;% distinct()\nmerged_data &lt;- left_join(tax_profile, env_data, by = \"sampleID\")\n\n\n\n\n\nExplanation: Correlate soil variables with diversity metrics and selected taxa.\n\n\nR code\n# Compute diversity metrics\nabundance_matrix &lt;- merged_data %&gt;% select(-sampleID, -soilMoisture, -soilTemp, -soilInWaterpH, -soilInCaClpH, -elevation)\nshannon &lt;- diversity(abundance_matrix, index = \"shannon\")\nsimpson &lt;- diversity(abundance_matrix, index = \"simpson\")\nrichness &lt;- specnumber(abundance_matrix)\n\n# Combine with environmental data\ncorr_df &lt;- merged_data %&gt;% select(sampleID, soilMoisture, soilTemp, soilInWaterpH, soilInCaClpH, elevation) %&gt;%\n  mutate(Shannon = shannon, Simpson = simpson, Richness = richness)\n\n# Correlation plot\nggplot(corr_df, aes(x = soilMoisture, y = Shannon)) +\n  geom_point() + geom_smooth(method = \"lm\") +\n  labs(title = \"Correlation: Soil Moisture vs Shannon Diversity\")\n\n\n\n\n\n\n\n\n\n\n\n\nExplanation: Identify taxa enriched along pH or moisture gradients.\n\n\nR code\n# Example: Correlation of Acidobacteriota abundance with pH\ntarget_taxon &lt;- \"Acidobacteriota\"\nmerged_data %&gt;%\n  ggplot(aes(x = soilInWaterpH, y = !!sym(target_taxon))) +\n  geom_point() + geom_smooth(method = \"lm\") +\n  labs(title = paste(\"Gradient Analysis: \", target_taxon, \" vs pH\"))\n\n\n\n\n\n\n\n\n\n\n\n\nExplanation: Link community composition to environmental variables.\n\n\nR code\n# Prepare matrices\nabundance_matrix &lt;- merged_data %&gt;% select(-sampleID, -soilMoisture, -soilTemp, -soilInWaterpH, -soilInCaClpH, -elevation)\nenv_matrix &lt;- merged_data %&gt;% select(soilMoisture, soilTemp, soilInWaterpH, soilInCaClpH, elevation)\n\n# Perform CCA\ncca_model &lt;- cca(abundance_matrix ~ ., data = env_matrix)\nsummary(cca_model)\n\n\n\nCall:\ncca(formula = abundance_matrix ~ soilMoisture + soilTemp + soilInWaterpH +      soilInCaClpH + elevation, data = env_matrix) \n\nPartitioning of scaled Chi-square:\n              Inertia Proportion\nTotal          2.6009     1.0000\nConstrained    0.3828     0.1472\nUnconstrained  2.2182     0.8528\n\nEigenvalues, and their contribution to the scaled Chi-square \n\nImportance of components:\n                         CCA1    CCA2    CCA3     CCA4     CCA5    CA1     CA2\nEigenvalue            0.19931 0.12342 0.04250 0.010087 0.007432 0.6531 0.15254\nProportion Explained  0.07663 0.04745 0.01634 0.003878 0.002857 0.2511 0.05865\nCumulative Proportion 0.07663 0.12409 0.14043 0.144304 0.147161 0.3983 0.45690\n                          CA3     CA4     CA5     CA6     CA7     CA8     CA9\nEigenvalue            0.13116 0.10967 0.10706 0.09812 0.08784 0.07540 0.06901\nProportion Explained  0.05043 0.04217 0.04116 0.03773 0.03377 0.02899 0.02653\nCumulative Proportion 0.50733 0.54949 0.59065 0.62838 0.66215 0.69114 0.71767\n                         CA10    CA11    CA12    CA13    CA14    CA15    CA16\nEigenvalue            0.06175 0.05868 0.05130 0.04824 0.04689 0.04333 0.04170\nProportion Explained  0.02374 0.02256 0.01973 0.01855 0.01803 0.01666 0.01603\nCumulative Proportion 0.74141 0.76398 0.78370 0.80225 0.82028 0.83694 0.85297\n                         CA17    CA18    CA19    CA20    CA21    CA22    CA23\nEigenvalue            0.03881 0.03710 0.03476 0.03293 0.03150 0.02997 0.02699\nProportion Explained  0.01492 0.01426 0.01336 0.01266 0.01211 0.01152 0.01038\nCumulative Proportion 0.86789 0.88216 0.89552 0.90818 0.92029 0.93182 0.94219\n                         CA24     CA25     CA26     CA27     CA28     CA29\nEigenvalue            0.02606 0.024659 0.022182 0.016794 0.015542 0.013283\nProportion Explained  0.01002 0.009481 0.008529 0.006457 0.005976 0.005107\nCumulative Proportion 0.95221 0.961693 0.970222 0.976679 0.982654 0.987762\n                          CA30     CA31     CA32     CA33      CA34\nEigenvalue            0.011622 0.009967 0.005175 0.003759 0.0013079\nProportion Explained  0.004469 0.003832 0.001990 0.001445 0.0005029\nCumulative Proportion 0.992230 0.996062 0.998052 0.999497 1.0000000\n\nAccumulated constrained eigenvalues\nImportance of components:\n                        CCA1   CCA2   CCA3    CCA4     CCA5\nEigenvalue            0.1993 0.1234 0.0425 0.01009 0.007432\nProportion Explained  0.5207 0.3225 0.1110 0.02635 0.019416\nCumulative Proportion 0.5207 0.8432 0.9542 0.98058 1.000000\n\n\nR code\n# Interactive CCA plot\ncca_sites &lt;- scores(cca_model, display = \"sites\")\ncca_env &lt;- scores(cca_model, display = \"bp\")\ncca_df &lt;- as.data.frame(cca_sites)\ncca_df$sampleID &lt;- rownames(cca_df)\ncca_env_df &lt;- as.data.frame(cca_env)\ncca_env_df$variable &lt;- rownames(cca_env_df)\n\np_cca &lt;- ggplot() +\n  geom_point(data = cca_df, aes(x = CCA1, y = CCA2, text = sampleID), color = \"blue\") +\n  geom_segment(data = cca_env_df, aes(x = 0, y = 0, xend = CCA1, yend = CCA2, text = variable),\n               arrow = arrow(length = unit(0.3, \"cm\")), color = \"red\") +\n  geom_text(data = cca_env_df, aes(x = CCA1, y = CCA2, label = variable), vjust = -0.5) +\n  labs(title = \"CCA: Community vs Environment\") + theme_minimal()\n\nggplotly(p_cca, tooltip = c(\"text\"))"
  },
  {
    "objectID": "labs/NEON_MAGs_Environmental_Correlations.html#step-1-load-and-prepare-data",
    "href": "labs/NEON_MAGs_Environmental_Correlations.html#step-1-load-and-prepare-data",
    "title": "Environmental Correlations in Microbial Communities (NEON MAGs)",
    "section": "",
    "text": "R code\ndf &lt;- read_csv(\"NEON_soilMAGs_soilChem.csv\")\n\n# Select relevant columns\ndf &lt;- df %&gt;%\n  filter(!is.na(genomicsSampleID)) %&gt;%\n  mutate(siteID = site_ID,\n         sampleID = genomicsSampleID) %&gt;%\n  select(sampleID, siteID, phylum, soilMoisture, soilTemp, soilInWaterpH, soilInCaClpH, elevation)"
  },
  {
    "objectID": "labs/NEON_MAGs_Environmental_Correlations.html#step-2-aggregate-data-by-sample",
    "href": "labs/NEON_MAGs_Environmental_Correlations.html#step-2-aggregate-data-by-sample",
    "title": "Environmental Correlations in Microbial Communities (NEON MAGs)",
    "section": "",
    "text": "R code\ntax_profile &lt;- df %&gt;%\n  group_by(sampleID, phylum) %&gt;%\n  summarise(count = n(), .groups = \"drop\") %&gt;%\n  pivot_wider(names_from = phylum, values_from = count, values_fill = 0)\n\n# Merge with environmental data\nenv_data &lt;- df %&gt;% select(sampleID, soilMoisture, soilTemp, soilInWaterpH, soilInCaClpH, elevation) %&gt;% distinct()\nmerged_data &lt;- left_join(tax_profile, env_data, by = \"sampleID\")"
  },
  {
    "objectID": "labs/NEON_MAGs_Environmental_Correlations.html#step-3-correlation-analysis",
    "href": "labs/NEON_MAGs_Environmental_Correlations.html#step-3-correlation-analysis",
    "title": "Environmental Correlations in Microbial Communities (NEON MAGs)",
    "section": "",
    "text": "Explanation: Correlate soil variables with diversity metrics and selected taxa.\n\n\nR code\n# Compute diversity metrics\nabundance_matrix &lt;- merged_data %&gt;% select(-sampleID, -soilMoisture, -soilTemp, -soilInWaterpH, -soilInCaClpH, -elevation)\nshannon &lt;- diversity(abundance_matrix, index = \"shannon\")\nsimpson &lt;- diversity(abundance_matrix, index = \"simpson\")\nrichness &lt;- specnumber(abundance_matrix)\n\n# Combine with environmental data\ncorr_df &lt;- merged_data %&gt;% select(sampleID, soilMoisture, soilTemp, soilInWaterpH, soilInCaClpH, elevation) %&gt;%\n  mutate(Shannon = shannon, Simpson = simpson, Richness = richness)\n\n# Correlation plot\nggplot(corr_df, aes(x = soilMoisture, y = Shannon)) +\n  geom_point() + geom_smooth(method = \"lm\") +\n  labs(title = \"Correlation: Soil Moisture vs Shannon Diversity\")"
  },
  {
    "objectID": "labs/NEON_MAGs_Environmental_Correlations.html#step-4-gradient-analysis",
    "href": "labs/NEON_MAGs_Environmental_Correlations.html#step-4-gradient-analysis",
    "title": "Environmental Correlations in Microbial Communities (NEON MAGs)",
    "section": "",
    "text": "Explanation: Identify taxa enriched along pH or moisture gradients.\n\n\nR code\n# Example: Correlation of Acidobacteriota abundance with pH\ntarget_taxon &lt;- \"Acidobacteriota\"\nmerged_data %&gt;%\n  ggplot(aes(x = soilInWaterpH, y = !!sym(target_taxon))) +\n  geom_point() + geom_smooth(method = \"lm\") +\n  labs(title = paste(\"Gradient Analysis: \", target_taxon, \" vs pH\"))"
  },
  {
    "objectID": "labs/NEON_MAGs_Environmental_Correlations.html#step-5-canonical-correspondence-analysis-cca",
    "href": "labs/NEON_MAGs_Environmental_Correlations.html#step-5-canonical-correspondence-analysis-cca",
    "title": "Environmental Correlations in Microbial Communities (NEON MAGs)",
    "section": "",
    "text": "Explanation: Link community composition to environmental variables.\n\n\nR code\n# Prepare matrices\nabundance_matrix &lt;- merged_data %&gt;% select(-sampleID, -soilMoisture, -soilTemp, -soilInWaterpH, -soilInCaClpH, -elevation)\nenv_matrix &lt;- merged_data %&gt;% select(soilMoisture, soilTemp, soilInWaterpH, soilInCaClpH, elevation)\n\n# Perform CCA\ncca_model &lt;- cca(abundance_matrix ~ ., data = env_matrix)\nsummary(cca_model)\n\n\n\nCall:\ncca(formula = abundance_matrix ~ soilMoisture + soilTemp + soilInWaterpH +      soilInCaClpH + elevation, data = env_matrix) \n\nPartitioning of scaled Chi-square:\n              Inertia Proportion\nTotal          2.6009     1.0000\nConstrained    0.3828     0.1472\nUnconstrained  2.2182     0.8528\n\nEigenvalues, and their contribution to the scaled Chi-square \n\nImportance of components:\n                         CCA1    CCA2    CCA3     CCA4     CCA5    CA1     CA2\nEigenvalue            0.19931 0.12342 0.04250 0.010087 0.007432 0.6531 0.15254\nProportion Explained  0.07663 0.04745 0.01634 0.003878 0.002857 0.2511 0.05865\nCumulative Proportion 0.07663 0.12409 0.14043 0.144304 0.147161 0.3983 0.45690\n                          CA3     CA4     CA5     CA6     CA7     CA8     CA9\nEigenvalue            0.13116 0.10967 0.10706 0.09812 0.08784 0.07540 0.06901\nProportion Explained  0.05043 0.04217 0.04116 0.03773 0.03377 0.02899 0.02653\nCumulative Proportion 0.50733 0.54949 0.59065 0.62838 0.66215 0.69114 0.71767\n                         CA10    CA11    CA12    CA13    CA14    CA15    CA16\nEigenvalue            0.06175 0.05868 0.05130 0.04824 0.04689 0.04333 0.04170\nProportion Explained  0.02374 0.02256 0.01973 0.01855 0.01803 0.01666 0.01603\nCumulative Proportion 0.74141 0.76398 0.78370 0.80225 0.82028 0.83694 0.85297\n                         CA17    CA18    CA19    CA20    CA21    CA22    CA23\nEigenvalue            0.03881 0.03710 0.03476 0.03293 0.03150 0.02997 0.02699\nProportion Explained  0.01492 0.01426 0.01336 0.01266 0.01211 0.01152 0.01038\nCumulative Proportion 0.86789 0.88216 0.89552 0.90818 0.92029 0.93182 0.94219\n                         CA24     CA25     CA26     CA27     CA28     CA29\nEigenvalue            0.02606 0.024659 0.022182 0.016794 0.015542 0.013283\nProportion Explained  0.01002 0.009481 0.008529 0.006457 0.005976 0.005107\nCumulative Proportion 0.95221 0.961693 0.970222 0.976679 0.982654 0.987762\n                          CA30     CA31     CA32     CA33      CA34\nEigenvalue            0.011622 0.009967 0.005175 0.003759 0.0013079\nProportion Explained  0.004469 0.003832 0.001990 0.001445 0.0005029\nCumulative Proportion 0.992230 0.996062 0.998052 0.999497 1.0000000\n\nAccumulated constrained eigenvalues\nImportance of components:\n                        CCA1   CCA2   CCA3    CCA4     CCA5\nEigenvalue            0.1993 0.1234 0.0425 0.01009 0.007432\nProportion Explained  0.5207 0.3225 0.1110 0.02635 0.019416\nCumulative Proportion 0.5207 0.8432 0.9542 0.98058 1.000000\n\n\nR code\n# Interactive CCA plot\ncca_sites &lt;- scores(cca_model, display = \"sites\")\ncca_env &lt;- scores(cca_model, display = \"bp\")\ncca_df &lt;- as.data.frame(cca_sites)\ncca_df$sampleID &lt;- rownames(cca_df)\ncca_env_df &lt;- as.data.frame(cca_env)\ncca_env_df$variable &lt;- rownames(cca_env_df)\n\np_cca &lt;- ggplot() +\n  geom_point(data = cca_df, aes(x = CCA1, y = CCA2, text = sampleID), color = \"blue\") +\n  geom_segment(data = cca_env_df, aes(x = 0, y = 0, xend = CCA1, yend = CCA2, text = variable),\n               arrow = arrow(length = unit(0.3, \"cm\")), color = \"red\") +\n  geom_text(data = cca_env_df, aes(x = CCA1, y = CCA2, label = variable), vjust = -0.5) +\n  labs(title = \"CCA: Community vs Environment\") + theme_minimal()\n\nggplotly(p_cca, tooltip = c(\"text\"))"
  },
  {
    "objectID": "labs/NEON_MAGs_Biogeography_Comprehensive.html",
    "href": "labs/NEON_MAGs_Biogeography_Comprehensive.html",
    "title": "Comprehensive Biogeography & Spatial Patterns in Microbial Communities (NEON MAGs)",
    "section": "",
    "text": "This comprehensive tutorial combines: - Static and interactive maps of taxa distributions. - Spatial autocorrelation using Mantel test. - Distance-decay analysis with significance tests and interactive visualization.\n\n\n\n\nR code\ndf &lt;- read_csv(\"NEON_soilMAGs_soilChem.csv\")\n\ndf &lt;- df %&gt;%\n  filter(!is.na(decimalLatitude), !is.na(decimalLongitude)) %&gt;%\n  mutate(siteID = site_ID,\n         sampleID = genomicsSampleID) %&gt;%\n  select(sampleID, siteID, decimalLatitude, decimalLongitude, phylum)\n\n\n\n\n\nExplanation: Visualize where specific taxa occur using ggplot2.\n\n\nR code\nacidobacteria &lt;- df %&gt;% filter(phylum == \"Acidobacteriota\")\n\nggplot(acidobacteria, aes(x = decimalLongitude, y = decimalLatitude)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  labs(title = \"Distribution of Acidobacteriota\", x = \"Longitude\", y = \"Latitude\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nExplanation: Explore taxa distributions interactively.\n\n\nR code\nleaflet(acidobacteria) %&gt;%\n  addTiles() %&gt;%\n  addCircleMarkers(~decimalLongitude, ~decimalLatitude,\n                   popup = ~paste(\"Site:\", siteID),\n                   color = \"blue\", radius = 5, fillOpacity = 0.7) %&gt;%\n  addLegend(\"bottomright\", colors = \"blue\", labels = \"Acidobacteriota\", title = \"Taxon\")\n\n\n\n\n\n\n\n\n\nExplanation: Test if microbial communities are spatially structured.\n\n\nR code\nabundance &lt;- df %&gt;%\n  group_by(sampleID, phylum) %&gt;%\n  summarise(count = n(), .groups = \"drop\") %&gt;%\n  pivot_wider(names_from = phylum, values_from = count, values_fill = 0)\n\ncoords &lt;- df %&gt;% select(sampleID, decimalLatitude, decimalLongitude) %&gt;% distinct()\ngeo_dist &lt;- distm(coords %&gt;% select(decimalLongitude, decimalLatitude), fun = distHaversine)\nrownames(geo_dist) &lt;- coords$sampleID\ncolnames(geo_dist) &lt;- coords$sampleID\n\nabundance_matrix &lt;- abundance %&gt;% select(-sampleID)\nbray_dist &lt;- vegdist(abundance_matrix, method = \"bray\")\n\nmantel_result &lt;- mantel(bray_dist, as.dist(geo_dist))\nmantel_result\n\n\n\nMantel statistic based on Pearson's product-moment correlation \n\nCall:\nmantel(xdis = bray_dist, ydis = as.dist(geo_dist)) \n\nMantel statistic r: 0.008293 \n      Significance: 0.36 \n\nUpper quantiles of permutations (null model):\n   90%    95%  97.5%    99% \n0.0328 0.0432 0.0531 0.0711 \nPermutation: free\nNumber of permutations: 999"
  },
  {
    "objectID": "labs/NEON_MAGs_Biogeography_Comprehensive.html#step-1-load-and-prepare-data",
    "href": "labs/NEON_MAGs_Biogeography_Comprehensive.html#step-1-load-and-prepare-data",
    "title": "Comprehensive Biogeography & Spatial Patterns in Microbial Communities (NEON MAGs)",
    "section": "",
    "text": "R code\ndf &lt;- read_csv(\"NEON_soilMAGs_soilChem.csv\")\n\ndf &lt;- df %&gt;%\n  filter(!is.na(decimalLatitude), !is.na(decimalLongitude)) %&gt;%\n  mutate(siteID = site_ID,\n         sampleID = genomicsSampleID) %&gt;%\n  select(sampleID, siteID, decimalLatitude, decimalLongitude, phylum)"
  },
  {
    "objectID": "labs/NEON_MAGs_Biogeography_Comprehensive.html#step-2-static-mapping-of-taxa-distributions",
    "href": "labs/NEON_MAGs_Biogeography_Comprehensive.html#step-2-static-mapping-of-taxa-distributions",
    "title": "Comprehensive Biogeography & Spatial Patterns in Microbial Communities (NEON MAGs)",
    "section": "",
    "text": "Explanation: Visualize where specific taxa occur using ggplot2.\n\n\nR code\nacidobacteria &lt;- df %&gt;% filter(phylum == \"Acidobacteriota\")\n\nggplot(acidobacteria, aes(x = decimalLongitude, y = decimalLatitude)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  labs(title = \"Distribution of Acidobacteriota\", x = \"Longitude\", y = \"Latitude\") +\n  theme_minimal()"
  },
  {
    "objectID": "labs/NEON_MAGs_Biogeography_Comprehensive.html#step-3-interactive-mapping-with-leaflet",
    "href": "labs/NEON_MAGs_Biogeography_Comprehensive.html#step-3-interactive-mapping-with-leaflet",
    "title": "Comprehensive Biogeography & Spatial Patterns in Microbial Communities (NEON MAGs)",
    "section": "",
    "text": "Explanation: Explore taxa distributions interactively.\n\n\nR code\nleaflet(acidobacteria) %&gt;%\n  addTiles() %&gt;%\n  addCircleMarkers(~decimalLongitude, ~decimalLatitude,\n                   popup = ~paste(\"Site:\", siteID),\n                   color = \"blue\", radius = 5, fillOpacity = 0.7) %&gt;%\n  addLegend(\"bottomright\", colors = \"blue\", labels = \"Acidobacteriota\", title = \"Taxon\")"
  },
  {
    "objectID": "labs/NEON_MAGs_Biogeography_Comprehensive.html#step-4-spatial-autocorrelation-mantel-test",
    "href": "labs/NEON_MAGs_Biogeography_Comprehensive.html#step-4-spatial-autocorrelation-mantel-test",
    "title": "Comprehensive Biogeography & Spatial Patterns in Microbial Communities (NEON MAGs)",
    "section": "",
    "text": "Explanation: Test if microbial communities are spatially structured.\n\n\nR code\nabundance &lt;- df %&gt;%\n  group_by(sampleID, phylum) %&gt;%\n  summarise(count = n(), .groups = \"drop\") %&gt;%\n  pivot_wider(names_from = phylum, values_from = count, values_fill = 0)\n\ncoords &lt;- df %&gt;% select(sampleID, decimalLatitude, decimalLongitude) %&gt;% distinct()\ngeo_dist &lt;- distm(coords %&gt;% select(decimalLongitude, decimalLatitude), fun = distHaversine)\nrownames(geo_dist) &lt;- coords$sampleID\ncolnames(geo_dist) &lt;- coords$sampleID\n\nabundance_matrix &lt;- abundance %&gt;% select(-sampleID)\nbray_dist &lt;- vegdist(abundance_matrix, method = \"bray\")\n\nmantel_result &lt;- mantel(bray_dist, as.dist(geo_dist))\nmantel_result\n\n\n\nMantel statistic based on Pearson's product-moment correlation \n\nCall:\nmantel(xdis = bray_dist, ydis = as.dist(geo_dist)) \n\nMantel statistic r: 0.008293 \n      Significance: 0.36 \n\nUpper quantiles of permutations (null model):\n   90%    95%  97.5%    99% \n0.0328 0.0432 0.0531 0.0711 \nPermutation: free\nNumber of permutations: 999"
  },
  {
    "objectID": "labs/NEON_MAGs_microbial_network_analysis_comprehensive.html",
    "href": "labs/NEON_MAGs_microbial_network_analysis_comprehensive.html",
    "title": "Comprehensive Microbial Co-occurrence and Network Analysis",
    "section": "",
    "text": "This comprehensive report includes: - Data preprocessing and normalization. - Static network visualization. - Interactive network visualization with taxonomy tooltips and module color coding. - Module detection visualization. - Interactive soil chemistry correlation heatmap.\n\n\n\n\nR code\nlibrary(tidyverse)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(tidygraph)\nlibrary(readr)\nlibrary(visNetwork)\nlibrary(plotly)"
  },
  {
    "objectID": "labs/NEON_MAGs_microbial_network_analysis_comprehensive.html#required-packages",
    "href": "labs/NEON_MAGs_microbial_network_analysis_comprehensive.html#required-packages",
    "title": "Comprehensive Microbial Co-occurrence and Network Analysis",
    "section": "",
    "text": "R code\nlibrary(tidyverse)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(tidygraph)\nlibrary(readr)\nlibrary(visNetwork)\nlibrary(plotly)"
  },
  {
    "objectID": "labs/lab13_ai_soilChemMAGs.html",
    "href": "labs/lab13_ai_soilChemMAGs.html",
    "title": "Lab 13 - Creating a detailed report with explanations from the NEON MAG data",
    "section": "",
    "text": "In lab 7 we asked you to load NEON_soilMAGs_soilChem.csv file into co-pilot. Then ask for suggestions in the types of analysis you can do. Here were some possibilies co-pilot gave me.\n\nCommunity Composition & Diversity\n\n\nTaxonomic profiling: Summarize relative abundances of phyla, classes, or genera across sites.\nAlpha diversity: Compute metrics like Shannon, Simpson, and richness for each site.\nBeta diversity: Compare community composition between sites using Bray-Curtis or UniFrac distances.\nOrdination: Perform PCA, NMDS, or PCoA to visualize differences in microbial communities.\n\n\nEnvironmental Correlations\n\n\nSoil chemistry vs. taxonomy: Correlate pH, moisture, temperature, and elevation with microbial diversity or specific taxa.\nGradient analysis: Identify taxa enriched along environmental gradients (e.g., moisture or pH).\nCanonical Correspondence Analysis (CCA): Link community composition to environmental variables.\n\n\nBiogeography & Spatial Patterns\n\n\nMapping taxa distributions: Visualize where specific taxa or functional groups occur across latitude/longitude.\nSpatial autocorrelation: Test if microbial communities are structured by geography.\nDistance-decay analysis: Examine how community similarity decreases with geographic distance.\n\n\nCo-occurrence & Network Analysis\n\n\nMicrobial interaction networks: Build co-occurrence networks to identify potential ecological interactions.\nModule detection: Identify clusters of taxa that respond similarly to environmental factors.\n\nAsk co-pilot for guidance and code in doing your analysis. As with last week this is an open ended lab. The expectations are that you spend class time exploring one of these areas and submit a lab report with your code.\nHere is an example co-pilot prompt that could be used to start getting results and graphs related to this categories\nThis is a table Metagenome assembled genomes (MAGS) from metagenomes from sites that are part of the National Ecological Observatory Network. The table includes information on the MAGs including their taxonomy and quality measures. It also includes soil chemistry data associated with each metagenome. At each site there are 1 or more subplots. The string in the column genomesSampleID includes the siteID, the subplot number, the soil layer (O or M) and the date collected. Could you create a tutorial in a R markdown notebook including plots and explanations using Tidverse and other packages to understand Community Composition & Diversity including\n\nTaxonomic profiling: Summarize relative abundances of phyla, classes, or genera across sites.\nAlpha diversity: Compute metrics like Shannon, Simpson, and richness for each site.\nBeta diversity: Compare community composition between sites using Bray-Curtis or UniFrac distances.\nOrdination: Perform PCA, NMDS, or PCoA to visualize differences in microbial communities.\n\nAfter several iterations including error fixes, the following results were obtained for each category within 30 minutes.\n\nCommunity Composition & Diversity\nEnvironmental Correlations\nBiogeography & Spatial Patterns\nCo-occurrence & Network Analysis\n\n\nThere are some important limitations to our data set to keep in mind for these analyses. We have only a list of the MAGs, not their abundance at a particular site. To get the abundance we would need to map the reads from the unassembled metagenome to get a count of the number of reads from each MAG. This is a proxy for the number of counts for each genome. In the above code the counts are coming from the number of different taxa in each category."
  },
  {
    "objectID": "labs/lab13_ai_soilChemMAGs.html#overview",
    "href": "labs/lab13_ai_soilChemMAGs.html#overview",
    "title": "Lab 13 - Creating a detailed report with explanations from the NEON MAG data",
    "section": "",
    "text": "In lab 7 we asked you to load NEON_soilMAGs_soilChem.csv file into co-pilot. Then ask for suggestions in the types of analysis you can do. Here were some possibilies co-pilot gave me.\n\nCommunity Composition & Diversity\n\n\nTaxonomic profiling: Summarize relative abundances of phyla, classes, or genera across sites.\nAlpha diversity: Compute metrics like Shannon, Simpson, and richness for each site.\nBeta diversity: Compare community composition between sites using Bray-Curtis or UniFrac distances.\nOrdination: Perform PCA, NMDS, or PCoA to visualize differences in microbial communities.\n\n\nEnvironmental Correlations\n\n\nSoil chemistry vs. taxonomy: Correlate pH, moisture, temperature, and elevation with microbial diversity or specific taxa.\nGradient analysis: Identify taxa enriched along environmental gradients (e.g., moisture or pH).\nCanonical Correspondence Analysis (CCA): Link community composition to environmental variables.\n\n\nBiogeography & Spatial Patterns\n\n\nMapping taxa distributions: Visualize where specific taxa or functional groups occur across latitude/longitude.\nSpatial autocorrelation: Test if microbial communities are structured by geography.\nDistance-decay analysis: Examine how community similarity decreases with geographic distance.\n\n\nCo-occurrence & Network Analysis\n\n\nMicrobial interaction networks: Build co-occurrence networks to identify potential ecological interactions.\nModule detection: Identify clusters of taxa that respond similarly to environmental factors.\n\nAsk co-pilot for guidance and code in doing your analysis. As with last week this is an open ended lab. The expectations are that you spend class time exploring one of these areas and submit a lab report with your code.\nHere is an example co-pilot prompt that could be used to start getting results and graphs related to this categories\nThis is a table Metagenome assembled genomes (MAGS) from metagenomes from sites that are part of the National Ecological Observatory Network. The table includes information on the MAGs including their taxonomy and quality measures. It also includes soil chemistry data associated with each metagenome. At each site there are 1 or more subplots. The string in the column genomesSampleID includes the siteID, the subplot number, the soil layer (O or M) and the date collected. Could you create a tutorial in a R markdown notebook including plots and explanations using Tidverse and other packages to understand Community Composition & Diversity including\n\nTaxonomic profiling: Summarize relative abundances of phyla, classes, or genera across sites.\nAlpha diversity: Compute metrics like Shannon, Simpson, and richness for each site.\nBeta diversity: Compare community composition between sites using Bray-Curtis or UniFrac distances.\nOrdination: Perform PCA, NMDS, or PCoA to visualize differences in microbial communities.\n\nAfter several iterations including error fixes, the following results were obtained for each category within 30 minutes.\n\nCommunity Composition & Diversity\nEnvironmental Correlations\nBiogeography & Spatial Patterns\nCo-occurrence & Network Analysis\n\n\nThere are some important limitations to our data set to keep in mind for these analyses. We have only a list of the MAGs, not their abundance at a particular site. To get the abundance we would need to map the reads from the unassembled metagenome to get a count of the number of reads from each MAG. This is a proxy for the number of counts for each genome. In the above code the counts are coming from the number of different taxa in each category."
  },
  {
    "objectID": "labs/NEON_MAGs_Community_Analysis_Final_Comprehensive.html",
    "href": "labs/NEON_MAGs_Community_Analysis_Final_Comprehensive.html",
    "title": "Comprehensive Community Composition & Diversity Analysis of NEON MAGs with Soil Chemistry",
    "section": "",
    "text": "This comprehensive tutorial combines: - Taxonomic Profiling - Alpha Diversity (Shannon, Simpson, Richness) - Beta Diversity (Bray-Curtis) - Ordination (PCoA, NMDS) - Constrained Ordination (CCA, RDA) with soil chemistry - Color-coded biplots for environmental variables - Interactive plots using Plotly - Variance explained and significance tests\nEach section includes explanations for interpretation.\n\n\nExplanation: Extract sample identifiers and soil chemistry variables.\n\n\nR code\ndf &lt;- read_csv(\"NEON_soilMAGs_soilChem.csv\")\n\ndf &lt;- df %&gt;%\n  filter(!is.na(genomicsSampleID)) %&gt;%\n  mutate(siteID = str_extract(genomicsSampleID, \"^[A-Z]+\"),\n         subplot = str_extract(genomicsSampleID, \"(?&lt;=_)[0-9]+\"),\n         layer = str_extract(genomicsSampleID, \"(?&lt;=-)[OM]\"),\n         sampleID = paste(siteID, subplot, layer, sep = \"_\")) %&gt;%\n  select(sampleID, phylum, soilMoisture, soilTemp, soilInWaterpH, soilInCaClpH)\n\n\n\n\n\nExplanation: Shows relative abundance of phyla per sample.\n\n\nR code\ntax_profile &lt;- df %&gt;%\n  group_by(sampleID, phylum) %&gt;%\n  summarise(count = n(), .groups = \"drop\") %&gt;%\n  mutate(rel_abundance = count / sum(count))\n\nggplot(tax_profile, aes(x = sampleID, y = rel_abundance, fill = phylum)) +\n  geom_bar(stat = \"identity\") +\n  theme_minimal() +\n  labs(title = \"Taxonomic Profile by Sample\", y = \"Relative Abundance\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), legend.position = \"right\")\n\n\n\n\n\n\n\n\n\n\n\n\nExplanation: Shannon = richness + evenness, Simpson = dominance, Richness = number of taxa.\n\n\nR code\nabundance_matrix &lt;- tax_profile %&gt;%\n  pivot_wider(names_from = phylum, values_from = count, values_fill = 0)\nabundance_matrix &lt;- abundance_matrix %&gt;% distinct(sampleID, .keep_all = TRUE)\nrn &lt;- abundance_matrix$sampleID\nabundance_matrix &lt;- abundance_matrix %&gt;% select(-sampleID)\nabundance_matrix &lt;- as.data.frame(lapply(abundance_matrix, as.numeric))\nrownames(abundance_matrix) &lt;- rn\nabundance_matrix &lt;- abundance_matrix[rowSums(abundance_matrix) &gt; 0, ]\n\nshannon &lt;- diversity(abundance_matrix, index = \"shannon\")\nsimpson &lt;- diversity(abundance_matrix, index = \"simpson\")\nrichness &lt;- specnumber(abundance_matrix)\nalpha_div &lt;- tibble(sampleID = rownames(abundance_matrix), Shannon = shannon, Simpson = simpson, Richness = richness)\nalpha_div\n\n\n# A tibble: 292 × 4\n   sampleID   Shannon  Simpson Richness\n   &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;int&gt;\n 1 BONA_001_O 0.00173 0.000359        2\n 2 BONA_002_O 0.694   0.500           3\n 3 BONA_004_O 0.00173 0.000359        2\n 4 BONA_006_O 0.694   0.500           3\n 5 BONA_009_O 1.10    0.667           4\n 6 BONA_013_O 0.00173 0.000359        2\n 7 BONA_070_O 0.694   0.500           3\n 8 BONA_071_O 0.00173 0.000359        2\n 9 BONA_080_O 0.00173 0.000359        2\n10 BONA_084_O 0.694   0.500           3\n# ℹ 282 more rows\n\n\n\n\n\nExplanation: Bray-Curtis measures compositional dissimilarity.\n\n\nR code\nbray_dist &lt;- vegdist(abundance_matrix, method = \"bray\")\nas.matrix(bray_dist)[1:5, 1:5]\n\n\n           BONA_001_O BONA_002_O BONA_004_O BONA_006_O BONA_009_O\nBONA_001_O  0.0000000  0.5237892  0.2941176  0.5789315  0.6999838\nBONA_002_O  0.5237892  0.0000000  0.3749650  0.5555157  0.3684121\nBONA_004_O  0.2941176  0.3749650  0.0000000  0.4285421  0.5999713\nBONA_006_O  0.5789315  0.5555157  0.4285421  0.0000000  0.2941065\nBONA_009_O  0.6999838  0.3684121  0.5999713  0.2941065  0.0000000\n\n\n\n\n\nExplanation: Visualize sample relationships in reduced dimensions.\n\n\nR code\npcoa_coords &lt;- cmdscale(bray_dist, k = 2)\npcoa_df &lt;- as_tibble(pcoa_coords) %&gt;% mutate(sampleID = rownames(pcoa_coords))\n\nggplot(pcoa_df, aes(x = V1, y = V2, label = sampleID)) + geom_point() + geom_text(vjust = -0.5) + labs(title = \"PCoA (Bray-Curtis)\")\n\n\n\n\n\n\n\n\n\nR code\nnmds &lt;- metaMDS(abundance_matrix, distance = \"bray\", k = 2)\n\n\nWisconsin double standardization\nRun 0 stress 0.08532757 \nRun 1 stress 0.1200854 \nRun 2 stress 0.08703641 \nRun 3 stress 0.09333725 \nRun 4 stress 0.08902399 \nRun 5 stress 0.1081445 \nRun 6 stress 0.08492605 \n... New best solution\n... Procrustes: rmse 0.01883282  max resid 0.2615482 \nRun 7 stress 0.08791272 \nRun 8 stress 0.1139224 \nRun 9 stress 0.0854949 \nRun 10 stress 0.1158335 \nRun 11 stress 0.119714 \nRun 12 stress 0.09171769 \nRun 13 stress 0.08640987 \nRun 14 stress 0.08262548 \n... New best solution\n... Procrustes: rmse 0.008558615  max resid 0.06322369 \nRun 15 stress 0.08490224 \nRun 16 stress 0.08386775 \nRun 17 stress 0.1077224 \nRun 18 stress 0.08189793 \n... New best solution\n... Procrustes: rmse 0.008302257  max resid 0.0633948 \nRun 19 stress 0.0937349 \nRun 20 stress 0.08654519 \n*** Best solution was not repeated -- monoMDS stopping criteria:\n    10: no. of iterations &gt;= maxit\n     8: stress ratio &gt; sratmax\n     2: scale factor of the gradient &lt; sfgrmin\n\n\nR code\nnmds_df &lt;- as_tibble(nmds$points) %&gt;% mutate(sampleID = rownames(nmds$points))\n\nggplot(nmds_df, aes(x = MDS1, y = MDS2, label = sampleID)) + geom_point() + geom_text(vjust = -0.5) + labs(title = \"NMDS (Bray-Curtis)\")\n\n\n\n\n\n\n\n\n\n\n\n\nExplanation: Arrows show environmental influence; color-coded by variable type.\n\n\nR code\nsoil_data &lt;- df %&gt;% select(sampleID, soilMoisture, soilTemp, soilInWaterpH, soilInCaClpH) %&gt;% distinct(sampleID, .keep_all = TRUE)\nsoil_data &lt;- soil_data %&gt;% filter(sampleID %in% rownames(abundance_matrix))\nsoil_data &lt;- column_to_rownames(soil_data, \"sampleID\")\nsoil_data &lt;- as.data.frame(lapply(soil_data, as.numeric))\nrownames(soil_data) &lt;- rownames(abundance_matrix)\n\nvariable_types &lt;- tibble(variable = c(\"soilMoisture\", \"soilTemp\", \"soilInWaterpH\", \"soilInCaClpH\"),\n                         type = c(\"Moisture\", \"Temperature\", \"pH\", \"pH\"))\n\n# CCA\ncca_model &lt;- cca(abundance_matrix ~ ., data = soil_data)\ncca_var &lt;- summary(cca_model)$concont$importance[2,]\ncca_sites &lt;- scores(cca_model, display = \"sites\")\ncca_env &lt;- scores(cca_model, display = \"bp\")\ncca_df &lt;- as.data.frame(cca_sites)\ncca_df$sampleID &lt;- rownames(cca_df)\ncca_env_df &lt;- as.data.frame(cca_env)\ncca_env_df$variable &lt;- rownames(cca_env_df)\ncca_env_df &lt;- left_join(cca_env_df, variable_types, by = \"variable\")\n\np_cca &lt;- ggplot() +\n  geom_point(data = cca_df, aes(x = CCA1, y = CCA2, text = sampleID), color = \"blue\") +\n  geom_segment(data = cca_env_df, aes(x = 0, y = 0, xend = CCA1, yend = CCA2, color = type, text = variable),\n               arrow = arrow(length = unit(0.3, \"cm\"))) +\n  geom_text(data = cca_env_df, aes(x = CCA1, y = CCA2, label = variable), vjust = -0.5) +\n  scale_color_manual(values = c(\"Moisture\" = \"blue\", \"Temperature\" = \"orange\", \"pH\" = \"green\")) +\n  labs(title = paste(\"CCA Interactive Biplot (Variance Explained: CCA1 =\", round(cca_var[1]*100, 1), \"%, CCA2 =\", round(cca_var[2]*100, 1), \"% )\"),\n       color = \"Variable Type\") + theme_minimal()\n\nggplotly(p_cca, tooltip = c(\"text\"))\n\n\n\n\n\n\nR code\n# RDA\nrda_model &lt;- rda(abundance_matrix ~ ., data = soil_data)\nrda_var &lt;- summary(rda_model)$cont$importance[2,]\nrda_sites &lt;- scores(rda_model, display = \"sites\")\nrda_env &lt;- scores(rda_model, display = \"bp\")\nrda_df &lt;- as.data.frame(rda_sites)\nrda_df$sampleID &lt;- rownames(rda_df)\nrda_env_df &lt;- as.data.frame(rda_env)\nrda_env_df$variable &lt;- rownames(rda_env_df)\nrda_env_df &lt;- left_join(rda_env_df, variable_types, by = \"variable\")\n\np_rda &lt;- ggplot() +\n  geom_point(data = rda_df, aes(x = RDA1, y = RDA2, text = sampleID), color = \"darkgreen\") +\n  geom_segment(data = rda_env_df, aes(x = 0, y = 0, xend = RDA1, yend = RDA2, color = type, text = variable),\n               arrow = arrow(length = unit(0.3, \"cm\"))) +\n  geom_text(data = rda_env_df, aes(x = RDA1, y = RDA2, label = variable), vjust = -0.5) +\n  scale_color_manual(values = c(\"Moisture\" = \"blue\", \"Temperature\" = \"orange\", \"pH\" = \"green\")) +\n  labs(title = paste(\"RDA Interactive Biplot (Variance Explained: RDA1 =\", round(rda_var[1]*100, 1), \"%, RDA2 =\", round(rda_var[2]*100, 1), \"% )\"),\n       color = \"Variable Type\") + theme_minimal()\n\nggplotly(p_rda, tooltip = c(\"text\"))"
  },
  {
    "objectID": "labs/NEON_MAGs_Community_Analysis_Final_Comprehensive.html#step-1-load-and-prepare-data",
    "href": "labs/NEON_MAGs_Community_Analysis_Final_Comprehensive.html#step-1-load-and-prepare-data",
    "title": "Comprehensive Community Composition & Diversity Analysis of NEON MAGs with Soil Chemistry",
    "section": "",
    "text": "Explanation: Extract sample identifiers and soil chemistry variables.\n\n\nR code\ndf &lt;- read_csv(\"NEON_soilMAGs_soilChem.csv\")\n\ndf &lt;- df %&gt;%\n  filter(!is.na(genomicsSampleID)) %&gt;%\n  mutate(siteID = str_extract(genomicsSampleID, \"^[A-Z]+\"),\n         subplot = str_extract(genomicsSampleID, \"(?&lt;=_)[0-9]+\"),\n         layer = str_extract(genomicsSampleID, \"(?&lt;=-)[OM]\"),\n         sampleID = paste(siteID, subplot, layer, sep = \"_\")) %&gt;%\n  select(sampleID, phylum, soilMoisture, soilTemp, soilInWaterpH, soilInCaClpH)"
  },
  {
    "objectID": "labs/NEON_MAGs_Community_Analysis_Final_Comprehensive.html#step-2-taxonomic-profiling",
    "href": "labs/NEON_MAGs_Community_Analysis_Final_Comprehensive.html#step-2-taxonomic-profiling",
    "title": "Comprehensive Community Composition & Diversity Analysis of NEON MAGs with Soil Chemistry",
    "section": "",
    "text": "Explanation: Shows relative abundance of phyla per sample.\n\n\nR code\ntax_profile &lt;- df %&gt;%\n  group_by(sampleID, phylum) %&gt;%\n  summarise(count = n(), .groups = \"drop\") %&gt;%\n  mutate(rel_abundance = count / sum(count))\n\nggplot(tax_profile, aes(x = sampleID, y = rel_abundance, fill = phylum)) +\n  geom_bar(stat = \"identity\") +\n  theme_minimal() +\n  labs(title = \"Taxonomic Profile by Sample\", y = \"Relative Abundance\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), legend.position = \"right\")"
  },
  {
    "objectID": "labs/NEON_MAGs_Community_Analysis_Final_Comprehensive.html#step-3-alpha-diversity",
    "href": "labs/NEON_MAGs_Community_Analysis_Final_Comprehensive.html#step-3-alpha-diversity",
    "title": "Comprehensive Community Composition & Diversity Analysis of NEON MAGs with Soil Chemistry",
    "section": "",
    "text": "Explanation: Shannon = richness + evenness, Simpson = dominance, Richness = number of taxa.\n\n\nR code\nabundance_matrix &lt;- tax_profile %&gt;%\n  pivot_wider(names_from = phylum, values_from = count, values_fill = 0)\nabundance_matrix &lt;- abundance_matrix %&gt;% distinct(sampleID, .keep_all = TRUE)\nrn &lt;- abundance_matrix$sampleID\nabundance_matrix &lt;- abundance_matrix %&gt;% select(-sampleID)\nabundance_matrix &lt;- as.data.frame(lapply(abundance_matrix, as.numeric))\nrownames(abundance_matrix) &lt;- rn\nabundance_matrix &lt;- abundance_matrix[rowSums(abundance_matrix) &gt; 0, ]\n\nshannon &lt;- diversity(abundance_matrix, index = \"shannon\")\nsimpson &lt;- diversity(abundance_matrix, index = \"simpson\")\nrichness &lt;- specnumber(abundance_matrix)\nalpha_div &lt;- tibble(sampleID = rownames(abundance_matrix), Shannon = shannon, Simpson = simpson, Richness = richness)\nalpha_div\n\n\n# A tibble: 292 × 4\n   sampleID   Shannon  Simpson Richness\n   &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;int&gt;\n 1 BONA_001_O 0.00173 0.000359        2\n 2 BONA_002_O 0.694   0.500           3\n 3 BONA_004_O 0.00173 0.000359        2\n 4 BONA_006_O 0.694   0.500           3\n 5 BONA_009_O 1.10    0.667           4\n 6 BONA_013_O 0.00173 0.000359        2\n 7 BONA_070_O 0.694   0.500           3\n 8 BONA_071_O 0.00173 0.000359        2\n 9 BONA_080_O 0.00173 0.000359        2\n10 BONA_084_O 0.694   0.500           3\n# ℹ 282 more rows"
  },
  {
    "objectID": "labs/NEON_MAGs_Community_Analysis_Final_Comprehensive.html#step-4-beta-diversity",
    "href": "labs/NEON_MAGs_Community_Analysis_Final_Comprehensive.html#step-4-beta-diversity",
    "title": "Comprehensive Community Composition & Diversity Analysis of NEON MAGs with Soil Chemistry",
    "section": "",
    "text": "Explanation: Bray-Curtis measures compositional dissimilarity.\n\n\nR code\nbray_dist &lt;- vegdist(abundance_matrix, method = \"bray\")\nas.matrix(bray_dist)[1:5, 1:5]\n\n\n           BONA_001_O BONA_002_O BONA_004_O BONA_006_O BONA_009_O\nBONA_001_O  0.0000000  0.5237892  0.2941176  0.5789315  0.6999838\nBONA_002_O  0.5237892  0.0000000  0.3749650  0.5555157  0.3684121\nBONA_004_O  0.2941176  0.3749650  0.0000000  0.4285421  0.5999713\nBONA_006_O  0.5789315  0.5555157  0.4285421  0.0000000  0.2941065\nBONA_009_O  0.6999838  0.3684121  0.5999713  0.2941065  0.0000000"
  },
  {
    "objectID": "labs/NEON_MAGs_Community_Analysis_Final_Comprehensive.html#step-5-ordination-pcoa-and-nmds",
    "href": "labs/NEON_MAGs_Community_Analysis_Final_Comprehensive.html#step-5-ordination-pcoa-and-nmds",
    "title": "Comprehensive Community Composition & Diversity Analysis of NEON MAGs with Soil Chemistry",
    "section": "",
    "text": "Explanation: Visualize sample relationships in reduced dimensions.\n\n\nR code\npcoa_coords &lt;- cmdscale(bray_dist, k = 2)\npcoa_df &lt;- as_tibble(pcoa_coords) %&gt;% mutate(sampleID = rownames(pcoa_coords))\n\nggplot(pcoa_df, aes(x = V1, y = V2, label = sampleID)) + geom_point() + geom_text(vjust = -0.5) + labs(title = \"PCoA (Bray-Curtis)\")\n\n\n\n\n\n\n\n\n\nR code\nnmds &lt;- metaMDS(abundance_matrix, distance = \"bray\", k = 2)\n\n\nWisconsin double standardization\nRun 0 stress 0.08532757 \nRun 1 stress 0.1200854 \nRun 2 stress 0.08703641 \nRun 3 stress 0.09333725 \nRun 4 stress 0.08902399 \nRun 5 stress 0.1081445 \nRun 6 stress 0.08492605 \n... New best solution\n... Procrustes: rmse 0.01883282  max resid 0.2615482 \nRun 7 stress 0.08791272 \nRun 8 stress 0.1139224 \nRun 9 stress 0.0854949 \nRun 10 stress 0.1158335 \nRun 11 stress 0.119714 \nRun 12 stress 0.09171769 \nRun 13 stress 0.08640987 \nRun 14 stress 0.08262548 \n... New best solution\n... Procrustes: rmse 0.008558615  max resid 0.06322369 \nRun 15 stress 0.08490224 \nRun 16 stress 0.08386775 \nRun 17 stress 0.1077224 \nRun 18 stress 0.08189793 \n... New best solution\n... Procrustes: rmse 0.008302257  max resid 0.0633948 \nRun 19 stress 0.0937349 \nRun 20 stress 0.08654519 \n*** Best solution was not repeated -- monoMDS stopping criteria:\n    10: no. of iterations &gt;= maxit\n     8: stress ratio &gt; sratmax\n     2: scale factor of the gradient &lt; sfgrmin\n\n\nR code\nnmds_df &lt;- as_tibble(nmds$points) %&gt;% mutate(sampleID = rownames(nmds$points))\n\nggplot(nmds_df, aes(x = MDS1, y = MDS2, label = sampleID)) + geom_point() + geom_text(vjust = -0.5) + labs(title = \"NMDS (Bray-Curtis)\")"
  },
  {
    "objectID": "labs/NEON_MAGs_Community_Analysis_Final_Comprehensive.html#step-6-soil-chemistry-integration-for-cca-and-rda-with-color-coded-interactive-biplots",
    "href": "labs/NEON_MAGs_Community_Analysis_Final_Comprehensive.html#step-6-soil-chemistry-integration-for-cca-and-rda-with-color-coded-interactive-biplots",
    "title": "Comprehensive Community Composition & Diversity Analysis of NEON MAGs with Soil Chemistry",
    "section": "",
    "text": "Explanation: Arrows show environmental influence; color-coded by variable type.\n\n\nR code\nsoil_data &lt;- df %&gt;% select(sampleID, soilMoisture, soilTemp, soilInWaterpH, soilInCaClpH) %&gt;% distinct(sampleID, .keep_all = TRUE)\nsoil_data &lt;- soil_data %&gt;% filter(sampleID %in% rownames(abundance_matrix))\nsoil_data &lt;- column_to_rownames(soil_data, \"sampleID\")\nsoil_data &lt;- as.data.frame(lapply(soil_data, as.numeric))\nrownames(soil_data) &lt;- rownames(abundance_matrix)\n\nvariable_types &lt;- tibble(variable = c(\"soilMoisture\", \"soilTemp\", \"soilInWaterpH\", \"soilInCaClpH\"),\n                         type = c(\"Moisture\", \"Temperature\", \"pH\", \"pH\"))\n\n# CCA\ncca_model &lt;- cca(abundance_matrix ~ ., data = soil_data)\ncca_var &lt;- summary(cca_model)$concont$importance[2,]\ncca_sites &lt;- scores(cca_model, display = \"sites\")\ncca_env &lt;- scores(cca_model, display = \"bp\")\ncca_df &lt;- as.data.frame(cca_sites)\ncca_df$sampleID &lt;- rownames(cca_df)\ncca_env_df &lt;- as.data.frame(cca_env)\ncca_env_df$variable &lt;- rownames(cca_env_df)\ncca_env_df &lt;- left_join(cca_env_df, variable_types, by = \"variable\")\n\np_cca &lt;- ggplot() +\n  geom_point(data = cca_df, aes(x = CCA1, y = CCA2, text = sampleID), color = \"blue\") +\n  geom_segment(data = cca_env_df, aes(x = 0, y = 0, xend = CCA1, yend = CCA2, color = type, text = variable),\n               arrow = arrow(length = unit(0.3, \"cm\"))) +\n  geom_text(data = cca_env_df, aes(x = CCA1, y = CCA2, label = variable), vjust = -0.5) +\n  scale_color_manual(values = c(\"Moisture\" = \"blue\", \"Temperature\" = \"orange\", \"pH\" = \"green\")) +\n  labs(title = paste(\"CCA Interactive Biplot (Variance Explained: CCA1 =\", round(cca_var[1]*100, 1), \"%, CCA2 =\", round(cca_var[2]*100, 1), \"% )\"),\n       color = \"Variable Type\") + theme_minimal()\n\nggplotly(p_cca, tooltip = c(\"text\"))\n\n\n\n\n\n\nR code\n# RDA\nrda_model &lt;- rda(abundance_matrix ~ ., data = soil_data)\nrda_var &lt;- summary(rda_model)$cont$importance[2,]\nrda_sites &lt;- scores(rda_model, display = \"sites\")\nrda_env &lt;- scores(rda_model, display = \"bp\")\nrda_df &lt;- as.data.frame(rda_sites)\nrda_df$sampleID &lt;- rownames(rda_df)\nrda_env_df &lt;- as.data.frame(rda_env)\nrda_env_df$variable &lt;- rownames(rda_env_df)\nrda_env_df &lt;- left_join(rda_env_df, variable_types, by = \"variable\")\n\np_rda &lt;- ggplot() +\n  geom_point(data = rda_df, aes(x = RDA1, y = RDA2, text = sampleID), color = \"darkgreen\") +\n  geom_segment(data = rda_env_df, aes(x = 0, y = 0, xend = RDA1, yend = RDA2, color = type, text = variable),\n               arrow = arrow(length = unit(0.3, \"cm\"))) +\n  geom_text(data = rda_env_df, aes(x = RDA1, y = RDA2, label = variable), vjust = -0.5) +\n  scale_color_manual(values = c(\"Moisture\" = \"blue\", \"Temperature\" = \"orange\", \"pH\" = \"green\")) +\n  labs(title = paste(\"RDA Interactive Biplot (Variance Explained: RDA1 =\", round(rda_var[1]*100, 1), \"%, RDA2 =\", round(rda_var[2]*100, 1), \"% )\"),\n       color = \"Variable Type\") + theme_minimal()\n\nggplotly(p_rda, tooltip = c(\"text\"))"
  },
  {
    "objectID": "labs/lab13_ai_soilChemMAGs.html#exercises",
    "href": "labs/lab13_ai_soilChemMAGs.html#exercises",
    "title": "Lab 13 - Creating a detailed report with explanations from the NEON MAG data",
    "section": "Exercises",
    "text": "Exercises\n\nStart by saving a .qmd file for Lab 13 to the folder that represents your GitHub directory. From your index.qmd file create a link to the rendered html version of Lab 13. Push these changes to your GitHub repository and make sure you can see the linked version of Lab 13.\nUpload to co-pilot the NEON_soilMAGs_soilChem.csv file. Then ask for suggestions in the types of analysis you can do or create a prompt using one of the four categories above.\nIterate with co-pilot to create a comprehensive analysis of the category area.\nAdd explanations for the different types of analyses. For example what is Shannon Diversity? The final report should be something you could give another student or person in your lab.\nPush the final report to GitHub. Submit a link to your website (not your repository)"
  },
  {
    "objectID": "labs/PB_test.html",
    "href": "labs/PB_test.html",
    "title": "PB seminar",
    "section": "",
    "text": "First make a new folder “PBseminar” for the course materials\nIn RStudio Upload the file into your directory (import directly from github).\n\n\nR code\nraw_url &lt;- \"https://raw.githubusercontent.com/jeffreyblanchard/PathoGen2025/refs/heads/main/labs/NEON_soilMAGs_soilChem.csv\"\n\nmag &lt;- readr::read_csv(raw_url, show_col_types = FALSE)\n\n\nIn co-pilot paste this prompt This is a file with metagenomic assembled genomes (MAGs) from National Ecological Observatory Network metagenomic data. It contains the MAG taxonomy, genome statistics and associated soil chemistry data. Using R and tidyverse could you write the code to make a graph shows the phyla that are present at the NEON sites.\n\n\nR code\nlibrary(tidyverse)\nlibrary(janitor)    # clean_names()\nlibrary(lubridate)\n\n# 1) Load\npath &lt;- \"NEON_soilMAGs_soilChem.csv\"\nmag &lt;- readr::read_csv(path, show_col_types = FALSE) |&gt; janitor::clean_names()\n\n# 2) (Optional) Define a quality filter for MAGs — adjust thresholds to taste\nmag &lt;- mag |&gt;\n  mutate(\n    bin_quality = factor(bin_quality, levels = c(\"HQ\",\"MQ\",\"LQ\"), ordered = TRUE),\n    hq_flag = (bin_completeness &gt;= 90 & bin_contamination &lt;= 5) | bin_quality == \"HQ\"\n  )\n\n# 3) Choose whether to use all MAGs or only high-quality ones\nuse_hq_only &lt;- TRUE\nmag_use &lt;- if (use_hq_only) mag |&gt; filter(hq_flag) else mag\n\n# 4) Compute presence per (site_id, phylum)\npresence_df &lt;- mag_use |&gt;\n  filter(!is.na(site_id), !is.na(phylum)) |&gt;\n  distinct(site_id, phylum, bin_id) |&gt;               # distinct MAGs\n  summarize(present = n() &gt; 0, .by = c(site_id, phylum)) # logical presence\n\n# 5) Plot presence/absence as a tile heatmap\nggplot(presence_df, aes(x = site_id, y = phylum, fill = present)) +\n  geom_tile(color = \"grey80\") +\n  scale_fill_manual(values = c(`TRUE` = \"#2c7fb8\", `FALSE` = \"white\"),\n                    breaks = c(TRUE, FALSE),\n                    labels = c(\"Present\", \"Absent\")) +\n  labs(title = \"Phyla present at each NEON site\",\n       x = \"Site\", y = \"Phylum\", fill = NULL) +\n  theme_minimal(base_size = 12) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        panel.grid = element_blank())"
  },
  {
    "objectID": "methods/survey_plots.html",
    "href": "methods/survey_plots.html",
    "title": "Graphs of AI survey results",
    "section": "",
    "text": "Load libraries and read in survey\n\n\nPrior Coding Experience - Barplot\n\n\n\n\n\n\n\n\n\n\n\nPrior Coding Experience - Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nFrequency of AI Use - Barplot\n\n\n\n\n\n\n\n\n\n\n\nFrequency of AI Use - Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nHow much time did using AI save me?- Barplot\n\n\n\n\n\n\n\n\n\n\n\nHow much time did using AI save me?- Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nFrequency of incorrect AI output - Barplot\n\n\n\n\n\n\n\n\n\n\n\nFrequency of incorrect AI output - Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nDid you become more engaged or interactive in solving problems using AI? - Barplot\n\n\n\n\n\n\n\n\n\n\n\nDid you become more engaged or interactive in solving problems using AI? - Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nConfidence change in programming in R - Barplot\n\n\n\n\n\n\n\n\n\n\n\nConfidence change in programming in R - Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nClarity of instructions on AI use - Barplot\n\n\n\n\n\n\n\n\n\n\n\nClarity of instructions on AI use - Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nMain Likert graph\n\n\n\n\n\n\n\n\n\n\n\nWhen AI was wrong, I typically… - Barplot\n\n\n\n\n\n\n\n\n\n\n\nCommon issues encountered - Barplot\n\n\n\n\n\n\n\n\n\n\n\nWhere more support would help - Barplot\n\n\n\n\n\n\n\n\n\n\n\nTable - Overall effectiveness / Likelihood to recommend AI\n\n\n\n\n\ncolumn\nmean\nsd\nn\n\n\n\n\nLikelihood to recommend AI use in similar courses\n7.86\n1.98\n43\n\n\nOverall effectiveness of AI in this course\n8.60\n1.22\n43"
  },
  {
    "objectID": "methods/survey_plots.html#load-libraries-and-read-in-survey",
    "href": "methods/survey_plots.html#load-libraries-and-read-in-survey",
    "title": "Graphs of AI survey results",
    "section": "",
    "text": "R code\nlibrary(tidyverse) \nlibrary(janitor)\nlibrary(scales)\nlibrary(patchwork)   \nlibrary(sessioninfo)\n\n\n\n\nR code\n# Read CSV (adjust path if needed)\ndf_survey &lt;- readr::read_csv(\"course_survey_responses.csv\") |&gt;\n  clean_names()"
  },
  {
    "objectID": "methods/survey_plots.html#graphs",
    "href": "methods/survey_plots.html#graphs",
    "title": "Graphs of AI survey results",
    "section": "Graphs",
    "text": "Graphs\n\nPrior Coding Experience - Barplot\n\n\nR code\nexperience_levels = c(\n\"Extensive (multiple courses or projects)\",\n\"Moderate (regular use in another language)\",\n\"Some (one course or self-study)\",\n\"Very limited (a few tutorials or labs)\",\n\"None\"\n)\n\ndf_survey |&gt;\n  count(prior_coding_experience, name = \"n\") |&gt;\n   complete(prior_coding_experience = experience_levels, fill = list(n = 0))  |&gt;\n  mutate(prior_coding_experience = factor(prior_coding_experience, levels = experience_levels))  |&gt;\n  mutate(pct = n / sum(n)) |&gt;\n\n# Plot: horizontal bars with count + percent at end of each bar\nggplot(aes(x = prior_coding_experience, y = pct)) +\n  geom_col(fill = \"#4C78A8\", width = 0.7) +\n  coord_flip() +\n  scale_y_continuous(labels = percent_format(accuracy = 1), expand = expansion(mult = c(0, 0.12))) +\n  geom_text(aes(label = paste0(n, \" (\", scales::percent(pct, accuracy = 1), \")\")),\n            hjust = -0.02, size = 3.2) +\n  labs(title = \"Prior coding experience\",\n       x = NULL, y = \"Percentage of respondents\") +\n  theme_minimal(base_size = 12) +\n  theme(panel.grid.major.y = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nFrequency of AI Use - Barplot\n\n\nR code\nexperience_levels &lt;- c(\n\"I did not use AI\",\n\"I tried it once or twice\",\n\"About once per week\",\n\"A few times per week\",\n\"Daily\"\n)\n\ndf_survey |&gt;\n  count(frequency_of_ai_use_in_this_course, name = \"n\") |&gt;\n  complete(frequency_of_ai_use_in_this_course = experience_levels, fill = list(n = 0))  |&gt;\n  mutate(frequency_of_ai_use_in_this_course = factor(frequency_of_ai_use_in_this_course, levels = experience_levels))  |&gt;\n  mutate(pct = n / sum(n)) |&gt;\n# Plot: horizontal bars with count + percent at end of each bar\nggplot(aes(x = frequency_of_ai_use_in_this_course, y = pct)) +\n  geom_col(fill = \"#4C78A8\", width = 0.7) +\n  coord_flip() +\n  scale_y_continuous(labels = percent_format(accuracy = 1), expand = expansion(mult = c(0, 0.12))) +\n  geom_text(aes(label = paste0(n, \" (\", scales::percent(pct, accuracy = 1), \")\")),\n            hjust = -0.02, size = 3.2) +\n  labs(title = \"Frequency of AI Use\",\n       x = NULL, y = \"Percentage of respondents\") +\n  theme_minimal(base_size = 12) +\n  theme(panel.grid.major.y = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nHow much time did using AI save me?- Barplot\n\n\nR code\nexperience_levels &lt;- c(\n\"&lt; 0.5 hr total\",\n\"0.5-1 hrs\",\n\"1-2 hrs\",\n\"2-3 hrs\",\n\"3+ hrs\"\n)\n\ndf_survey |&gt;\n  count(how_much_time_did_using_ai_save_me, name = \"n\") |&gt;\n  complete(how_much_time_did_using_ai_save_me = experience_levels, fill = list(n = 0))  |&gt;\n  mutate(how_much_time_did_using_ai_save_me = factor(how_much_time_did_using_ai_save_me, levels = experience_levels))  |&gt;\n  mutate(pct = n / sum(n)) |&gt;\n# Plot: horizontal bars with count + percent at end of each bar\nggplot(aes(x = how_much_time_did_using_ai_save_me, y = pct)) +\n  geom_col(fill = \"#4C78A8\", width = 0.7) +\n  coord_flip() +\n  scale_y_continuous(labels = percent_format(accuracy = 1), expand = expansion(mult = c(0, 0.12))) +\n  geom_text(aes(label = paste0(n, \" (\", scales::percent(pct, accuracy = 1), \")\")),\n            hjust = -0.02, size = 3.2) +\n  labs(title = \"How much time did using AI save me?\",\n       x = NULL, y = \"Percentage of respondents\") +\n  theme_minimal(base_size = 12) +\n  theme(panel.grid.major.y = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nFrequency of incorrect AI output - Barplot\n\n\nR code\nexperience_levels &lt;- c(\n\"Never\",\n\"Rarely\",\n\"Sometimes\",\n\"Often\",\n\"Very Often\"\n)\n\ndf_survey |&gt;\n  count(frequency_of_incorrect_ai_output, name = \"n\") |&gt;\n  complete(frequency_of_incorrect_ai_output = experience_levels, fill = list(n = 0))  |&gt;\n  mutate(frequency_of_incorrect_ai_output = factor(frequency_of_incorrect_ai_output, levels = experience_levels))  |&gt;\nmutate(pct = n / sum(n)) |&gt;\n# Plot: horizontal bars with count + percent at end of each bar\nggplot(aes(x = frequency_of_incorrect_ai_output, y = pct)) +\n  geom_col(fill = \"#4C78A8\", width = 0.7) +\n  coord_flip() +\n  scale_y_continuous(labels = percent_format(accuracy = 1), expand = expansion(mult = c(0, 0.12))) +\n  geom_text(aes(label = paste0(n, \" (\", scales::percent(pct, accuracy = 1), \")\")),\n            hjust = -0.02, size = 3.2) +\n  labs(title = \"Frequency of incorrect AI output\",\n       x = NULL, y = \"Percentage of respondents\") +\n  theme_minimal(base_size = 12) +\n  theme(panel.grid.major.y = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nDid you become more engaged or interactive in solving problems using AI? - Barplot\n\n\nR code\nexperience_levels &lt;- c(\n\"Much less engaged\",\n\"Slightly less engaged\",\n\"No change\",\n\"Slightly more engaged\",\n\"Much more engaged\"\n)\n\ndf_survey |&gt;\n  count(did_you_become_more_engaged_or_interactive_in_solving_problems_using_ai, name = \"n\") |&gt;\n  complete(did_you_become_more_engaged_or_interactive_in_solving_problems_using_ai = experience_levels, fill = list(n = 0))  |&gt;\n  mutate(did_you_become_more_engaged_or_interactive_in_solving_problems_using_ai = factor(did_you_become_more_engaged_or_interactive_in_solving_problems_using_ai, levels = experience_levels))  |&gt;\nmutate(pct = n / sum(n)) |&gt;\n# Plot: horizontal bars with count + percent at end of each bar\nggplot(aes(x = did_you_become_more_engaged_or_interactive_in_solving_problems_using_ai, y = pct)) +\n  geom_col(fill = \"#4C78A8\", width = 0.7) +\n  coord_flip() +\n  scale_y_continuous(labels = percent_format(accuracy = 1), expand = expansion(mult = c(0, 0.12))) +\n  geom_text(aes(label = paste0(n, \" (\", scales::percent(pct, accuracy = 1), \")\")),\n            hjust = -0.02, size = 3.2) +\n  labs(title = \"Did you become more engaged or interactive in solving problems using AI?\",\n       x = NULL, y = \"Percentage of respondents\") +\n  theme_minimal(base_size = 12) +\n  theme(panel.grid.major.y = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nConfidence change in programming in R - Barplot\n\n\nR code\nexperience_levels &lt;- c(\n\"Much lower\",\n\"Lower\",\n\"About the same\",\n\"Higher\",\n\"Much higher\"\n)\n\ndf_survey |&gt;\n  count(confidence_change_in_programming_in_r, name = \"n\") |&gt;\n  complete(confidence_change_in_programming_in_r = experience_levels, fill = list(n = 0))  |&gt;\n  mutate(confidence_change_in_programming_in_r = factor(confidence_change_in_programming_in_r, levels = experience_levels))  |&gt;\nmutate(pct = n / sum(n)) |&gt;\n# Plot: horizontal bars with count + percent at end of each bar\nggplot(aes(x = confidence_change_in_programming_in_r, y = pct)) +\n  geom_col(fill = \"#4C78A8\", width = 0.7) +\n  coord_flip() +\n  scale_y_continuous(labels = percent_format(accuracy = 1), expand = expansion(mult = c(0, 0.12))) +\n  geom_text(aes(label = paste0(n, \" (\", scales::percent(pct, accuracy = 1), \")\")),\n            hjust = -0.02, size = 3.2) +\n  labs(title = \"Confidence change in programming in R\",\n       x = NULL, y = \"Percentage of respondents\") +\n  theme_minimal(base_size = 12) +\n  theme(panel.grid.major.y = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nClarity of instructions on AI use - Barplot\n\n\nR code\nexperience_levels &lt;- c(\n\"Very unclear\",\n\"Unclear\",\n\"Neutral\",\n\"Clear\",\n\"Very clear\"\n)\n\ndf_survey |&gt;\n  count(clarity_of_instructions_on_ai_use, name = \"n\") |&gt;\n  complete(clarity_of_instructions_on_ai_use = experience_levels, fill = list(n = 0))  |&gt;\n  mutate(clarity_of_instructions_on_ai_use = factor(clarity_of_instructions_on_ai_use, levels = experience_levels))  |&gt;\nmutate(pct = n / sum(n)) |&gt;\n# Plot: horizontal bars with count + percent at end of each bar\nggplot(aes(x = clarity_of_instructions_on_ai_use, y = pct)) +\n  geom_col(fill = \"#4C78A8\", width = 0.7) +\n  coord_flip() +\n  scale_y_continuous(labels = percent_format(accuracy = 1), expand = expansion(mult = c(0, 0.12))) +\n  geom_text(aes(label = paste0(n, \" (\", scales::percent(pct, accuracy = 1), \")\")),\n            hjust = -0.02, size = 3.2) +\n  labs(title = \"Clarity of instructions on AI use\",\n       x = NULL, y = \"Percentage of respondents\") +\n  theme_minimal(base_size = 12) +\n  theme(panel.grid.major.y = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nWhen AI was wrong, I typically… - Barplot\n\n\nR code\n# Choose whether percentages should be relative to respondents who answered this question (\"answerers\") or relative to all survey respondents (\"all\"):\npercent_base &lt;- \"answerers\" # \"answerers\" or \"all\"\ncsv_path &lt;- \"course_survey_responses.csv\" # adjust path if needed\n\n\n\n\nR code\n## Read data and detect the column\ndf &lt;- readr::read_csv(csv_path)\n\n# detect the multi-response column (flexible by partial name)\nresp_col &lt;- names(df)[str_detect(names(df), regex(\"When AI was wrong\", ignore_case = TRUE))][1]\nif(is.na(resp_col)) stop(\"Could not find a column matching 'When AI was wrong' in the CSV.\")\nresp_col\n\n\n[1] \"When AI was wrong, I typically…\"\n\n\n\n\nR code\n# Prepare answers (split multi-response entries)\n# ensure respondent id exists\nif(!\"respondent_id\" %in% names(df)) df &lt;- df %&gt;% mutate(respondent_id = row_number())\n\ndf_answers &lt;- df %&gt;%\n  select(respondent_id, all_of(resp_col)) %&gt;%\n  rename(raw = all_of(resp_col)) %&gt;%\n  filter(!is.na(raw) & str_trim(raw) != \"\") %&gt;%               # only respondents who answered this question\n  mutate(respondent_id = as.character(respondent_id)) %&gt;%\n  # split by comma or semicolon (adjust sep if your data uses a different delimiter)\n  separate_rows(raw, sep = \"\\\\s*[,;]\\\\s*\") %&gt;%\n  # treat \"Other: ...\" as \"Other\" (if you want to inspect free text, modify this)\n  mutate(answer = str_remove(raw, regex(\"^Other[:\\\\-]\\\\s*\", ignore_case = TRUE)) %&gt;% str_trim()) %&gt;%\n  mutate(answer = if_else(str_detect(raw, regex(\"^Other[:\\\\-]\", ignore_case = TRUE)), \"Other\", answer)) %&gt;%\n  distinct(respondent_id, answer) %&gt;%   # avoid double-count within same respondent\n  filter(answer != \"\" & !is.na(answer))\n\n# quick look at distinct answers found\n# df_answers %&gt;% distinct(answer) %&gt;% arrange(answer)\n\n\n\n\nR code\n# Custom ordering example (edit as desired):\n# options_order &lt;- c(\n#   \"Ran and tested the code in R\",\n#   \"Asked follow-up questions to the AI\",\n#   \"Checked documentation or help pages\",\n#   \"Consulted classmates\",\n#   \"Ignored or discarded AI suggestions\",\n#   \"Other\"\n# )\n\n# If custom order not provided, infer from observed answers:\noptions_order &lt;- if(exists(\"options_order\")) options_order else df_answers %&gt;% distinct(answer) %&gt;% pull(answer)\n\n\n\n\nR code\n# Count and compute percentages\ncounts &lt;- df_answers %&gt;%\n  count(answer, name = \"n\") %&gt;%\n  complete(answer = options_order, fill = list(n = 0)) %&gt;%\n  mutate(answer = factor(answer, levels = options_order))\n\nif(tolower(percent_base) == \"all\"){\n  counts &lt;- counts %&gt;% mutate(pct = n / nrow(df))\n  y_label &lt;- \"Percent of all respondents\"\n} else {\n  counts &lt;- counts %&gt;% mutate(pct = n / n_distinct(df_answers$respondent_id))\n  y_label &lt;- \"Percent of respondents who answered this question\"\n}\n\n\n\n\nR code\n# Plot\nlibrary(ggplot2)\n\np &lt;- ggplot(counts, aes(x = fct_rev(answer), y = pct)) +\n  geom_col(fill = \"#4C78A8\", width = 0.7) +\n  coord_flip() +\n  scale_y_continuous(labels = percent_format(accuracy = 1), expand = expansion(mult = c(0, 0.12))) +\n  geom_text(aes(label = paste0(n, \" (\", scales::percent(pct, accuracy = 1), \")\")),\n            hjust = -0.02, size = 3.2) +\n  labs(title = \"When AI was wrong, I typically…\",\n       x = NULL,\n       y = y_label) +\n  theme_minimal(base_size = 12) +\n  theme(panel.grid.major.y = element_blank())\n\np\n\n\n\n\n\n\n\n\n\n\n\nCommon issues encountered - Barplot\n\n\nR code\n# remove all objects from the environment\nrm(list = ls())\n\n\n\n\nR code\n# Choose whether percentages should be relative to respondents who answered this question (\"answerers\") or relative to all survey respondents (\"all\"):\npercent_base &lt;- \"answerers\" # \"answerers\" or \"all\"\ncsv_path &lt;- \"course_survey_responses.csv\" # adjust path if needed\n\n\n\n\nR code\n## Read data and detect the column\ndf &lt;- readr::read_csv(csv_path)\n\nresp_col &lt;- names(df)[str_detect(names(df), regex(\"Common issues encountered\", ignore_case = TRUE))][1]\nif(is.na(resp_col)) stop(\"Could not find a column matching 'Common issues encountered' in the CSV.\")\nresp_col\n\n\n[1] \"Common issues encountered\"\n\n\n\n\nR code\n# Prepare answers (split multi-response entries)\n# ensure respondent id exists\nif(!\"respondent_id\" %in% names(df)) df &lt;- df %&gt;% mutate(respondent_id = row_number())\n\ndf_answers &lt;- df %&gt;%\n  select(respondent_id, all_of(resp_col)) %&gt;%\n  rename(raw = all_of(resp_col)) %&gt;%\n  filter(!is.na(raw) & str_trim(raw) != \"\") %&gt;%               # only respondents who answered this question\n  mutate(respondent_id = as.character(respondent_id)) %&gt;%\n  # split by comma or semicolon (adjust sep if your data uses a different delimiter)\n  separate_rows(raw, sep = \"\\\\s*[,;]\\\\s*\") %&gt;%\n  # treat \"Other: ...\" as \"Other\" (if you want to inspect free text, modify this)\n  mutate(answer = str_remove(raw, regex(\"^Other[:\\\\-]\\\\s*\", ignore_case = TRUE)) %&gt;% str_trim()) %&gt;%\n  mutate(answer = if_else(str_detect(raw, regex(\"^Other[:\\\\-]\", ignore_case = TRUE)), \"Other\", answer)) %&gt;%\n  distinct(respondent_id, answer) %&gt;%   # avoid double-count within same respondent\n  filter(answer != \"\" & !is.na(answer))\n\n# quick look at distinct answers found\n# df_answers %&gt;% distinct(answer) %&gt;% arrange(answer)\n\n\n\n\nR code\n# Custom ordering example (edit as desired):\n# options_order &lt;- c(\n#   \"Ran and tested the code in R\",\n#   \"Asked follow-up questions to the AI\",\n#   \"Checked documentation or help pages\",\n#   \"Consulted classmates\",\n#   \"Ignored or discarded AI suggestions\",\n#   \"Other\"\n# )\n\n# If custom order not provided, infer from observed answers:\noptions_order &lt;- if(exists(\"options_order\")) options_order else df_answers %&gt;% distinct(answer) %&gt;% pull(answer)\n\n\n\n\nR code\n# Count and compute percentages\ncounts &lt;- df_answers %&gt;%\n  count(answer, name = \"n\") %&gt;%\n  complete(answer = options_order, fill = list(n = 0)) %&gt;%\n  mutate(answer = factor(answer, levels = options_order))\n\nif(tolower(percent_base) == \"all\"){\n  counts &lt;- counts %&gt;% mutate(pct = n / nrow(df))\n  y_label &lt;- \"Percent of all respondents\"\n} else {\n  counts &lt;- counts %&gt;% mutate(pct = n / n_distinct(df_answers$respondent_id))\n  y_label &lt;- \"Percent of respondents who answered this question\"\n}\n\n\n\n\nR code\n# Plot\nlibrary(ggplot2)\n\np &lt;- ggplot(counts, aes(x = fct_rev(answer), y = pct)) +\n  geom_col(fill = \"#4C78A8\", width = 0.7) +\n  coord_flip() +\n  scale_y_continuous(labels = percent_format(accuracy = 1), expand = expansion(mult = c(0, 0.12))) +\n  geom_text(aes(label = paste0(n, \" (\", scales::percent(pct, accuracy = 1), \")\")),\n            hjust = -0.02, size = 3.2) +\n  labs(title = \"Common issues encountered\",\n       x = NULL,\n       y = y_label) +\n  theme_minimal(base_size = 12) +\n  theme(panel.grid.major.y = element_blank())\n\np\n\n\n\n\n\n\n\n\n\n\n\nWhere more support would help - Barplot\n\n\nR code\n# remove all objects from the environment\nrm(list = ls())\n\n\n\n\nR code\n# Choose whether percentages should be relative to respondents who answered this question (\"answerers\") or relative to all survey respondents (\"all\"):\npercent_base &lt;- \"answerers\" # \"answerers\" or \"all\"\ncsv_path &lt;- \"course_survey_responses.csv\" # adjust path if needed\n\n\n\n\nR code\n## Read data and detect the column\ndf &lt;- readr::read_csv(csv_path)\n\nresp_col &lt;- names(df)[str_detect(names(df), regex(\"Where more support would help\", ignore_case = TRUE))][1]\nif(is.na(resp_col)) stop(\"Could not find a column matching 'Where more support would help' in the CSV.\")\nresp_col\n\n\n[1] \"Where more support would help\"\n\n\n\n\nR code\n# Prepare answers (split multi-response entries)\n# ensure respondent id exists\nif(!\"respondent_id\" %in% names(df)) df &lt;- df %&gt;% mutate(respondent_id = row_number())\n\ndf_answers &lt;- df %&gt;%\n  select(respondent_id, all_of(resp_col)) %&gt;%\n  rename(raw = all_of(resp_col)) %&gt;%\n  filter(!is.na(raw) & str_trim(raw) != \"\") %&gt;%               # only respondents who answered this question\n  mutate(respondent_id = as.character(respondent_id)) %&gt;%\n  # split by comma or semicolon (adjust sep if your data uses a different delimiter)\n  separate_rows(raw, sep = \"\\\\s*[,;]\\\\s*\") %&gt;%\n  # treat \"Other: ...\" as \"Other\" (if you want to inspect free text, modify this)\n  mutate(answer = str_remove(raw, regex(\"^Other[:\\\\-]\\\\s*\", ignore_case = TRUE)) %&gt;% str_trim()) %&gt;%\n  mutate(answer = if_else(str_detect(raw, regex(\"^Other[:\\\\-]\", ignore_case = TRUE)), \"Other\", answer)) %&gt;%\n  distinct(respondent_id, answer) %&gt;%   # avoid double-count within same respondent\n  filter(answer != \"\" & !is.na(answer))\n\n# quick look at distinct answers found\n# df_answers %&gt;% distinct(answer) %&gt;% arrange(answer)\n\n\n\n\nR code\n# Custom ordering example (edit as desired):\n# options_order &lt;- c(\n#   \"Ran and tested the code in R\",\n#   \"Asked follow-up questions to the AI\",\n#   \"Checked documentation or help pages\",\n#   \"Consulted classmates\",\n#   \"Ignored or discarded AI suggestions\",\n#   \"Other\"\n# )\n\n# If custom order not provided, infer from observed answers:\noptions_order &lt;- if(exists(\"options_order\")) options_order else df_answers %&gt;% distinct(answer) %&gt;% pull(answer)\n\n\n\n\nR code\n# Count and compute percentages\ncounts &lt;- df_answers %&gt;%\n  count(answer, name = \"n\") %&gt;%\n  complete(answer = options_order, fill = list(n = 0)) %&gt;%\n  mutate(answer = factor(answer, levels = options_order))\n\nif(tolower(percent_base) == \"all\"){\n  counts &lt;- counts %&gt;% mutate(pct = n / nrow(df))\n  y_label &lt;- \"Percent of all respondents\"\n} else {\n  counts &lt;- counts %&gt;% mutate(pct = n / n_distinct(df_answers$respondent_id))\n  y_label &lt;- \"Percent of respondents who answered this question\"\n}\n\n\n\n\nR code\n# Plot\nlibrary(ggplot2)\n\np &lt;- ggplot(counts, aes(x = fct_rev(answer), y = pct)) +\n  geom_col(fill = \"#4C78A8\", width = 0.7) +\n  coord_flip() +\n  scale_y_continuous(labels = percent_format(accuracy = 1), expand = expansion(mult = c(0, 0.12))) +\n  geom_text(aes(label = paste0(n, \" (\", scales::percent(pct, accuracy = 1), \")\")),\n            hjust = -0.02, size = 3.2) +\n  labs(title = \"Where more support would help\",\n       x = NULL,\n       y = y_label) +\n  theme_minimal(base_size = 12) +\n  theme(panel.grid.major.y = element_blank())\n\np\n\n\n\n\n\n\n\n\n\n\n\nLikert graphs\n\n\nR code\n# Read CSV (adjust path if needed)\ndf &lt;- readr::read_csv(\"course_survey_responses.csv\")\n\n# Requested domain order (will be used to control facet/plot sequence)\ndomains &lt;- c(\n  \"Helpfulness of generative AI for each use case\",\n  \"Impact on learning\",\n  \"Impact on metagenomics learning\",\n  \"Code quality outcomes using AI\",\n  \"Trust calibration\",\n  \"Ethical alignment\",\n  \"Inquiry & exploration\",\n  \"\\\"Vibe code\\\" outcomes\"\n)\n\n# find columns that match any domain substring\ncol_matches &lt;- names(df)[sapply(names(df), function(nm) any(sapply(domains, function(d) str_detect(nm, fixed(d)))))]\nif (length(col_matches) == 0) stop(\"No matching columns found. Check column names or adjust domain patterns.\")\nraw_help &lt;- df %&gt;% select(all_of(col_matches))\n\n\n\n\nR code\n# helper to detect domain and extract item text inside brackets if present\ndetect_domain &lt;- function(colname, domain_vec) {\n  idx &lt;- which(sapply(domain_vec, function(d) str_detect(colname, fixed(d))))\n  if (length(idx) == 0) return(NA_character_)\n  domain_vec[idx[1]]\n}\n\ncol_info &lt;- tibble(orig_name = names(raw_help)) %&gt;%\n  mutate(domain = map_chr(orig_name, ~ detect_domain(.x, domains)),\n         item = if_else(str_detect(orig_name, \"\\\\[.*\\\\]\"),\n                        str_extract(orig_name, \"(?&lt;=\\\\[).+?(?=\\\\])\"),\n                        str_remove(orig_name, fixed(domain))),\n         item = str_trim(item),\n         item = if_else(item == \"\" | is.na(item), orig_name, item))\n\n\n\n\nR code\n# Agreement levels and robust mapping function\nagree_levels &lt;- c(\"Strongly disagree\", \"Disagree\", \"Neutral\", \"Agree\", \"Strongly agree\")\n\nmap_to_agree &lt;- function(x) {\n  x &lt;- stringr::str_squish(as.character(x))\n  x_lower &lt;- tolower(x)\n  case_when(\n    # helpfulness -&gt; agreement mapping\n    x_lower %in% c(\"not at all helpful\", \"not helpful\", \"not at all helpful to not helpful\") ~ \"Strongly disagree\",\n    x_lower %in% c(\"slightly helpful\") ~ \"Disagree\",\n    x_lower %in% c(\"moderately helpful\") ~ \"Neutral\",\n    x_lower %in% c(\"very helpful\") ~ \"Agree\",\n    x_lower %in% c(\"extremely helpful\") ~ \"Strongly agree\",\n    # direct agreement variants\n    x_lower %in% c(\"strongly disagree\", \"strong disagree\") ~ \"Strongly disagree\",\n    x_lower %in% c(\"disagree\") ~ \"Disagree\",\n    x_lower %in% c(\"neutral\", \"neither agree nor disagree\", \"neither\") ~ \"Neutral\",\n    x_lower %in% c(\"agree\") ~ \"Agree\",\n    x_lower %in% c(\"strongly agree\", \"strong agree\") ~ \"Strongly agree\",\n    # already canonical\n    x %in% agree_levels ~ x,\n    TRUE ~ NA_character_\n  )\n}\n\n# pivot to long, join col_info, map responses, compute counts and percentages\nlong &lt;- raw_help %&gt;%\n  pivot_longer(cols = everything(), names_to = \"orig_name\", values_to = \"response_raw\") %&gt;%\n  left_join(col_info, by = \"orig_name\") %&gt;%\n  mutate(response_mapped = map_to_agree(response_raw),\n         response_mapped = factor(response_mapped, levels = agree_levels, ordered = TRUE)) %&gt;%\n  group_by(domain, item, response_mapped) %&gt;%\n  tally(name = \"n\") %&gt;%\n  group_by(domain, item) %&gt;%\n  mutate(pct = n / sum(n)) %&gt;%\n  ungroup()\n\n# report any unmapped response variants\nunmapped &lt;- raw_help %&gt;%\n  pivot_longer(cols = everything(), names_to = \"orig_name\", values_to = \"response_raw\") %&gt;%\n  distinct(response_raw) %&gt;%\n  mutate(mapped = map_to_agree(response_raw)) %&gt;%\n  filter(is.na(mapped))\n\n# if (nrow(unmapped) &gt; 0) {\n#  message(\"Unmapped response variants detected (consider adding mappings to map_to_agree):\")\n#  print(unmapped$response_raw)\n#} else {\n#  message(\"No unmapped response variants detected based on current mapping.\")\n# }\n\n\n\n\nR code\n# compute per-item positive proportion (Agree + Strongly agree) per domain\nitem_summary &lt;- long %&gt;%\n  filter(!is.na(response_mapped)) %&gt;%\n  group_by(domain, item) %&gt;%\n  summarize(total_n = sum(n, na.rm = TRUE),\n            positive_n = sum(n[response_mapped %in% c(\"Agree\", \"Strongly agree\")], na.rm = TRUE),\n            positive_pct = if_else(total_n &gt; 0, positive_n / total_n, 0),\n            .groups = \"drop\")\n\n\n\n\nR code\n# Prepare per-domain data frames (only domains present)\ndomain_dfs &lt;- long %&gt;%\n  filter(!is.na(response_mapped)) %&gt;%\n  group_split(domain)\nnames(domain_dfs) &lt;- map_chr(domain_dfs, ~ unique(.x$domain))\n\n# keep only requested domains that actually exist in data\npresent_domains &lt;- intersect(domains, names(domain_dfs))\ndomain_dfs &lt;- domain_dfs[present_domains]\n\n# count items per domain for relative heights\nitem_counts &lt;- map_int(domain_dfs, ~ length(unique(.x$item)))\n\n# color palette and common y-limits\ncols &lt;- c(\"Strongly disagree\" = \"#d73027\",\n          \"Disagree\" = \"#fc8d59\",\n          \"Neutral\" = \"#fee090\",\n          \"Agree\" = \"#91bfdb\",\n          \"Strongly agree\" = \"#1a9850\")\ny_limits &lt;- c(0, 1)\n\n# Build one ggplot per domain. Ensure scale_fill includes all five categories and uses a one-row legend.\nplot_list &lt;- imap(domain_dfs, function(df_dom, dom_name) {\n  # determine ordering by positive_pct (descending), tie-break by total_n (descending)\n  item_order &lt;- item_summary %&gt;%\n    filter(domain == dom_name) %&gt;%\n    arrange(-desc(positive_pct), desc(total_n)) %&gt;%\n    pull(item)\n  if (length(item_order) == 0) item_order &lt;- unique(df_dom$item)\n\n  df_dom2 &lt;- df_dom %&gt;%\n    mutate(item = factor(item, levels = item_order, ordered = TRUE))\n\n  ggplot(df_dom2, aes(x = item, y = pct, fill = response_mapped)) +\n    geom_col(width = 0.75, color = NA) +\n    coord_flip() +\n    scale_y_continuous(labels = percent_format(accuracy = 1), limits = y_limits, expand = c(0,0)) +\n    scale_fill_manual(values = cols,\n                      breaks = agree_levels,\n                      guide = guide_legend(nrow = 1, byrow = TRUE),\n                      name = \"Response\") +\n    labs(title = dom_name, x = NULL, y = NULL) +\n    theme_minimal(base_size = 12) +\n    theme(plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),\n          axis.text.y = element_text(size = 10),\n          panel.grid.major.y = element_blank(),\n          plot.margin = margin(6,6,6,6),\n          legend.position = \"bottom\")   # keep legend present for collection\n})\n\n\n\n\nR code\n# Combine vertically with a single collected legend at bottom\ncombined &lt;- wrap_plots(plot_list, ncol = 1, guides = \"collect\") +\n  plot_layout(heights = item_counts, guides = \"collect\") & theme(legend.position = \"bottom\")\n\nprint(combined)\n\n\n\n\n\n\n\n\n\n\n\nOverall effectiveness and Likelihood to recommend AI - Table\n\n\nR code\ncols &lt;- c(\n  \"Overall effectiveness of AI in this course\",\n  \"Likelihood to recommend AI use in similar courses\"\n)\n\nres &lt;- df %&gt;%\n  select(any_of(cols)) %&gt;%\n  pivot_longer(everything(), names_to = \"column\", values_to = \"value\") %&gt;%\n  mutate(value = as.numeric(value)) %&gt;%                # convert to numeric\n  group_by(column) %&gt;%\n  summarise(\n    mean = mean(value, na.rm = TRUE),\n    sd   = sd(value, na.rm = TRUE),\n    n    = sum(!is.na(value)),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(mean = round(mean, 2), sd = round(sd, 2))\n\nprint(res)\n\n\n# A tibble: 2 × 4\n  column                                             mean    sd     n\n  &lt;chr&gt;                                             &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 Likelihood to recommend AI use in similar courses  7.86  1.98    43\n2 Overall effectiveness of AI in this course         8.6   1.22    43"
  },
  {
    "objectID": "methods/survey_plots_embedded_agree4.html",
    "href": "methods/survey_plots_embedded_agree4.html",
    "title": "Survey — Agreement-style Likert Plot (Final)",
    "section": "",
    "text": "Save the text below exactly as likert_all_domains_final.Rmd in the same folder as course_survey_responses.csv, then open it in RStudio and click Knit (or run rmarkdown::render(“likert_all_domains_final.Rmd”)). This is the full updated R Markdown: it explicitly loads tidyverse, orders items within each domain by positive proportion (Agree + Strongly agree) descending, and produces one column of domain plots where a single shared legend (all five response categories) appears at the bottom.\nThis R Markdown reads course_survey_responses.csv and builds a single-column stack of domain plots. Items in each domain are ordered so those with the largest combined proportion of “Agree” + “Strongly agree” appear at the top. A single legend showing all five response categories is collected and placed at the bottom.\n\n\nR code\nlibrary(tidyverse)   # includes dplyr, ggplot2, tidyr, purrr, stringr, forcats, etc.\nlibrary(scales)\nlibrary(patchwork)   # combine plots with custom relative heights\nlibrary(sessioninfo)\n\n\n\n\nR code\n# Read CSV (adjust path if needed)\ndf &lt;- readr::read_csv(\"course_survey_responses.csv\")\n\n# Requested domain order (will be used to control facet/plot sequence)\ndomains &lt;- c(\n  \"Helpfulness of generative AI for each use case\",\n  \"Impact on learning\",\n  \"Impact on metagenomics learning\",\n  \"Code quality outcomes using AI\",\n  \"Trust calibration\",\n  \"Ethical alignment\",\n  \"Inquiry & exploration\",\n  \"\\\"Vibe code\\\" outcomes\"\n)\n\n# find columns that match any domain substring\ncol_matches &lt;- names(df)[sapply(names(df), function(nm) any(sapply(domains, function(d) str_detect(nm, fixed(d)))))]\nif (length(col_matches) == 0) stop(\"No matching columns found. Check column names or adjust domain patterns.\")\nraw_help &lt;- df %&gt;% select(all_of(col_matches))\n\n\n\n\nR code\n# helper to detect domain and extract item text inside brackets if present\ndetect_domain &lt;- function(colname, domain_vec) {\n  idx &lt;- which(sapply(domain_vec, function(d) str_detect(colname, fixed(d))))\n  if (length(idx) == 0) return(NA_character_)\n  domain_vec[idx[1]]\n}\n\ncol_info &lt;- tibble(orig_name = names(raw_help)) %&gt;%\n  mutate(domain = map_chr(orig_name, ~ detect_domain(.x, domains)),\n         item = if_else(str_detect(orig_name, \"\\\\[.*\\\\]\"),\n                        str_extract(orig_name, \"(?&lt;=\\\\[).+?(?=\\\\])\"),\n                        str_remove(orig_name, fixed(domain))),\n         item = str_trim(item),\n         item = if_else(item == \"\" | is.na(item), orig_name, item))\ncol_info\n\n\n# A tibble: 39 × 3\n   orig_name                                                        domain item \n   &lt;chr&gt;                                                            &lt;chr&gt;  &lt;chr&gt;\n 1 Helpfulness of generative AI for each use case  [Suggesting R c… Helpf… Sugg…\n 2 Helpfulness of generative AI for each use case  [Explaining R c… Helpf… Expl…\n 3 Helpfulness of generative AI for each use case  [Improving code… Helpf… Impr…\n 4 Helpfulness of generative AI for each use case  [Brainstorming … Helpf… Brai…\n 5 Helpfulness of generative AI for each use case  [‘Vibe code’: g… Helpf… ‘Vib…\n 6 Helpfulness of generative AI for each use case  [Data wrangling… Helpf… Data…\n 7 Helpfulness of generative AI for each use case  [Understanding … Helpf… Unde…\n 8 Helpfulness of generative AI for each use case  [Using RStudio … Helpf… Usin…\n 9 Impact on learning [AI made learning R easier.]                  Impac… AI m…\n10 Impact on learning [AI helped me understand R syntax and functi… Impac… AI h…\n# ℹ 29 more rows\n\n\n\n\nR code\n# Agreement levels and robust mapping function\nagree_levels &lt;- c(\"Strongly disagree\", \"Disagree\", \"Neutral\", \"Agree\", \"Strongly agree\")\n\nmap_to_agree &lt;- function(x) {\n  x &lt;- stringr::str_squish(as.character(x))\n  x_lower &lt;- tolower(x)\n  case_when(\n    # helpfulness -&gt; agreement mapping\n    x_lower %in% c(\"not at all helpful\", \"not helpful\", \"not at all helpful to not helpful\") ~ \"Strongly disagree\",\n    x_lower %in% c(\"slightly helpful\") ~ \"Disagree\",\n    x_lower %in% c(\"moderately helpful\") ~ \"Neutral\",\n    x_lower %in% c(\"very helpful\") ~ \"Agree\",\n    x_lower %in% c(\"extremely helpful\") ~ \"Strongly agree\",\n    # direct agreement variants\n    x_lower %in% c(\"strongly disagree\", \"strong disagree\") ~ \"Strongly disagree\",\n    x_lower %in% c(\"disagree\") ~ \"Disagree\",\n    x_lower %in% c(\"neutral\", \"neither agree nor disagree\", \"neither\") ~ \"Neutral\",\n    x_lower %in% c(\"agree\") ~ \"Agree\",\n    x_lower %in% c(\"strongly agree\", \"strong agree\") ~ \"Strongly agree\",\n    # already canonical\n    x %in% agree_levels ~ x,\n    TRUE ~ NA_character_\n  )\n}\n\n# pivot to long, join col_info, map responses, compute counts and percentages\nlong &lt;- raw_help %&gt;%\n  pivot_longer(cols = everything(), names_to = \"orig_name\", values_to = \"response_raw\") %&gt;%\n  left_join(col_info, by = \"orig_name\") %&gt;%\n  mutate(response_mapped = map_to_agree(response_raw),\n         response_mapped = factor(response_mapped, levels = agree_levels, ordered = TRUE)) %&gt;%\n  group_by(domain, item, response_mapped) %&gt;%\n  tally(name = \"n\") %&gt;%\n  group_by(domain, item) %&gt;%\n  mutate(pct = n / sum(n)) %&gt;%\n  ungroup()\n\n# report any unmapped response variants\nunmapped &lt;- raw_help %&gt;%\n  pivot_longer(cols = everything(), names_to = \"orig_name\", values_to = \"response_raw\") %&gt;%\n  distinct(response_raw) %&gt;%\n  mutate(mapped = map_to_agree(response_raw)) %&gt;%\n  filter(is.na(mapped))\n\nif (nrow(unmapped) &gt; 0) {\n  message(\"Unmapped response variants detected (consider adding mappings to map_to_agree):\")\n  print(unmapped$response_raw)\n} else {\n  message(\"No unmapped response variants detected based on current mapping.\")\n}\n\n\n[1] \"Stongly disagree\"\n\n\n\n\nR code\n# compute per-item positive proportion (Agree + Strongly agree) per domain\nitem_summary &lt;- long %&gt;%\n  filter(!is.na(response_mapped)) %&gt;%\n  group_by(domain, item) %&gt;%\n  summarize(total_n = sum(n, na.rm = TRUE),\n            positive_n = sum(n[response_mapped %in% c(\"Agree\", \"Strongly agree\")], na.rm = TRUE),\n            positive_pct = if_else(total_n &gt; 0, positive_n / total_n, 0),\n            .groups = \"drop\")\nitem_summary\n\n\n# A tibble: 39 × 5\n   domain                           item         total_n positive_n positive_pct\n   &lt;chr&gt;                            &lt;chr&gt;          &lt;int&gt;      &lt;int&gt;        &lt;dbl&gt;\n 1 \"\\\"Vibe code\\\" outcomes\"         AI-generate…      43         32        0.744\n 2 \"\\\"Vibe code\\\" outcomes\"         I could cri…      43         21        0.488\n 3 \"\\\"Vibe code\\\" outcomes\"         I understoo…      43         31        0.721\n 4 \"\\\"Vibe code\\\" outcomes\"         I would be …      43         27        0.628\n 5 \"Code quality outcomes using AI\" I relied on…      43         36        0.837\n 6 \"Code quality outcomes using AI\" My analyses…      43         35        0.814\n 7 \"Code quality outcomes using AI\" My code bec…      43         37        0.860\n 8 \"Code quality outcomes using AI\" My code bec…      43         34        0.791\n 9 \"Code quality outcomes using AI\" My code bec…      43         35        0.814\n10 \"Ethical alignment\"              AI use enco…      43         26        0.605\n# ℹ 29 more rows\n\n\n\n\nR code\n# Prepare per-domain data frames (only domains present)\ndomain_dfs &lt;- long %&gt;%\n  filter(!is.na(response_mapped)) %&gt;%\n  group_split(domain)\nnames(domain_dfs) &lt;- map_chr(domain_dfs, ~ unique(.x$domain))\n\n# keep only requested domains that actually exist in data\npresent_domains &lt;- intersect(domains, names(domain_dfs))\ndomain_dfs &lt;- domain_dfs[present_domains]\n\n# count items per domain for relative heights\nitem_counts &lt;- map_int(domain_dfs, ~ length(unique(.x$item)))\n\n# color palette and common y-limits\ncols &lt;- c(\"Strongly disagree\" = \"#d73027\",\n          \"Disagree\" = \"#fc8d59\",\n          \"Neutral\" = \"#fee090\",\n          \"Agree\" = \"#91bfdb\",\n          \"Strongly agree\" = \"#1a9850\")\ny_limits &lt;- c(0, 1)\n\n# Build one ggplot per domain. Ensure scale_fill includes all five categories and uses a one-row legend.\nplot_list &lt;- imap(domain_dfs, function(df_dom, dom_name) {\n  # determine ordering by positive_pct (descending), tie-break by total_n (descending)\n  item_order &lt;- item_summary %&gt;%\n    filter(domain == dom_name) %&gt;%\n    arrange(-desc(positive_pct), desc(total_n)) %&gt;%\n    pull(item)\n  if (length(item_order) == 0) item_order &lt;- unique(df_dom$item)\n\n  df_dom2 &lt;- df_dom %&gt;%\n    mutate(item = factor(item, levels = item_order, ordered = TRUE))\n\n  ggplot(df_dom2, aes(x = item, y = pct, fill = response_mapped)) +\n    geom_col(width = 0.75, color = NA) +\n    coord_flip() +\n    scale_y_continuous(labels = percent_format(accuracy = 1), limits = y_limits, expand = c(0,0)) +\n    scale_fill_manual(values = cols,\n                      breaks = agree_levels,\n                      guide = guide_legend(nrow = 1, byrow = TRUE),\n                      name = \"Response\") +\n    labs(title = dom_name, x = NULL, y = NULL) +\n    theme_minimal(base_size = 12) +\n    theme(plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),\n          axis.text.y = element_text(size = 10),\n          panel.grid.major.y = element_blank(),\n          plot.margin = margin(6,6,6,6),\n          legend.position = \"bottom\")   # keep legend present for collection\n})\n\n\n\n\nR code\n# Combine vertically with a single collected legend at bottom\ncombined &lt;- wrap_plots(plot_list, ncol = 1, guides = \"collect\") +\n  plot_layout(heights = item_counts, guides = \"collect\") & theme(legend.position = \"bottom\")\n\nprint(combined)\n\n\n\n\n\n\n\n\n\nSession info\n\n\n setting  value\n version  R version 4.4.1 (2024-06-14)\n os       Ubuntu 24.04.3 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2025-12-27\n pandoc   3.4 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/x86_64/ (via rmarkdown)\n quarto   1.6.42 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/quarto\n\n\nNotes - Items in each domain are ordered by positive_pct (Agree + Strongly agree) descending; ties broken by total responses. If you prefer a different tie-break or a manual ordering for a specific domain, tell me which domain and the exact item order. - The scale_fill_manual(breaks = agree_levels) ensures the legend shows all five response categories in the specified order. If a category has zero observations it will still appear in the legend. - Per-item bar thickness is kept consistent by assigning each domain a relative height equal to its item count in plot_layout(heights = item_counts). Adjust the heights multiplier if you want thicker/thinner bars (e.g., heights = item_counts * 1.2).\nIf you’d like, I can knit this Rmd for you and return the self-contained HTML file.\nBelow is a self-contained R chunk you can insert into your existing Rmd (after you read the CSV). It makes a single horizontal stacked bar — the same style & colors used before — showing the distribution of responses from the column\nDid you become more engaged or interactive in solving problems using AI?\nThe chunk: - maps engagement text to the five agreement-style levels (so it uses your existing color scheme), - computes proportions, - draws one horizontal stacked bar with the same legend/colors.\nInsert this after the code that defines df (or anywhere after the mapping functions are defined). If you want this as its own small plot file instead of inside the main Rmd, you can run the code in a separate R script — it is self-contained.\n\n\nR code\nlibrary(tidyverse)\nlibrary(scales)\n\neng_col &lt;- \"Did you become more engaged or interactive in solving problems using AI?\"\n\nif (!eng_col %in% names(df)) stop(paste0(\"Column not found: \", eng_col))\n\neng_raw &lt;- df %&gt;%\n  select(all_of(eng_col)) %&gt;%\n  rename(response_raw = all_of(eng_col)) %&gt;%\n  mutate(response_raw = stringr::str_squish(as.character(response_raw)))\n\n# mapping to agreement-style levels (adjust variants as needed)\nagree_levels &lt;- c(\"Strongly disagree\", \"Disagree\", \"Neutral\", \"Agree\", \"Strongly agree\")\neng_map &lt;- function(x) {\n  x_lower &lt;- tolower(x)\n  dplyr::case_when(\n    x_lower %in% c(\"much more engaged\", \"much more engaged \") ~ \"Strongly agree\",\n    x_lower %in% c(\"slightly more engaged\", \"slightly more engaged \") ~ \"Agree\",\n    x_lower %in% c(\"no change\", \"neither more nor less\", \"no change \") ~ \"Neutral\",\n    x_lower %in% c(\"slightly less engaged\", \"slightly less engaged \") ~ \"Disagree\",\n    x_lower %in% c(\"much less engaged\", \"much less engaged \") ~ \"Strongly disagree\",\n    x %in% agree_levels ~ x,\n    TRUE ~ NA_character_\n  )\n}\n\n# Count responses in character space to avoid factor/ordered factor joins\neng_counts &lt;- eng_raw %&gt;%\n  mutate(response_mapped = eng_map(response_raw)) %&gt;%   # keep as character for now\n  count(response_mapped, name = \"n\")\n\n# Ensure all five levels present (left join), then compute pct using a scalar total\neng_complete &lt;- tibble(response_mapped = agree_levels) %&gt;%\n  left_join(eng_counts, by = \"response_mapped\") %&gt;%\n  replace_na(list(n = 0)) %&gt;%\n  mutate(response_mapped = factor(response_mapped, levels = agree_levels))\n\ntotal_n &lt;- sum(eng_complete$n)\neng_complete &lt;- eng_complete %&gt;%\n  mutate(pct = if (total_n &gt; 0) n / total_n else 0)\n\n# color palette (matches your other plots)\ncols &lt;- c(\"Strongly disagree\" = \"#d73027\",\n          \"Disagree\" = \"#fc8d59\",\n          \"Neutral\" = \"#fee090\",\n          \"Agree\" = \"#91bfdb\",\n          \"Strongly agree\" = \"#1a9850\")\n\n# Single horizontal stacked bar (label as \"Engagement\")\np_eng &lt;- eng_complete %&gt;%\n  mutate(label = \"Engagement\") %&gt;%\n  ggplot(aes(x = label, y = pct, fill = response_mapped)) +\n  geom_col(width = 0.6, color = NA) +\n  coord_flip() +\n  scale_y_continuous(labels = percent_format(accuracy = 1), limits = c(0,1), expand = c(0,0)) +\n  scale_fill_manual(values = cols, breaks = agree_levels, name = \"Response\",\n                    guide = guide_legend(nrow = 1, byrow = TRUE)) +\n  labs(x = NULL, y = \"Proportion\", title = \"Did you become more engaged / interactive using AI?\") +\n  theme_minimal(base_size = 12) +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.title = element_text(size = 12, face = \"bold\"),\n        legend.position = \"bottom\")\n\nprint(p_eng)\n\n\n\n\n\n\n\n\n\n\n\nR code\nlibrary(tidyverse)\nlibrary(scales)\n\ncol_name &lt;- \"Prior coding experience\"\n\nif (!col_name %in% names(df)) stop(paste0(\"Column not found: \", col_name))\n\npc &lt;- df %&gt;%\n  transmute(resp = stringr::str_squish(as.character(.data[[col_name]]))) %&gt;%\n  mutate(resp = if_else(resp == \"\" | is.na(resp), \"&lt;missing&gt;\", resp)) %&gt;%\n  count(resp, name = \"n\") %&gt;%\n  arrange(desc(n)) %&gt;%\n  mutate(pct = n / sum(n),\n         resp = forcats::fct_reorder(resp, n))  # order by frequency\n\n# Plot: horizontal bars with count + percent at end of each bar\nggplot(pc, aes(x = resp, y = pct)) +\n  geom_col(fill = \"#4C78A8\", width = 0.6) +\n  coord_flip() +\n  scale_y_continuous(labels = percent_format(accuracy = 1), expand = expansion(mult = c(0, 0.12))) +\n  geom_text(aes(label = paste0(n, \" (\", scales::percent(pct, accuracy = 1), \")\")),\n            hjust = -0.02, size = 3.2) +\n  labs(title = \"Prior coding experience\",\n       x = NULL, y = \"Percentage of respondents\") +\n  theme_minimal(base_size = 12) +\n  theme(panel.grid.major.y = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nR code\nlibrary(tidyverse)\n\n# read data if df not already in memory\nif (!exists(\"df\")) df &lt;- readr::read_csv(\"course_survey_responses.csv\")\n\ncols &lt;- c(\n  \"Overall effectiveness of AI in this course\",\n  \"Likelihood to recommend AI use in similar courses\"\n)\n\nres &lt;- df %&gt;%\n  select(any_of(cols)) %&gt;%\n  pivot_longer(everything(), names_to = \"column\", values_to = \"value\") %&gt;%\n  mutate(value = as.numeric(value)) %&gt;%                # convert to numeric\n  group_by(column) %&gt;%\n  summarise(\n    mean = mean(value, na.rm = TRUE),\n    sd   = sd(value, na.rm = TRUE),\n    n    = sum(!is.na(value)),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(mean = round(mean, 2), sd = round(sd, 2))\n\nprint(res)\n\n\n# A tibble: 2 × 4\n  column                                             mean    sd     n\n  &lt;chr&gt;                                             &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 Likelihood to recommend AI use in similar courses  7.86  1.98    43\n2 Overall effectiveness of AI in this course         8.6   1.22    43"
  },
  {
    "objectID": "methods/survey_plots_v2.html",
    "href": "methods/survey_plots_v2.html",
    "title": "Graphs of AI survey results",
    "section": "",
    "text": "Main Likert graph\n\n\n\n\n\n\n\n\n\n\n\nPrior Coding Experience\n\n\n\n\n\n\n\n\n\n\n\nFrequency of AI Use\n\n\n\n\n\n\n\n\n\n\n\nHow much time did using AI save me?\n\n\n\n\n\n\n\n\n\n\n\nFrequency of incorrect AI output\n\n\n\n\n\n\n\n\n\n\n\nDid you become more engaged or interactive in solving problems using AI?\n\n\n\n\n\n\n\n\n\n\n\nConfidence change in programming in R\n\n\n\n\n\n\n\n\n\n\n\nClarity of instructions on AI use\n\n\n\n\n\n\n\n\n\n\n\nWhen AI was wrong, I typically…\n\n\n\n\n\n\n\n\n\n\n\nCommon issues encountered\n\n\n\n\n\n\n\n\n\n\n\nWhere more support would help\n\n\n\n\n\n\n\n\n\n\n\nTable - Overall effectiveness / Likelihood to recommend AI\n\n\n\n\n\ncolumn\nmean\nsd\nn\n\n\n\n\nLikelihood to recommend AI use in similar courses\n7.86\n1.98\n43\n\n\nOverall effectiveness of AI in this course\n8.60\n1.22\n43"
  },
  {
    "objectID": "methods/survey_plots_v3.html",
    "href": "methods/survey_plots_v3.html",
    "title": "Graphs of AI survey results",
    "section": "",
    "text": "Load libraries and read in survey\n\n\nPrior Coding Experience\n\n\n\n\n\n\n\n\n\n\n\nFrequency of AI Use\n\n\n\n\n\n\n\n\n\n\n\nHow much time did using AI save me?\n\n\n\n\n\n\n\n\n\n\n\nFrequency of incorrect AI output\n\n\n\n\n\n\n\n\n\n\n\nDid you become more engaged or interactive in solving problems using AI?\n\n\n\n\n\n\n\n\n\n\n\nConfidence change in programming in R\n\n\n\n\n\n\n\n\n\n\n\nClarity of instructions on AI use\n\n\n\n\n\n\n\n\n\n\n\nMain Likert graph\n\n\n\n\n\n\n\n\n\n\n\nWhen AI was wrong, I typically… - Barplot\n\n\n\n\n\n\n\n\n\n\n\nCommon issues encountered - Barplot\n\n\n\n\n\n\n\n\n\n\n\nWhere more support would help - Barplot\n\n\n\n\n\n\n\n\n\n\n\nTable - Overall effectiveness / Likelihood to recommend AI\n\n\n\n\n\ncolumn\nmean\nsd\nn\n\n\n\n\nLikelihood to recommend AI use in similar courses\n7.86\n1.98\n43\n\n\nOverall effectiveness of AI in this course\n8.60\n1.22\n43"
  },
  {
    "objectID": "methods/bio486_survey_plots.html",
    "href": "methods/bio486_survey_plots.html",
    "title": "Learning to Code in R with Generative AI - Survey Response Graphs",
    "section": "",
    "text": "Main Likert graph\n\n\n\n\n\n\n\n\n\n\n\nPrior Coding Experience\n\n\n\n\n\n\n\n\n\n\n\nFrequency of AI Use\n\n\n\n\n\n\n\n\n\n\n\nHow much time did using AI save me?\n\n\n\n\n\n\n\n\n\n\n\nFrequency of incorrect AI output\n\n\n\n\n\n\n\n\n\n\n\nDid you become more engaged or interactive in solving problems using AI?\n\n\n\n\n\n\n\n\n\n\n\nConfidence change in programming in R\n\n\n\n\n\n\n\n\n\n\n\nClarity of instructions on AI use\n\n\n\n\n\n\n\n\n\n\n\nWhen AI was wrong, I typically…\n\n\n\n\n\n\n\n\n\n\n\nCommon issues encountered\n\n\n\n\n\n\n\n\n\n\n\nWhere more support would help\n\n\n\n\n\n\n\n\n\n\n\nTable - Overall effectiveness / Likelihood to recommend AI\n\n\n\n\n\ncolumn\nmean\nsd\nn\n\n\n\n\nLikelihood to recommend AI use in similar courses\n7.86\n1.98\n43\n\n\nOverall effectiveness of AI in this course\n8.60\n1.22\n43"
  },
  {
    "objectID": "methods/survey_plots_old.html",
    "href": "methods/survey_plots_old.html",
    "title": "Graphs of AI survey results",
    "section": "",
    "text": "Load libraries and read in survey\n\n\nPrior Coding Experience - Barplot\n\n\n\n\n\n\n\n\n\n\n\nPrior Coding Experience - Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nFrequency of AI Use - Barplot\n\n\n\n\n\n\n\n\n\n\n\nFrequency of AI Use - Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nHow much time did using AI save me?- Barplot\n\n\n\n\n\n\n\n\n\n\n\nHow much time did using AI save me?- Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nFrequency of incorrect AI output - Barplot\n\n\n\n\n\n\n\n\n\n\n\nFrequency of incorrect AI output - Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nDid you become more engaged or interactive in solving problems using AI? - Barplot\n\n\n\n\n\n\n\n\n\n\n\nDid you become more engaged or interactive in solving problems using AI? - Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nConfidence change in programming in R - Barplot\n\n\n\n\n\n\n\n\n\n\n\nConfidence change in programming in R - Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nClarity of instructions on AI use - Barplot\n\n\n\n\n\n\n\n\n\n\n\nClarity of instructions on AI use - Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nMain Likert graph\n\n\n\n\n\n\n\n\n\n\n\nWhen AI was wrong, I typically… - Barplot\n\n\n\n\n\n\n\n\n\n\n\nCommon issues encountered - Barplot\n\n\n\n\n\n\n\n\n\n\n\nWhere more support would help - Barplot\n\n\n\n\n\n\n\n\n\n\n\nTable - Overall effectiveness / Likelihood to recommend AI\n\n\n\n\n\ncolumn\nmean\nsd\nn\n\n\n\n\nLikelihood to recommend AI use in similar courses\n7.86\n1.98\n43\n\n\nOverall effectiveness of AI in this course\n8.60\n1.22\n43"
  },
  {
    "objectID": "methods/bio486_survey_plots_fig1.html",
    "href": "methods/bio486_survey_plots_fig1.html",
    "title": "Learning to Code in R with Generative AI - Survey Response Graphs",
    "section": "",
    "text": "Main Likert graph"
  },
  {
    "objectID": "methods/bio478_survey_plots.html",
    "href": "methods/bio478_survey_plots.html",
    "title": "Learning to Code in R with Generative AI - Survey Response Graphs",
    "section": "",
    "text": "Main Likert graph\n\n\n\n\n\n\n\n\n\n\n\nPrior Coding Experience\n\n\n\n\n\n\n\n\n\n\n\nFrequency of AI Use\n\n\n\n\n\n\n\n\n\n\n\nHow much time did using AI save me?\n\n\n\n\n\n\n\n\n\n\n\nFrequency of incorrect AI output\n\n\n\n\n\n\n\n\n\n\n\nDid you become more engaged or interactive in solving problems using AI?\n\n\n\n\n\n\n\n\n\n\n\nConfidence change in programming in R\n\n\n\n\n\n\n\n\n\n\n\nClarity of instructions on AI use\n\n\n\n\n\n\n\n\n\n\n\nWhen AI was wrong, I typically…\n\n\n\n\n\n\n\n\n\n\n\nCommon issues encountered\n\n\n\n\n\n\n\n\n\n\n\nWhere more support would help\n\n\n\n\n\n\n\n\n\n\n\nTable - Overall effectiveness / Likelihood to recommend AI\n\n\n\n\n\ncolumn\nmean\nsd\nn\n\n\n\n\nLikelihood to recommend AI use in similar courses\n7.86\n1.98\n43\n\n\nOverall effectiveness of AI in this course\n8.60\n1.22\n43"
  },
  {
    "objectID": "methods/bio478_survey_plots_old.html",
    "href": "methods/bio478_survey_plots_old.html",
    "title": "Graphs of AI survey results",
    "section": "",
    "text": "Load libraries and read in survey\n\n\nPrior Coding Experience - Barplot\n\n\n\n\n\n\n\n\n\n\n\nPrior Coding Experience - Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nFrequency of AI Use - Barplot\n\n\n\n\n\n\n\n\n\n\n\nFrequency of AI Use - Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nHow much time did using AI save me?- Barplot\n\n\n\n\n\n\n\n\n\n\n\nHow much time did using AI save me?- Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nFrequency of incorrect AI output - Barplot\n\n\n\n\n\n\n\n\n\n\n\nFrequency of incorrect AI output - Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nDid you become more engaged or interactive in solving problems using AI? - Barplot\n\n\n\n\n\n\n\n\n\n\n\nDid you become more engaged or interactive in solving problems using AI? - Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nConfidence change in programming in R - Barplot\n\n\n\n\n\n\n\n\n\n\n\nConfidence change in programming in R - Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nClarity of instructions on AI use - Barplot\n\n\n\n\n\n\n\n\n\n\n\nClarity of instructions on AI use - Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nMain Likert graph\n\n\n\n\n\n\n\n\n\n\n\nWhen AI was wrong, I typically… - Barplot\n\n\n\n\n\n\n\n\n\n\n\nCommon issues encountered - Barplot\n\n\n\n\n\n\n\n\n\n\n\nWhere more support would help - Barplot\n\n\n\n\n\n\n\n\n\n\n\nTable - Overall effectiveness / Likelihood to recommend AI\n\n\n\n\n\ncolumn\nmean\nsd\nn\n\n\n\n\nLikelihood to recommend AI use in similar courses\n7.86\n1.98\n43\n\n\nOverall effectiveness of AI in this course\n8.60\n1.22\n43"
  },
  {
    "objectID": "methods/bio478_survey_plots_fig1.html",
    "href": "methods/bio478_survey_plots_fig1.html",
    "title": "Learning to Code in R with Generative AI - Survey Response Graphs",
    "section": "",
    "text": "Main Likert graph"
  },
  {
    "objectID": "survey/bio478_survey_plots.html",
    "href": "survey/bio478_survey_plots.html",
    "title": "Learning to Code in R with Generative AI - Survey Results",
    "section": "",
    "text": "Overview\nThese graphs, tables and open responses are from an anonymous Google forms survey was designed to solicit students’ personal experience on the effectiveness of using generative AI in this course. The resulting csv file with the survey results is in the course GitHub repo (https://github.com/jeffreyblanchard/PathoGen2025/tree/main/survey/bio478_survey_responses.csv). These results are generated from this csv file.\n\n\nPrior Coding Experience\n\n\n\n\n\n\n\n\n\n\n\nFrequency of AI Use\n\n\n\n\n\n\n\n\n\n\n\nHow much time did using AI save me?\n\n\n\n\n\n\n\n\n\n\n\nFrequency of incorrect AI output\n\n\n\n\n\n\n\n\n\n\n\nDid you become more engaged or interactive in solving problems using AI?\n\n\n\n\n\n\n\n\n\n\n\nConfidence change in programming in R\n\n\n\n\n\n\n\n\n\n\n\nClarity of instructions on AI use\n\n\n\n\n\n\n\n\n\n\n\nCombined likert graph of multiple choice grids\n\n\n\n\n\n\n\n\n\n\n\nWhen AI was wrong, I typically… - multiple response\n\n\n\n\n\n\n\n\n\n\n\nCommon issues encountered - multiple response\n\n\n\n\n\n\n\n\n\n\n\nWhere more support would help - multiple response\n\n\n\n\n\n\n\n\n\n\n\nExamples of helpful AI responses - Open response\n\n\nIt telling me how to create statistical analysis formulas to compare data\npoints, and how to map graphs\n\nSometime when I didn't know how to attack a problem it would take me through the\nsteps I needed to do before I wrote a code to answer the question like cleaning\nup or merging data\n\nBefore we learned the definitions of regex patterns and I tried to ask AI\nto create patterns for effective character finding, I used AI to give an\nexplanation of the reasoning and meaning of the regex patterns it generated\nEven before we formally learned the skill, AI helped me think of advanced regex\npatterns\n\nWriting code blocks utilizing functions/libraries I hadn't used before\n\nclarity on homework / studying\n\nGiving me different types of analysis I could do to see spatial autocorrelation\nfor the MAG and metadata dataset\n\nMaking sense of the outputs generated by the code Or clarifying what table\ncategories that have slightly different names from the instructions are most\nlikely to be what the instructions are trying to direct you to\n\nIt really helped a lot during the lab exercises and it also explains it too\n\nWhen working with the NEON Mags dataset, the AI gave very good suggestions on\nhow to go about formatting my data and what errors were preventing my code from\nrunning\n\nNA\n\nA lot of the time I asked AI to help explain certain coding functions or I would\nask them why I am getting the error message I am getting when rendering my lab\nand that's helped me learn from why I can't write the code in certain ways So\nAI has definitely helped me improve my coding syntax so that less errors come up\nwhen I render my labs\n\nAI was very good at telling me what an error message meant, provided that I gave\nthem the exact error message R was giving me AI would often explain what the\nerror message meant and then provide multiple common ways to solve this problem\n\nWhen an error occurs it will explain my error to me and give me code to correct\nit\n\n-producing helpful explanations when codes do not run, providing simple code\nfixes\n\nAI was able to come up analyses and graphs for the data that I never would have\nthought of It also provided the code for each analysis which was very helpful\nwhen trying to reproduce the graph in RStudio\n\nI frequently would give copilot what I said and the error code and it would then\nbold what it changes and why\n\nOnce I was supposed to run a code to make graphs in a certain way but I wasn't\nreally sure on how to do that AI helped me brainstorm ways to make my code run\nthe way I needed it to\n\nI asked it how to create a pie chart and it described it for me Another thing\nwas to make a graph that was interactive so when your mouse hovers over a point\nyou see what it is\n\nOne that integrated the suggested code in its response, and followed that with\nan explanation of why the code worked Another common one was when I reached an\nissue with one of my code chunks it would correct the issue and explain why my\ncode wasn't working\n\nGiving an example of the code and then explainging what each part does, making\nit easy to know what use/add to get the product i needed\n\nAI has been helpful in providing responses to fix my coding errors For example,\nI would tell CoPilot the code I was trying to run, and say it was not working\nand enter the specific \"Error:\" I was getting Copilot would then send back the\ncorrect code (most of the time), and explain where I went wrong which was really\nhelpful so that I would not make the same mistake again\n\nI was having trouble finding code to help me build a PCA plot for the MAG data\nset I upload the code that I already had as well as the MAG csv file and AI was\nable to give me a better code that also gave me instructions on what to change\nThat was very helpful and allowed me to see what I was doing wrong\n\nI usually used AI to correct errors within the code I created, so a helpful\nresponse would be something along the lines of \"I see the problem is ______\nwhich can be solved by _______\"\n\nit would give options on how to improve or fix the code and when taking a\nscreenshot of my code & the errors, it was almost always helpful with helping me\nfix the issues\n\nWhen I'd get an error, I'd give it to AI and often, it could tell me what I did\nwrong and how to fix it\n\nOften times I will just input my error messages into AI (specifically ChatGPT)\nand even without seeing my original code, it generates a successful fix for my\ncode Chat has been very successful in providing code fixes and formats on how to\ncode certain things\n\nI thought the most helpful Ai responses were the ones that gave me step by\nstep instructions for the type of code to generate depending on the prompt By\nbreaking it down, I could understand the point of each chunk\n\nPutting in error code from R would 85% of the time be resolvable through AI\ndirectly Otherwise I could ask for alternative methods to alter my code\n\nI would ask it to help me with something, and then it would output the code and\nthe reasoning behind the code and how it works\n\nIt helped me be able to make graphs and complete exercises It helped me\nunderstand what was going on in general And it always showed different ways to\nwrite the code\n\nHaving an idea on how you want data to be analyzed but not being sure of what\nkind of graphic would be best suited for that AI can suggest a few different\nmethods and write the bulk of the code required to set the graphic up in RStudio\n\nTroubleshooting issues was when I found AI most effective, rendering problems\nand trying to understand coding terms\n\nAll of the assignments like Making a graph for neon image data\n\nWhen I have difficulty running a code due to an error in it and I could not find\na way to make the code work so I will put the code and the feedback that R gave\nme on copilot and ask the AI to help me understand how to fix the code\n\nAnything I don’t know ai can help\n\nNot an exact example but the answers that included step by step explanations of\nthe code were helpful\n\nAI can help clean out a code chunk by replacing verbose patterns with\nsimpler, more readable alternatives — for example, instead of writing\nfilter(!isna(site_id), !isna(phylum)) %&gt;% count(site_id, phylum), AI might\nsuggest drop_na(site_id, phylum) %&gt;% count(site_id, phylum), which is shorter\nand easier to understand\n\nHere’s a specific example: my question to AI was: “what's the difference\nbetween your solution and what I wrote?” AI gave me a step by step explanation\nof (1) what my code does and (2) problems my code causes (failing to address\nthe problem) The specific problem I was working on was How to display a table\nthat counts the number of MAGs by phylum at each site AI was very helpful\nin all circumstances, especially in *Suggesting R code to solve problems*\nand *Explaining R code syntax and logic* Those were my two top uses of AI Of\ncourse, AI helped with all the other categories too, but for the most part, the\ntwo I just mentioned were the categories that I most often consulted AI for I\nfeel like the category *Data wrangling with MAG taxonomy + genome properties\n+ soil chemistry* can be included underneath either of these two, because it\nwas related to problem-solving The reason I put *Extremely helpful* for all\ncategories because of the ability to consult AI for really almost everything\nFor the two categories I mentioned above, these are the prompts I would ask\n“Here’s my code It seems to me that it works fine, but can you verify that it\nreally does?” Or, “What does this function do? What’s the difference between\nthis one and this one?” Or, “Here’s the problem I’m trying to solve, and here’s\nmy code so far Can you help me with my code (either troubleshoot this error\nI’m getting or show me whether this is the best way to approach the problem)”\nI took advantage of making the most out of AI by asking it to explain functions\nit used that I wasn’t familiar with I also always tried to understand AI logic\nby walking myself through each line, one by one The next two categories that\nI used AI for mostly in this course was *Brainstorming research questions with\nNEON metagenomic data* *‘Vibe code’: generating statistical, network, or other\nadvanced analyses* kind of follows or falls under this This is especially\nhelpful because I don’t have a research background and while I can ask simple\nquestions like “What phyla are enriched in these soil pH ranges?”, I still\nwould need to expand on this question My problem is that I’m not creative\n(maybe not curious enough?) with my questions This is why using AI to jumpstart\nbrainstorming is especially helpful, especially because it can quickly provide\nbackground information that might spark my curiosity\n\nWhat's wrong here? - “You are getting this error because joinID does not exist\nin the right table Check colnames() and rename before left_join()”\n\nAI fixing errors in my code\n\nProviding multiple approaches\n\nMost useful with chunks of code for statistical analysis and plots readibility\n\nNA\n\n\n\n\nExamples of unhelpful or incorrect AI responses - Open response\n\n\nWhen it would tell me to use formulas that needed only numerical values but I\nhad N/A in my values It would tell me to use random names or column types that\ndidn't exist\n\nSometimes if I had an error it would keep giving me incorrect codes to the point\nwhere nothing worked and I did'nt know how to tell fix this issue\n\nWhen reading the soil mags + soil chem csv file to create Vibe code, AI would\neither reveal that the file is too big to be read or it would pretend to read\nit Either way, it would generate code that assumed variables and column names,\nwhich were often incorrect\n\nMany AI responses generate code that contain errors\n\nusing AI for therapy\n\nThe codes it would give me would have a lot of errors, but this was fine because\nyou learn a lot by seeing where those mistakes are and how to fix them yourself\n\nWhen you're trying to fix a path directory error or use code it's giving you and\nyou don't realize the proper names of the files are wrong and thats the secret\nreason why nothing is working\n\nIt takes a couple of tries for copilot to understand what is being asked which\ncan be frustrating\n\nWhen it came to working with GitHub or other external R coding, AI was not\nable to give very helpful tips and was just reiterating the same response in\ndifferent wording\n\nNA\n\nSometimes when prompting AI with my questions, I've learned that you have to\ncraft the question very specifically in order for them to give generally what\nyou're looking for The times that I have found AI giving unhelpful responses is\nwhen I don't prompt it correctly and sometimes it will give me an answer/code\nthat uses a library or code function that we have not learned or not part of the\nlesson we are working on\n\nWhen combining data frames I found that using AI wasn't as helpful as just\nreferring back to the module AI would overcomplicate things and sometimes I\nwould lose some of my data\n\nAI isnt the best at helping with MAG data or anything in terms of\ntroubleshooting for working directory\n\nAI responses that don't provide a code or give super complicated unrunnable\ncodes\n\nAI is very prompt-specific, so if a prompt does not reflect exactly what the\nperson wants, it takes a lot of back and forth between the AI and the person to\nget a good result\n\nsometimes it gives wrong names for column\n\nSometimes when I would get an error in my code I was ask AI what I did wrong and\nhow I can fix It and most of the time it did help me but sometimes it would act\nup and not really give me proper guidance in what needed to be done\n\nAI telling me to install and load new libraries when I don't even need them\n\nIt would occasionally give me a code that wouldn't run or it wouldn't explain\nwhy a certain code wasn't working\n\ngiving just example code with no explanation\n\nThere have been times where Copilot was wrong I would have to work with it back\nand forth, and say things like \"No, that is not correct The error: is coming\nup\", etc However, after I explained more, it would show me the correct code\n(most of the time)\n\nIn the beginning of the class, using AI was a challenge because I did not know\nthe proper formatting or syntax I would as AI to give me code to transform a\ndata set but not really understand what was being done An example of this was me\nusing the \"merge\" function instead \"left join\"\n\nIf I used AI to generate new code, it would often assume file/column names When\nI input my edited code (with corrected file and column names), it would solve\nproblems in the code but still using the file names it originally created Not a\nterrible issue, just annoying\n\nsometimes it would make the code too long & complicated when it could really be\nmore concise & it became really challenging when trying to use Al to help with\nuploading thing like soilChem and definitely NEONmag\n\nSometimes, AI would suggest code that included words/phrases that weren't in the\ndata set, so I struggled to know what I should enter instead\n\nI was trying to join two data sets but the IDs in the data sets had slightly\ndifferent suffixes The suggestion that AI was giving me wasn't working in\nremoving the suffix but it kept going around in the same circle Eventually, AI\nwas able to make the join work but I am not sure how\n\nI found that some of the unhelpful/incorrect Ai responses typically had to do\nwith sometimes I felt like it did not listen to my requests or it would suggest\ntroubleshooting techniques that would not work\n\nWhen AI would try to overcomplicate analysis (that would not run) and make me\ndownload a bunch of new libraries for one prompt By the end of lab 13 I have\nlibrary(ggfortify), library(vegan), library(plotly), library(respirometry),\nlibrary(ggplot2), and library(corrplot)\n\nSometimes it would output code that did not do what I wanted it to do; then\nI would have to refine it and tell it to redo it until it got it right Also,\nsometimes the graphs or figures it would make would be cramped together and not\nlegible\n\nAI made some graphs that were not accurate or did not have a good set up Also\ncertain commands it was not good at using\n\nFor some of the more complicated data analysis there will be some lines where\nyou get an error and try to use AI to solve it However it can't solve it (maybe\ndue to package/R version incompatibility) and you end up going down a rabbit\nhole and wasting time\n\nWould over-complicate simple exercises, adding unnecessary steps\n\nIn lab S3 Connecting a Github repo site with a new RStudio project It kept\ngiving me wrong code to link and due to that the Lab was not loading\n\nWhen AI will give me different variations of the same code and it will still\nnot work even after going back and forth with it for 30 minutes It happened\nto me multiple times that I asked my classmates for help Another bad short\nexamples will be AI using outdated syntax, complex code for simple tax and\nmisinterpreting data types\n\nsome wrong solution making everything complicated, ai love to Change environment\nfor some reasones\n\nNot an exact example but the answers that misinterpreted the prompts or didn’t\nhave the full data sets weren’t helpful\n\nAn unhelpful AI response might be overly vague, like ‘Just fix the code\nsomehow’, which gives no actionable guidance Another incorrect response could\nconfidently suggest a function or method that doesn’t exist, leaving the user\nconfused and misled\n\nFor me, examples would be when the code simply doesn’t work Sometimes it’s due\nto base R issues or packages and version incompatibility—things like that I\ndon’t know any technical terms to describe what the issues are more specifically\nI also don’t have a good understanding of how computers work One specific\nexample is when I wanted to do an analysis of MAG data using a heat map based\non soil moisture My input was, “Here’s my code so far where I got the average\nsoil temperature for each phyla How do I make a heat map from here?” AI gave\nme a solution where I had to convert my data frame to a matrix first The code\ndidn’t work from there due to some issues with creating the matrix I believe I\ncould have troubleshooted some more, as well as learn more about matrices on my\nown, but in that moment I really just wanted a quick visual and wasn’t willing\nto spend time to learn about matrices I think the only issue with AI feedback\nhere is that my code is nuanced to the data I’m working with and AI doesn’t\nget to have the full picture I admit I could have been less lazy here and try\ntroubleshooting it myself!\n\n“Here is an answer” with facts that are wrong or made up (hallucinating)\n\nAI going in the wrong direction about my code\n\nMisunderstanding the prompt, hallucinations\n\nHardest to solve problems If something did not work or wrong answer, needed to\nfigure it out mostly on my own, AI hardly could fix it\n\nNA\n\n\n\n\nSuggested changes for future offering - Open response\n\n\nMore help on github pushing and just using github as a whole\n\nI think AI helped my learning expirence I would just add more to debugging and\nvarifying AI like how to maybe do a monday class on this instead of a lab\n\nI definitely think that lab reports should require students to report the prompt\nthat was used to generate any code they used, preceding an exercise I think this\nis important data for this class and can be used to think of effective prompting\nstrategies One idea I had, is to use Monday discussions to do either class-wide\nor team/table debriefing in which we talk about what sort of code we are getting\nfrom AI I think a collaborative aspect in this class would be helpful for\nkeeping students on track together\n\nThe problems often felt formatted/designed in a way where AI use could easily\nsolve them without requiring critical thinking from the student I think problems\nthat focus more on having students determine what steps (or generated code\nchunks) would need to be taken could help the course encourage greater learning\n\nNA\n\nCovering more on ethics and AI use\n\nNA\n\nI think because there is a Monday and Wednesday portion, The Monday portion\nshould be spent explaining what the lab on Wednesday will be I found myself\nusing AI a lot for things that I felt could’ve been discussed in class\n\nSome assignments that focus more on the content of the course (ie assignments\nthat teach about NEON and soil chemistry/make up and how the research is\nadvancing) and where AI is not used\n\nNA\n\nI think when starting with AI sometimes it's easy to just copy and paste the\nquestion we're asked, but I think it would be helpful to start off with examples\nof how we can prompt AI to guide us in answering our problems\n\nNA\n\nn/a\n\nI would not say change anything but maybe more AI use towards the end of a\nspecific subject so students have time to actually learn about the topic (ex No\nAI use allowed monday(introduxe topic), AI use allowed in wednesday labs\n\nIncorporating learning R code without AI first would help give students a\nbaseline of what R code should look and function like without having to rely on\nAI too much\n\nmaybe not allow AI until a little further into the course and making the\nassignments not so full so that we could focus on learning rather than just\ngetting all the assignments and examples and then exercises done\n\nUsing AI and giving it some background before asking questions so it has the\nknowledge to help instead of pulling information from places that are irreverent\nto the person using it\n\nNothing right now\n\nThe class relied too heavily on AI I think it would have been helpful to have\nthe first couple of weeks without AI so we could learn the basics of R before\ndelving into more complicated analysis with AI Also I felt that I didn't learn\nhow to properly cite AI sources with respect to a coding assignment\n\nNA\n\nI would say there is not much to change, I think AI definitely brought my\nabilities of working with R to the next level The only thing I'd recommend is\ngoing over how to verify the AI outputs because sometimes I would get the wrong\ncode or have to install packages that we do not need for this class\n\nI would think it would be great if we could get to the MAG data earlier in the\nclass and learn more about different metagenomic analyses\n\nI think it was sometimes confusing to know exactly how to cite AI in the course,\nso more clarity on when/how it should be used in the earlier labs would be\nhelpful\n\nI know students will use Al to help them in this course in one way or another\nbut if there's a way to help improve learning I know many of us had no\nbackground use of coding at all and we kind of jumped in quickly so I would\ndepend on Al a lot to simply just get my work done\n\nNA\n\nAI skyrocketed my coding capabilities However, I don't feel very confident in\ncoding without AI This class would benefit from spending more time on ingraining\nR/coding basics before moving to advanced R coding and metagenomic analysis\nMaybe the first half of the course could be ingraining both the basics in code\nand metagenomic analysis and the second half could be the explosive, exciting,\nAI based coding and analysis Using AI was a world-opening experience for me in\ncoding\n\nN/A\n\nAsk people to discuss there code, or allow for people to do independent projects\nI really enjoyed the last lab that was open ended If we did the end of year\nproject I think that I would have been able to utilize all the skills that I\nlearned\n\nI would suggest more instruction for how to ask AI questions to get the results\nyou want Also, more help to verify if the results that the AI produced is\nactually what you want / answers the question how you wanted it answered Also,\nhow to make AI create more legible graphs where the labels or axes are not all\nclumped together so much\n\nMore help with learning how to write code on your own before working AI into the\ncourse I feel like maybe the first few classes should be without AI and trying\nto do things on your own and the bring AI in to help make more advanced code\n\nUsing some measure to prevent over reliance on AI to write all the code without\nunderstanding it Maybe having fewer exercises per lab, but requiring students to\nannotate their code and explain what was accomplished by the finished code\n\nNA\n\nMore activities in the beginning of the course\n\nI had hoped that the class would be more focused on the NEON Lab data and allow\nmore time to work with it We only engaged with the dataset for the last 2 weeks\nof classes And while the previous R sessions did build foundational skills\nthat definitely helped in analyzing the NEON Lab data, it would have been more\nbeneficial to introduce and work with the data earlier during the course Because\nfor me, doing so could have helped with better understanding the value of using\ncoding tools for biological data\n\nNA\n\nHave a class specifically for how to ask copilot for help with R coding,\nincluding how to write a helpful prompt and dissect the answers so that you are\nable to use them in your actual code\n\nNA\n\nI feel like *Debugging with AI* is the most common issue I experience when\nusing AI for code because what we are working on can be so nuanced Because of\nthe nuance, I’m not sure how feasible in-class support with AI debugging would\nbe (in the context of a whole class period), especially if everyone is working\non different analyses / project? I would say the same thing for *Verifying AI\noutput* I think AI is a useful tool used for assistance, but I also found that\nwhat we did in the course has been very valuable— namely doing the chapter\noutlines They can take quite a while to get through, but I agree that it’s\nimportant for us to know the basics of R ourselves For Advanced analyses,\nmaybe it would be helpful to have a brief overview in class of things like\nalpha-diversity and beta-diversity I learned about this before in other classes\n(and don’t remember what they are), so maybe formally going over it in class,\nif only briefly, would be good Maybe taking an additional lecture to explain\nmore of what MAGs are and why they’re useful in research, since I didn’t get it\nthe first time Eventually I came to understand what they are Also, personally\nI don’t know anything about analyzing data, so if we received some advice about\nthings to pay attention to (for example, make sure you take a larger sample\nsize Or any research principles we aren’t aware of? And what is ANOVA test, if\nwe happened to need to use it) from class, just briefly, that would help As for\nusing AI to help generate code for Advanced analyses, it helped so much Perhaps\none thing I think would be good for this course is to require or at least\nhighly encourage students to explain what their code chunk does, and put that\nexplanation in the Quarto document above the code chunk Or to put comments in\ncode in cases where the code looks suspiciously flawless We sometimes do these\nthings by putting the header “AI note” and “Code chunk”, but I wonder whether it\nwould be good for students to go further than minimally stating what prompt they\nasked AI Either way, I suppose a student’s ability to gain some useful knowledge\nis up to how disciplined they are able to be when they have powerful resources\nlike AI\n\nN/A\n\nNA\n\nNA\n\nI think it's like a calculator, I would not use it for anything else It works\nless well than google for research, but it works as expected when given complex\nprompts You may need to amen/check the equation yourself for sure, but it will\nbe able to give you a decent starting point/do the easy stuff faster than you\n\nNA\n\n\n\n\nFinal comments and suggestions - Open response\n\n\nAi really did help and I feel like it is needed for advanced code based on the\nlevel of most students in this class, it just made it so coding felt robotic and\nI learned a little less\n\nI liked being able to use AI but I wish I knew what to do if I fall down a\nrabbit hole of incorrect codes with CoPilot\n\nYour suggestions of introducing the \"Vibe\" code mini labs earlier in the\nsemester is interesting to me I think it would be helpful in one way, as it sort\nof shows students our final goals for this class and can give ideas on what AI\ncan be used to achieve On the other hand, it might introduce advanced code too\nquickly and can make students feel lost or left behind\n\nNA\n\nNA\n\nVery well structured, I learned a lot\n\nNA\n\nNA\n\nNA\n\nNA\n\nI think the integration of AI is helpful in beginner courses as sometimes AI\nhas a good way to \"dumb\" things down almost to explain why the code produces the\noutput that it does Sometimes having a conversation with AI helps the learner\nunderstand a lot more than just reading the textbook because sometimes we need\nmore examples than just the ones provided to see what the concept is trying to\nshow us\n\nNA\n\nhelped me feel like i could confidently code if asked to\n\nLoved this class, Dr Blanchard was super helpful and made coding seem fun even\nthough I have never liked to code Love AI, makes sense for coding courses do not\nget rid of it completely\n\nI enjoyed this class and performed analyses I never would have thought of\nwithout AI suggestions I would have liked to learn coding without AI first to\nestablish a foundation, but ultimately, using AI allowed me to complete a lot\nmore work and understand more functions in R code\n\nI would not have been able to complete this class without the assistance of AI\nand if you want to continue to have a large workload AI is 100% necessary and is\nhelpful It is more on the individual student themselves to actually make sure we\nlearn the content rather than just copy and pasting\n\nWith zero knowledge of coding when coming into this class, AI helped me learn\nthings along the way and made my life so much easier\n\nNo comments\n\nGood course to introduce someone to the use of AI in R, but would have been\nhelpful/interesting to learn some of the basics beforehand\n\nWithout the help of AI, I don't think I would've been able to complete this\ncourse at all, honestly Coding is just extremely hard for me, and using Copilot\nreally helped to break it down into manageable chunks to learn/use\n\nThis has been a great semester! This class was super interesting!\n\nThanks for a great semester!\n\nI feel like the effectiveness of AI in this course varies between students, so\nthis survey is very interesting! I could see AI inhibiting learning if students\nused it heavily in the beginning of the course when we focused on R basics,\nbecause then that work is just busy work I feel like I learned the most from the\nbeginning of the course just using AI as an occasional troubleshooting tool, but\ndepending on AI from the start would have been detrimental\n\nNA\n\nAI helped me understand and learn a lot more in this course than I could have\notherwise without it!\n\nSee suggested changes for future offerings\n\nThis was a very cool class!\n\nUsing on AI was appropriate in this course, but I don't think that it would be\neffective in every course I attached my mycology class analysis in the comments\nif you are interested in the analysis I did I presented the information to my\nclass :) I really enjoyed the skills that I learned in this class and will be\nadding to my resume! Thank you for an amazing semester\n\nI wish there was less of a need to use AI in this course, but once things\nstarted getting complicated it was very hard to do anything without the use of\nAI, especially considering most of us in the class had no experience with coding\nof any sort For example, I did not use AI for the first couple of labs, and that\nwas on purpose because I did not want to have to rely on AI for everything I did\nThe first couple of labs were manageable to do without the use of AI, and they\nwere just challenging enough where I was actually learning a lot of the basics\nof coding However, after a certain point, I felt I needed to use AI to get the\nwork done because the level of difficulty increased too much for me to figure\nout the coding on my own After that point, I relied on AI too much, and I wasn't\nlearning anymore\n\nI really did enjoy the course overall, I like the room and set up of working\nwith a group I think Dr Blanchard was helpful when I went to office hours or\nneeded help in class I wish we learned a few more basic coding steps because\nI think I relied on AI too much I still think it should be a big part of the\ncourse but just that a little more coding without it at the start\n\nI think AI has its place in the modern world but that place is not everywhere I\nunderstand the potential AI has for making coding more accessible for newcomers\nbut it should not eliminate the learning process A balance needs to be found\nbetween AI use and organic learning\n\nWithout AI for this course, I doubt I would have been able to do the labs on my\nown I think the course name should be changed to \"Coding with R\" or something of\na similar nature, because that is what the assignments were, going through the\ntextbook While it was beneficial to get some exposure to R, it was not what I\nexpected at all If I knew what it would be like beforehand, I would never have\ntaken this course The time spent trying to resolve issues with no luck from AI\nand other sources added way too many hours into completing assignments I don't\nenjoy using AI and try to avoid it, but unfortunately with the way the class was\nformatted, I felt that there was no way I could have figured out the issues I\nhad or solved the exercises without it\n\nAI really helped understanding the coding\n\nUsing AI made me more comfortable with coding, and I am actually applying what I\nlearned to one of my classes for a project\n\nNA\n\nNone\n\nNA\n\nUsing AI is extremely helpful in a coding class especially for students like\nme without much familiarity with code I wish more coding classes were offered\nmore frequently in the Biology Department R programming is such a useful skill,\neven if I’m a beginner, and I will certainly be using it to make visuals in the\nfuture I even saw my classmate from here using R to make graphs for project in\na different course Since this is a research-based course and we have limited\ntime this semester (the semester is very short, only 3-4 months), I loved\nthat we gain hands-on, first-hand experience of working with real MAG data and\ndoing analyses with it Doing the chapter outlines from the textbook was very\nhelpful in providing us with foundational knowledge about code syntax and using\ndplyr and ggplot for visualization and analysis I like learning foundational\nprinciples of coding from a concrete source like a textbook, compared to\nself-teaching myself through google At the same time, I still think having\nresources like internet and AI are such a big help because they introduce me\nto so much I would’t have explored (like other functions, more example uses,\ngraph aesthetics, research ideas) And I’m sure that self-teaching is still\nan important part of learning to code and research, too (though I don’t like\nthinking of the idea so much) I found that AI saved so much time when it came to\nproblem-solving, troubleshooting errors, and me memorizing functions That can be\na pain when working on coding assignments, which I’ve experienced before I think\nit’s important to responsibly use AI as a tool to help with my understanding,\nbut not just use it to get the assignment done That’s why I don’t just use AI to\nask for code suggestions, but also for explanation for every function use I also\ntake a look at how the code was structured to understand the problem-solving\nlogic I think that goes back to how we emphasized vibe coding— you should be\nable to know how to approach the problem, even if you can’t write the code just\nyet I have one comment about why students asked less questions to the instructor\nabout the actual code itself and problem-solving (although maybe there were\nstill questions about debugging?) I am sure the reason is obviously having the\nassistance of AI, which is a big help with coding But I think the underlying\nreason could be that most students hope to avoid asking a question and then\nbeing told to look into it themselves and check their resources they already\nhave I hear instructors say this so often, even in my other research-based\nclass I’m taking this semester Personally, this is a main reason I refrain from\nasking questions, because given enough time and (sometimes hard) effort on my\nown, I can answer my own question I just have to look for the answers myself on\ngoogle or AI Most of the questions I want to ask are ones that I could answer\ngiven time and effort on my own I feel like instructors may think I am lazy if\nI don’t try to first figure it out Also, another reason is that students are\nconfused and want to ask questions, but don’t know what exactly to ask or how\nto phrase it I’ve heard a classmate say this in another course, and I feel the\nsame way sometimes It’s harder when there are technical terms involved because\nI don’t know how to describe what I’m confused about Overall, I personally\ndon’t find either of these things to be huge issues in this course, as you were\nalways willing to help us and were a very encouraging instructor I never felt\npressured in a negative way Given our resources, it made sense for me to try to\nat least try to problem solve myself, which often resulted in me figuring out\nan answer…thanks to my resources, like AI! Overall, I’m not sure what changes I\nwould suggest for this course, especially given our short semester The important\nthing was that I learned useful programming skills that I know I will apply in\nthe future Thank you Dr Blanchard!\n\nThank you :)\n\nAI removes the need to spend hours learning coding and allows me to focus on\nmore important things\n\nNA\n\nI genuinely think the point of AI is like computers and calculators, make\ncomplex work easier I just think we should not confuse a glorified calculator\nwith a friend, God's word, or an actual intelligent person (Cogito, ergo sum)\nEven if it was an intelligent person, I would check their words and answers if\nI suspected it sounded too good to be true (that's probably because by nature,\nI also have a tendency to say things with a lot of confidence even if they are\nnot true—simply because I believe them) If someone came to me with the total\nconfidence that there are three types of T cells, I would know better and tell\nthem \"that's not true\" If they told me \"a new type of IgG was discovered in\na paper by Matt et al,\" I'd have a tendency to say \"yes? Can you show me?\" If\nI was told \"Did you know there are over 300 types of turtles?\" I may have a\nsmall doubt, but usually say \"oh nice!\" and accept it as a doubtful, but actual\nfact (and mess up and repeat it with total confidence to other people as a\ntruth) Asking any question to AI comes with the understanding that it is only a\ncomputer and just does math But even if it could think and was human, wouldn't\nwe also double-check their statement and thoughts? Or correct them, or ask for\nproofs?\n\nNA\n\n\n\n\nTable - Overall effectiveness / Likelihood to recommend AI\n\n\n\n\n\ncolumn\nmean\nsd\nn\n\n\n\n\nLikelihood to recommend AI use in similar courses\n7.86\n1.98\n43\n\n\nOverall effectiveness of AI in this course\n8.60\n1.22\n43"
  },
  {
    "objectID": "survey/bio478_survey_plots_old.html",
    "href": "survey/bio478_survey_plots_old.html",
    "title": "Graphs of AI survey results",
    "section": "",
    "text": "Load libraries and read in survey\n\n\nPrior Coding Experience - Barplot\n\n\n\n\n\n\n\n\n\n\n\nPrior Coding Experience - Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nFrequency of AI Use - Barplot\n\n\n\n\n\n\n\n\n\n\n\nFrequency of AI Use - Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nHow much time did using AI save me?- Barplot\n\n\n\n\n\n\n\n\n\n\n\nHow much time did using AI save me?- Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nFrequency of incorrect AI output - Barplot\n\n\n\n\n\n\n\n\n\n\n\nFrequency of incorrect AI output - Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nDid you become more engaged or interactive in solving problems using AI? - Barplot\n\n\n\n\n\n\n\n\n\n\n\nDid you become more engaged or interactive in solving problems using AI? - Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nConfidence change in programming in R - Barplot\n\n\n\n\n\n\n\n\n\n\n\nConfidence change in programming in R - Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nClarity of instructions on AI use - Barplot\n\n\n\n\n\n\n\n\n\n\n\nClarity of instructions on AI use - Single stacked bar\n\n\n\n\n\n\n\n\n\n\n\nMain Likert graph\n\n\n\n\n\n\n\n\n\n\n\nWhen AI was wrong, I typically… - Barplot\n\n\n\n\n\n\n\n\n\n\n\nCommon issues encountered - Barplot\n\n\n\n\n\n\n\n\n\n\n\nWhere more support would help - Barplot\n\n\n\n\n\n\n\n\n\n\n\nTable - Overall effectiveness / Likelihood to recommend AI\n\n\n\n\n\ncolumn\nmean\nsd\nn\n\n\n\n\nLikelihood to recommend AI use in similar courses\n7.86\n1.98\n43\n\n\nOverall effectiveness of AI in this course\n8.60\n1.22\n43"
  },
  {
    "objectID": "survey/bio478_survey_plots_fig1.html",
    "href": "survey/bio478_survey_plots_fig1.html",
    "title": "Learning to Code in R with Generative AI - Survey Response Graphs",
    "section": "",
    "text": "Main Likert graph"
  },
  {
    "objectID": "survey.html",
    "href": "survey.html",
    "title": "Learning R with Generative AI in a Metagenomics Data Science Course - Survey Results",
    "section": "",
    "text": "R code\nlibrary(tidyverse) \nlibrary(janitor)\nlibrary(scales)\nlibrary(patchwork)   \nlibrary(sessioninfo)\nlibrary(knitr) # this is needed for rendering\n\n\n\nOverview\nThese graphs, tables and open responses are from an anonymous Google forms survey that was designed to solicit students’ personal experience on the effectiveness of using generative AI in this course. The resulting csv file with the survey results is in the course GitHub repository. These results are generated from this csv file and the underlying R code can also be found in the above GitHub repo and are rendered on this survey page. The R code for producing many of these graphs was developed interactively using the UMass Amherst GenAI platform.\n\n\nPrior Coding Experience\n\n\nR code\n# 5 color likert palette for single stacked bar plots\ncols5 &lt;- c(\"#d73027\",\"#fc8d59\",\"#fee090\",\"#91bfdb\",\"#1a9850\")\n\n\n\n\nR code\n# Read CSV for single stacked bar plots\ndf_survey &lt;- readr::read_csv(\"survey/bio478_survey_responses.csv\") |&gt;\n  clean_names() \n\n\n\n\nR code\nexperience_levels = c(\n\"Extensive (multiple courses or projects)\",\n\"Moderate (regular use in another language)\",\n\"Some (one course or self-study)\",\n\"Very limited (a few tutorials or labs)\",\n\"None\"\n)\n\ndf_survey |&gt;\n  count(prior_coding_experience, name = \"n\") |&gt;\n  complete(prior_coding_experience = experience_levels, fill = list(n = 0)) |&gt;\n  mutate(prior_coding_experience = factor(prior_coding_experience, levels = experience_levels)) |&gt;\n  mutate(pct = n / sum(n)) |&gt;\n\nggplot(aes(x = 1, y = n, fill = prior_coding_experience)) +\n  geom_col(position = \"fill\") +          \n  scale_fill_manual(values = cols5) +\n  coord_flip() +                                                    \n  scale_y_continuous(labels = percent_format(accuracy = 1),\n                     expand = expansion(mult = c(0, 0.05))) +\n  labs(title = \"Prior coding experience\",\n       x = NULL, y = \"Percentage of respondents\", fill = NULL) +\n  theme_minimal(base_size = 12) +\n  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(),\n        panel.grid.major.y = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nFrequency of AI Use\n\n\nR code\nexperience_levels &lt;- c(\n\"I did not use AI\",\n\"I tried it once or twice\",\n\"About once per week\",\n\"A few times per week\",\n\"Daily\"\n)\n\ndf_survey |&gt;\n  count(frequency_of_ai_use_in_this_course, name = \"n\") |&gt;\n  complete(frequency_of_ai_use_in_this_course = experience_levels, fill = list(n = 0))  |&gt;\n  mutate(frequency_of_ai_use_in_this_course = factor(frequency_of_ai_use_in_this_course, levels = experience_levels))  |&gt;\n  mutate(pct = n / sum(n)) |&gt;\n\nggplot(aes(x = 1, y = n, fill = frequency_of_ai_use_in_this_course)) +\n  geom_col(position = \"fill\") +          \n  scale_fill_manual(values = cols5) +\n  coord_flip() +                                                   \n  scale_y_continuous(labels = percent_format(accuracy = 1),\n                     expand = expansion(mult = c(0, 0.05))) +\n  labs(title = \"Frequency of AI Use\",\n       x = NULL, y = \"Percentage of respondents\", fill = NULL) +\n  theme_minimal(base_size = 12) +\n  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(),\n        panel.grid.major.y = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nHow much time did using AI save me?\n\n\nR code\nexperience_levels &lt;- c(\n\"&lt; 0.5 hr total\",\n\"0.5-1 hrs\",\n\"1-2 hrs\",\n\"2-3 hrs\",\n\"3+ hrs\"\n)\n\ndf_survey |&gt;\n  count(how_much_time_did_using_ai_save_me, name = \"n\") |&gt;\n  complete(how_much_time_did_using_ai_save_me = experience_levels, fill = list(n = 0))  |&gt;\n  mutate(how_much_time_did_using_ai_save_me = factor(how_much_time_did_using_ai_save_me, levels = experience_levels))  |&gt;\n  mutate(pct = n / sum(n)) |&gt;\n\nggplot(aes(x = 1, y = n, fill = how_much_time_did_using_ai_save_me)) +\n  geom_col(position = \"fill\") +          \n  scale_fill_manual(values = cols5) +\n# stacked to 100%\n  coord_flip() +                                                     # horizontal bar\n  scale_y_continuous(labels = percent_format(accuracy = 1),\n                     expand = expansion(mult = c(0, 0.05))) +\n  labs(title = \"How much time did using AI save me?\",\n       x = NULL, y = \"Percentage of respondents\", fill = NULL) +\n  theme_minimal(base_size = 12) +\n  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(),\n        panel.grid.major.y = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nFrequency of incorrect AI output\n\n\nR code\nexperience_levels &lt;- c(\n\"Never\",\n\"Rarely\",\n\"Sometimes\",\n\"Often\",\n\"Very Often\"\n)\n\ndf_survey |&gt;\n  count(frequency_of_incorrect_ai_output, name = \"n\") |&gt;\n  complete(frequency_of_incorrect_ai_output = experience_levels, fill = list(n = 0))  |&gt;\n  mutate(frequency_of_incorrect_ai_output = factor(frequency_of_incorrect_ai_output, levels = experience_levels))  |&gt;\nmutate(pct = n / sum(n)) |&gt;\n\nggplot(aes(x = 1, y = n, fill = frequency_of_incorrect_ai_output)) +\n  geom_col(position = \"fill\") +          \n  scale_fill_manual(values = cols5) +\n# stacked to 100%\n  coord_flip() +                                                     # horizontal bar\n  scale_y_continuous(labels = percent_format(accuracy = 1),\n                     expand = expansion(mult = c(0, 0.05))) +\n  labs(title = \"Frequency of incorrect AI output\",\n       x = NULL, y = \"Percentage of respondents\", fill = NULL) +\n  theme_minimal(base_size = 12) +\n  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(),\n        panel.grid.major.y = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nDid you become more engaged or interactive in solving problems using AI?\n\n\nR code\nexperience_levels &lt;- c(\n\"Much less engaged\",\n\"Slightly less engaged\",\n\"No change\",\n\"Slightly more engaged\",\n\"Much more engaged\"\n)\n\ndf_survey |&gt;\n  count(did_you_become_more_engaged_or_interactive_in_solving_problems_using_ai, name = \"n\") |&gt;\n  complete(did_you_become_more_engaged_or_interactive_in_solving_problems_using_ai = experience_levels, fill = list(n = 0))  |&gt;\n  mutate(did_you_become_more_engaged_or_interactive_in_solving_problems_using_ai = factor(did_you_become_more_engaged_or_interactive_in_solving_problems_using_ai, levels = experience_levels))  |&gt;\nmutate(pct = n / sum(n)) |&gt;\n\n  ggplot(aes(x = 1, y = n, fill = did_you_become_more_engaged_or_interactive_in_solving_problems_using_ai)) +\n  geom_col(position = \"fill\") +          \n  scale_fill_manual(values = cols5) +\n# stacked to 100%\n  coord_flip() +                                                     # horizontal bar\n  scale_y_continuous(labels = percent_format(accuracy = 1),\n                     expand = expansion(mult = c(0, 0.05))) +\n  labs(title = \"Did you become more engaged or interactive in solving problems using AI?\",\n       x = NULL, y = \"Percentage of respondents\", fill = NULL) +\n  theme_minimal(base_size = 12) +\n  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(),\n        panel.grid.major.y = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nConfidence change in programming in R\n\n\nR code\nexperience_levels &lt;- c(\n\"Much lower\",\n\"Lower\",\n\"About the same\",\n\"Higher\",\n\"Much higher\"\n)\n\ndf_survey |&gt;\n  count(confidence_change_in_programming_in_r, name = \"n\") |&gt;\n  complete(confidence_change_in_programming_in_r = experience_levels, fill = list(n = 0))  |&gt;\n  mutate(confidence_change_in_programming_in_r = factor(confidence_change_in_programming_in_r, levels = experience_levels))  |&gt;\nmutate(pct = n / sum(n)) |&gt;\n\n  ggplot(aes(x = 1, y = n, fill = confidence_change_in_programming_in_r)) +\n  geom_col(position = \"fill\") +          \n  scale_fill_manual(values = cols5) +\n# stacked to 100%\n  coord_flip() +                                                     # horizontal bar\n  scale_y_continuous(labels = percent_format(accuracy = 1),\n                     expand = expansion(mult = c(0, 0.05))) +\n  labs(title = \"Confidence change in programming in R\",\n       x = NULL, y = \"Percentage of respondents\", fill = NULL) +\n  theme_minimal(base_size = 12) +\n  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(),\n        panel.grid.major.y = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nClarity of instructions on AI use\n\n\nR code\nexperience_levels &lt;- c(\n\"Very unclear\",\n\"Unclear\",\n\"Neutral\",\n\"Clear\",\n\"Very clear\"\n)\n\ndf_survey |&gt;\n  count(clarity_of_instructions_on_ai_use, name = \"n\") |&gt;\n  complete(clarity_of_instructions_on_ai_use = experience_levels, fill = list(n = 0))  |&gt;\n  mutate(clarity_of_instructions_on_ai_use = factor(clarity_of_instructions_on_ai_use, levels = experience_levels))  |&gt;\nmutate(pct = n / sum(n)) |&gt;\n\n    ggplot(aes(x = 1, y = n, fill = clarity_of_instructions_on_ai_use)) +\n  geom_col(position = \"fill\") +          \n  scale_fill_manual(values = cols5) +\n# stacked to 100%\n  coord_flip() +                                                     # horizontal bar\n  scale_y_continuous(labels = percent_format(accuracy = 1),\n                     expand = expansion(mult = c(0, 0.05))) +\n  labs(title = \"Clarity of instructions on AI use\",\n       x = NULL, y = \"Percentage of respondents\", fill = NULL) +\n  theme_minimal(base_size = 12) +\n  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(),\n        panel.grid.major.y = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nCombined likert graph of multiple choice grids\n\n\nR code\n# Read CSV\ndf &lt;- readr::read_csv(\"survey/bio478_survey_responses.csv\")\n\n# Domain order (used to control facet/plot sequence)\ndomains &lt;- c(\n  \"Helpfulness of generative AI\",\n  \"Inquiry & exploration\",\n  \"Impact on learning\",\n  \"Code quality outcomes using AI\",\n  \"\\\"Vibe code\\\" outcomes\",\n  \"Impact on metagenomics learning\",\n  \"Trust calibration\",\n  \"Ethical alignment\"\n)\n\n# find columns that match any domain substring\ncol_matches &lt;- names(df)[sapply(names(df), function(nm) any(sapply(domains, function(d) str_detect(nm, fixed(d)))))]\nif (length(col_matches) == 0) stop(\"No matching columns found. Check column names or adjust domain patterns.\")\nraw_help &lt;- df |&gt; select(all_of(col_matches))\n\n\n\n\nR code\n# helper to detect domain and extract item text inside brackets if present\ndetect_domain &lt;- function(colname, domain_vec) {\n  idx &lt;- which(sapply(domain_vec, function(d) str_detect(colname, fixed(d))))\n  if (length(idx) == 0) return(NA_character_)\n  domain_vec[idx[1]]\n}\n\ncol_info &lt;- tibble(orig_name = names(raw_help)) |&gt;\n  mutate(domain = map_chr(orig_name, ~ detect_domain(.x, domains)),\n         item = if_else(str_detect(orig_name, \"\\\\[.*\\\\]\"),\n                        str_extract(orig_name, \"(?&lt;=\\\\[).+?(?=\\\\])\"),\n                        str_remove(orig_name, fixed(domain))),\n         item = str_trim(item),\n         item = if_else(item == \"\" | is.na(item), orig_name, item))\n\n\n\n\nR code\n# Agreement levels and robust mapping function\nagree_levels &lt;- c(\"Strongly disagree\", \"Disagree\", \"Neutral\", \"Agree\", \"Strongly agree\")\n\nmap_to_agree &lt;- function(x) {\n  x &lt;- stringr::str_squish(as.character(x))\n  x_lower &lt;- tolower(x)\n  case_when(\n    # helpfulness -&gt; agreement mapping\n    x_lower %in% c(\"not at all helpful\", \"not helpful\", \"not at all helpful to not helpful\") ~ \"Strongly disagree\",\n    x_lower %in% c(\"slightly helpful\") ~ \"Disagree\",\n    x_lower %in% c(\"moderately helpful\") ~ \"Neutral\",\n    x_lower %in% c(\"very helpful\") ~ \"Agree\",\n    x_lower %in% c(\"extremely helpful\") ~ \"Strongly agree\",\n    # direct agreement variants\n    x_lower %in% c(\"strongly disagree\", \"strong disagree\") ~ \"Strongly disagree\",\n    x_lower %in% c(\"disagree\") ~ \"Disagree\",\n    x_lower %in% c(\"neutral\", \"neither agree nor disagree\", \"neither\") ~ \"Neutral\",\n    x_lower %in% c(\"agree\") ~ \"Agree\",\n    x_lower %in% c(\"strongly agree\", \"strong agree\") ~ \"Strongly agree\",\n    # already canonical\n    x %in% agree_levels ~ x,\n    TRUE ~ NA_character_\n  )\n}\n\n# pivot to long, join col_info, map responses, compute counts and percentages\nagree_levels &lt;- c(\"Strongly disagree\",\"Disagree\",\"Neutral\",\"Agree\",\"Strongly agree\")\n\nlong &lt;- raw_help %&gt;%\n  pivot_longer(cols = everything(), names_to = \"orig_name\", values_to = \"response_raw\") %&gt;%\n  left_join(col_info, by = \"orig_name\") %&gt;%\n  mutate(response_mapped = map_to_agree(response_raw)) %&gt;%\n  group_by(domain, item, response_mapped) %&gt;%\n  summarise(n = n(), .groups = \"drop_last\") %&gt;%\n  group_by(domain, item) %&gt;%\n  # add rows for any missing response levels per domain/item\n  complete(response_mapped = agree_levels, fill = list(n = 0)) %&gt;%\n  # make sure response_mapped is an ordered factor with the exact levels\n  mutate(response_mapped = factor(response_mapped, levels = agree_levels, ordered = TRUE),\n         total_n = sum(n),\n         pct = ifelse(total_n &gt; 0, n / total_n, 0)) %&gt;%\n  ungroup()\n\n\n\n\nR code\n# compute per-item positive proportion (Agree + Strongly agree) per domain\nitem_summary &lt;- long |&gt;\n  filter(!is.na(response_mapped)) |&gt;\n  group_by(domain, item) |&gt;\n  summarize(total_n = sum(n, na.rm = TRUE),\n            positive_n = sum(n[response_mapped %in% c(\"Agree\", \"Strongly agree\")], na.rm = TRUE),\n            positive_pct = if_else(total_n &gt; 0, positive_n / total_n, 0),\n            .groups = \"drop\")\n\n\n\n\nR code\n# Prepare per-domain data frames (only domains present)\ndomain_dfs &lt;- long |&gt;\n  filter(!is.na(response_mapped)) |&gt;\n  group_split(domain)\nnames(domain_dfs) &lt;- map_chr(domain_dfs, ~ unique(.x$domain))\n\n# keep only requested domains that actually exist in data\npresent_domains &lt;- intersect(domains, names(domain_dfs))\ndomain_dfs &lt;- domain_dfs[present_domains]\n\n# count items per domain for relative heights\nitem_counts &lt;- map_int(domain_dfs, ~ length(unique(.x$item)))\n\n# color palette and common y-limits\ncols &lt;- c(\"Strongly disagree\" = \"#d73027\",\n          \"Disagree\" = \"#fc8d59\",\n          \"Neutral\" = \"#fee090\",\n          \"Agree\" = \"#91bfdb\",\n          \"Strongly agree\" = \"#1a9850\")\ny_limits &lt;- c(0, 1)\n\n# Build one ggplot per domain. Ensure scale_fill includes all five categories and uses a one-row legend.\nplot_list &lt;- imap(domain_dfs, function(df_dom, dom_name) {\n  # determine ordering by positive_pct (descending), tie-break by total_n (descending)\n  item_order &lt;- item_summary |&gt;\n    filter(domain == dom_name) |&gt;\n    arrange(-desc(positive_pct), desc(total_n)) |&gt;\n    pull(item)\n  if (length(item_order) == 0) item_order &lt;- unique(df_dom$item)\n\n  df_dom2 &lt;- df_dom |&gt;\n    mutate(item = factor(item, levels = item_order, ordered = TRUE))\n\n  ggplot(df_dom2, aes(x = item, y = pct, fill = response_mapped)) +\n    geom_col(width = 0.75, color = NA) +\n    coord_flip() +\n    scale_y_continuous(labels = percent_format(accuracy = 1), limits = y_limits, expand = c(0,0)) +\n    scale_fill_manual(\n        values = cols,\n        breaks = agree_levels,\n        limits = agree_levels,    # ensure every plot's scale has the same keys\n        drop = FALSE,             # don't drop unused factor levels\n        guide = guide_legend(nrow = 1, byrow = TRUE),\n        name = \"Response\"\n    ) +\n    labs(title = dom_name, x = NULL, y = NULL) +\n    theme_minimal(base_size = 12) +\n    theme(plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),\n          axis.text.y = element_text(size = 10),\n          panel.grid.major.y = element_blank(),\n          plot.margin = margin(6,6,6,6),\n          text = element_text(color = \"black\"),\n          legend.position = \"bottom\")\n})\n\n\n\n\nR code\n# Combine vertically with a single collected legend at bottom\ncombined &lt;- wrap_plots(plot_list, ncol = 1, guides = \"collect\") +\n  plot_layout(heights = item_counts, guides = \"collect\") & theme(legend.position = \"bottom\")\n\ncombined\n\n\n\n\n\n\n\n\n\n\n\nWhen AI was wrong, I typically… - multiple response\n\n\nR code\n# remove all objects from the environment - ha!\nrm(list = ls())\n\n\n\n\nR code\n# Choose whether percentages should be relative to respondents who answered this question (\"answerers\") or relative to all survey respondents (\"all\"):\npercent_base &lt;- \"answerers\" # \"answerers\" or \"all\"\n\n## Read data and detect the column\ndf &lt;- read_csv(\"survey/bio478_survey_responses.csv\")\n\n# detect the multi-response column (flexible by partial name)\nresp_col &lt;- names(df)[str_detect(names(df), regex(\"When AI was wrong\", ignore_case = TRUE))][1]\nif(is.na(resp_col)) stop(\"Could not find a column matching 'When AI was wrong' in the CSV.\")\n# resp_col\n\n\n\n\nR code\n# Prepare answers (split multi-response entries)\n# ensure respondent id exists\nif(!\"respondent_id\" %in% names(df)) df &lt;- df |&gt; mutate(respondent_id = row_number())\n\ndf_answers &lt;- df |&gt;\n  select(respondent_id, all_of(resp_col)) |&gt;\n  rename(raw = all_of(resp_col)) |&gt;\n  filter(!is.na(raw) & str_trim(raw) != \"\") |&gt;               # only respondents who answered this question\n  mutate(respondent_id = as.character(respondent_id)) |&gt;\n  # split by comma or semicolon (adjust sep if your data uses a different delimiter)\n  separate_rows(raw, sep = \"\\\\s*[,;]\\\\s*\") |&gt;\n  # treat \"Other: ...\" as \"Other\" (if you want to inspect free text, modify this)\n  mutate(answer = str_remove(raw, regex(\"^Other[:\\\\-]\\\\s*\", ignore_case = TRUE)) |&gt; str_trim()) |&gt;\n  mutate(answer = if_else(str_detect(raw, regex(\"^Other[:\\\\-]\", ignore_case = TRUE)), \"Other\", answer)) |&gt;\n  distinct(respondent_id, answer) |&gt;   # avoid double-count within same respondent\n  filter(answer != \"\" & !is.na(answer))\n\n# quick look at distinct answers found\n# df_answers |&gt; distinct(answer) |&gt; arrange(answer)\n\n\n\n\nR code\n# If custom order not provided, infer from observed answers:\noptions_order &lt;- if(exists(\"options_order\")) options_order else df_answers |&gt; distinct(answer) |&gt; pull(answer)\n\n\n\n\nR code\n# Count and compute percentages\ncounts &lt;- df_answers |&gt;\n  count(answer, name = \"n\") |&gt;\n  complete(answer = options_order, fill = list(n = 0)) |&gt;\n  mutate(pct = n / sum(n),\n         answer = forcats::fct_reorder(answer, -n))  # order by frequency\n\nif(tolower(percent_base) == \"all\"){\n  counts &lt;- counts |&gt; mutate(pct = n / nrow(df))\n  y_label &lt;- \"Percent of all respondents\"\n} else {\n  counts &lt;- counts |&gt; mutate(pct = n / n_distinct(df_answers$respondent_id))\n  y_label &lt;- \"Percent of respondents who answered this question\"\n}\n\n\n\n\nR code\np &lt;- ggplot(counts, aes(x = fct_rev(answer), y = pct)) +\n  geom_col(fill = \"#881c1c\", width = 0.7) +\n  coord_flip() +\n  scale_y_continuous(labels = percent_format(accuracy = 1), expand = expansion(mult = c(0, 0.12))) +\n  geom_text(aes(label = paste0(n, \" (\", scales::percent(pct, accuracy = 1), \")\")),\n            hjust = -0.02, size = 3.2) +\n  labs(title = \"When AI was wrong, I typically…\",\n       x = NULL,\n       y = y_label) +\n  theme_minimal(base_size = 12) +\n  theme(panel.grid.major.y = element_blank())\n\np\n\n\n\n\n\n\n\n\n\n\n\nCommon issues encountered - multiple response\n\n\nR code\n# remove all objects from the environment\nrm(list = ls())\n\n\n\n\nR code\n# Choose whether percentages should be relative to respondents who answered this question (\"answerers\") or relative to all survey respondents (\"all\"):\npercent_base &lt;- \"answerers\" # \"answerers\" or \"all\"\n\n## Read data and detect the column\ndf &lt;- read_csv(\"survey/bio478_survey_responses.csv\")\n\nresp_col &lt;- names(df)[str_detect(names(df), regex(\"Common issues encountered\", ignore_case = TRUE))][1]\nif(is.na(resp_col)) stop(\"Could not find a column matching 'Common issues encountered' in the CSV.\")\n# resp_col\n\n\n\n\nR code\n# Prepare answers (split multi-response entries)\n# ensure respondent id exists\nif(!\"respondent_id\" %in% names(df)) df &lt;- df |&gt; mutate(respondent_id = row_number())\n\ndf_answers &lt;- df |&gt;\n  select(respondent_id, all_of(resp_col)) |&gt;\n  rename(raw = all_of(resp_col)) |&gt;\n  filter(!is.na(raw) & str_trim(raw) != \"\") |&gt;               # only respondents who answered this question\n  mutate(respondent_id = as.character(respondent_id)) |&gt;\n  # split by comma or semicolon (adjust sep if your data uses a different delimiter)\n  separate_rows(raw, sep = \"\\\\s*[,;]\\\\s*\") |&gt;\n  # treat \"Other: ...\" as \"Other\" (if you want to inspect free text, modify this)\n  mutate(answer = str_remove(raw, regex(\"^Other[:\\\\-]\\\\s*\", ignore_case = TRUE)) |&gt; str_trim()) |&gt;\n  mutate(answer = if_else(str_detect(raw, regex(\"^Other[:\\\\-]\", ignore_case = TRUE)), \"Other\", answer)) |&gt;\n  distinct(respondent_id, answer) |&gt;   # avoid double-count within same respondent\n  filter(answer != \"\" & !is.na(answer))\n\n# quick look at distinct answers found\n# df_answers |&gt; distinct(answer) |&gt; arrange(answer)\n\n\n\n\nR code\n# If custom order not provided, infer from observed answers:\noptions_order &lt;- if(exists(\"options_order\")) options_order else df_answers |&gt; distinct(answer) |&gt; pull(answer)\n\n\n\n\nR code\n# Count and compute percentages\ncounts &lt;- df_answers |&gt;\n  count(answer, name = \"n\") |&gt;\n  complete(answer = options_order, fill = list(n = 0)) |&gt;\n  mutate(pct = n / sum(n),\n         answer = forcats::fct_reorder(answer, -n))  # order by frequency\n\nif(tolower(percent_base) == \"all\"){\n  counts &lt;- counts |&gt; mutate(pct = n / nrow(df))\n  y_label &lt;- \"Percent of all respondents\"\n} else {\n  counts &lt;- counts |&gt; mutate(pct = n / n_distinct(df_answers$respondent_id))\n  y_label &lt;- \"Percent of respondents who answered this question\"\n}\n\n\n\n\nR code\np &lt;- ggplot(counts, aes(x = fct_rev(answer), y = pct)) +\n  geom_col(fill = \"#881c1c\", width = 0.7) +\n  coord_flip() +\n  scale_y_continuous(labels = percent_format(accuracy = 1), expand = expansion(mult = c(0, 0.12))) +\n  geom_text(aes(label = paste0(n, \" (\", scales::percent(pct, accuracy = 1), \")\")),\n            hjust = -0.02, size = 3.2) +\n  labs(title = \"Common issues encountered\",\n       x = NULL,\n       y = y_label) +\n  theme_minimal(base_size = 12) +\n  theme(panel.grid.major.y = element_blank())\n\np\n\n\n\n\n\n\n\n\n\n\n\nWhere more support would help - multiple response\n\n\nR code\n# remove all objects from the environment\nrm(list = ls())\n\n\n\n\nR code\n# Choose whether percentages should be relative to respondents who answered this question (\"answerers\") or relative to all survey respondents (\"all\"):\npercent_base &lt;- \"answerers\" # \"answerers\" or \"all\"\n\n## Read data and detect the column\ndf &lt;- read_csv(\"survey/bio478_survey_responses.csv\")\n\nresp_col &lt;- names(df)[str_detect(names(df), regex(\"Where more support would help\", ignore_case = TRUE))][1]\nif(is.na(resp_col)) stop(\"Could not find a column matching 'Where more support would help' in the CSV.\")\n# resp_col\n\n\n\n\nR code\n# Prepare answers (split multi-response entries)\n# ensure respondent id exists\nif(!\"respondent_id\" %in% names(df)) df &lt;- df |&gt; mutate(respondent_id = row_number())\n\ndf_answers &lt;- df |&gt;\n  select(respondent_id, all_of(resp_col)) |&gt;\n  rename(raw = all_of(resp_col)) |&gt;\n  filter(!is.na(raw) & str_trim(raw) != \"\") |&gt;               # only respondents who answered this question\n  mutate(respondent_id = as.character(respondent_id)) |&gt;\n  # split by comma or semicolon (adjust sep if your data uses a different delimiter)\n  separate_rows(raw, sep = \"\\\\s*[,;]\\\\s*\") |&gt;\n  # treat \"Other: ...\" as \"Other\" (if you want to inspect free text, modify this)\n  mutate(answer = str_remove(raw, regex(\"^Other[:\\\\-]\\\\s*\", ignore_case = TRUE)) |&gt; str_trim()) |&gt;\n  mutate(answer = if_else(str_detect(raw, regex(\"^Other[:\\\\-]\", ignore_case = TRUE)), \"Other\", answer)) |&gt;\n  distinct(respondent_id, answer) |&gt;   # avoid double-count within same respondent\n  filter(answer != \"\" & !is.na(answer))\n\n# quick look at distinct answers found\n# df_answers |&gt; distinct(answer) |&gt; arrange(answer)\n\n\n\n\nR code\n# If custom order not provided, infer from observed answers:\noptions_order &lt;- if(exists(\"options_order\")) options_order else df_answers |&gt; distinct(answer) |&gt; pull(answer)\n\n\n\n\nR code\n# Count and compute percentages\ncounts &lt;- df_answers |&gt;\n  count(answer, name = \"n\") |&gt;\n  complete(answer = options_order, fill = list(n = 0)) |&gt;\n  mutate(pct = n / sum(n),\n         answer = forcats::fct_reorder(answer, -n))  # order by frequency\n\nif(tolower(percent_base) == \"all\"){\n  counts &lt;- counts |&gt; mutate(pct = n / nrow(df))\n  y_label &lt;- \"Percent of all respondents\"\n} else {\n  counts &lt;- counts |&gt; mutate(pct = n / n_distinct(df_answers$respondent_id))\n  y_label &lt;- \"Percent of respondents who answered this question\"\n}\n\n\n\n\nR code\np &lt;- ggplot(counts, aes(x = fct_rev(answer), y = pct)) +\n  geom_col(fill = \"#881c1c\", width = 0.7) +\n  coord_flip() +\n  scale_y_continuous(labels = percent_format(accuracy = 1), expand = expansion(mult = c(0, 0.12))) +\n  geom_text(aes(label = paste0(n, \" (\", scales::percent(pct, accuracy = 1), \")\")),\n            hjust = -0.02, size = 3.2) +\n  labs(title = \"Where more support would help\",\n       x = NULL,\n       y = y_label) +\n  theme_minimal(base_size = 12) +\n  theme(panel.grid.major.y = element_blank())\n\np\n\n\n\n\n\n\n\n\n\n\n\nExamples of helpful AI responses - Open response\n\n\nR code\n# Read CSV for open_response\ndf_survey &lt;- readr::read_csv(\"survey/bio478_survey_responses.csv\") |&gt;\n  clean_names() \n\n\n\n\nR code\nwrapped_text &lt;- str_wrap(df_survey$could_you_give_short_examples_of_a_helpful_ai_response, width = 80, indent = 0, exdent = 0, whitespace_only = TRUE)\ncat(wrapped_text, sep = \"\\n\\n\")\n\n\nIt telling me how to create statistical analysis formulas to compare data\npoints, and how to map graphs\n\nSometime when I didn't know how to attack a problem it would take me through the\nsteps I needed to do before I wrote a code to answer the question like cleaning\nup or merging data\n\nBefore we learned the definitions of regex patterns and I tried to ask AI\nto create patterns for effective character finding, I used AI to give an\nexplanation of the reasoning and meaning of the regex patterns it generated\nEven before we formally learned the skill, AI helped me think of advanced regex\npatterns\n\nWriting code blocks utilizing functions/libraries I hadn't used before\n\nclarity on homework / studying\n\nGiving me different types of analysis I could do to see spatial autocorrelation\nfor the MAG and metadata dataset\n\nMaking sense of the outputs generated by the code Or clarifying what table\ncategories that have slightly different names from the instructions are most\nlikely to be what the instructions are trying to direct you to\n\nIt really helped a lot during the lab exercises and it also explains it too\n\nWhen working with the NEON Mags dataset, the AI gave very good suggestions on\nhow to go about formatting my data and what errors were preventing my code from\nrunning\n\nNA\n\nA lot of the time I asked AI to help explain certain coding functions or I would\nask them why I am getting the error message I am getting when rendering my lab\nand that's helped me learn from why I can't write the code in certain ways So\nAI has definitely helped me improve my coding syntax so that less errors come up\nwhen I render my labs\n\nAI was very good at telling me what an error message meant, provided that I gave\nthem the exact error message R was giving me AI would often explain what the\nerror message meant and then provide multiple common ways to solve this problem\n\nWhen an error occurs it will explain my error to me and give me code to correct\nit\n\n-producing helpful explanations when codes do not run, providing simple code\nfixes\n\nAI was able to come up analyses and graphs for the data that I never would have\nthought of It also provided the code for each analysis which was very helpful\nwhen trying to reproduce the graph in RStudio\n\nI frequently would give copilot what I said and the error code and it would then\nbold what it changes and why\n\nOnce I was supposed to run a code to make graphs in a certain way but I wasn't\nreally sure on how to do that AI helped me brainstorm ways to make my code run\nthe way I needed it to\n\nI asked it how to create a pie chart and it described it for me Another thing\nwas to make a graph that was interactive so when your mouse hovers over a point\nyou see what it is\n\nOne that integrated the suggested code in its response, and followed that with\nan explanation of why the code worked Another common one was when I reached an\nissue with one of my code chunks it would correct the issue and explain why my\ncode wasn't working\n\nGiving an example of the code and then explainging what each part does, making\nit easy to know what use/add to get the product i needed\n\nAI has been helpful in providing responses to fix my coding errors For example,\nI would tell CoPilot the code I was trying to run, and say it was not working\nand enter the specific \"Error:\" I was getting Copilot would then send back the\ncorrect code (most of the time), and explain where I went wrong which was really\nhelpful so that I would not make the same mistake again\n\nI was having trouble finding code to help me build a PCA plot for the MAG data\nset I upload the code that I already had as well as the MAG csv file and AI was\nable to give me a better code that also gave me instructions on what to change\nThat was very helpful and allowed me to see what I was doing wrong\n\nI usually used AI to correct errors within the code I created, so a helpful\nresponse would be something along the lines of \"I see the problem is ______\nwhich can be solved by _______\"\n\nit would give options on how to improve or fix the code and when taking a\nscreenshot of my code & the errors, it was almost always helpful with helping me\nfix the issues\n\nWhen I'd get an error, I'd give it to AI and often, it could tell me what I did\nwrong and how to fix it\n\nOften times I will just input my error messages into AI (specifically ChatGPT)\nand even without seeing my original code, it generates a successful fix for my\ncode Chat has been very successful in providing code fixes and formats on how to\ncode certain things\n\nI thought the most helpful Ai responses were the ones that gave me step by\nstep instructions for the type of code to generate depending on the prompt By\nbreaking it down, I could understand the point of each chunk\n\nPutting in error code from R would 85% of the time be resolvable through AI\ndirectly Otherwise I could ask for alternative methods to alter my code\n\nI would ask it to help me with something, and then it would output the code and\nthe reasoning behind the code and how it works\n\nIt helped me be able to make graphs and complete exercises It helped me\nunderstand what was going on in general And it always showed different ways to\nwrite the code\n\nHaving an idea on how you want data to be analyzed but not being sure of what\nkind of graphic would be best suited for that AI can suggest a few different\nmethods and write the bulk of the code required to set the graphic up in RStudio\n\nTroubleshooting issues was when I found AI most effective, rendering problems\nand trying to understand coding terms\n\nAll of the assignments like Making a graph for neon image data\n\nWhen I have difficulty running a code due to an error in it and I could not find\na way to make the code work so I will put the code and the feedback that R gave\nme on copilot and ask the AI to help me understand how to fix the code\n\nAnything I don’t know ai can help\n\nNot an exact example but the answers that included step by step explanations of\nthe code were helpful\n\nAI can help clean out a code chunk by replacing verbose patterns with\nsimpler, more readable alternatives — for example, instead of writing\nfilter(!isna(site_id), !isna(phylum)) %&gt;% count(site_id, phylum), AI might\nsuggest drop_na(site_id, phylum) %&gt;% count(site_id, phylum), which is shorter\nand easier to understand\n\nHere’s a specific example: my question to AI was: “what's the difference\nbetween your solution and what I wrote?” AI gave me a step by step explanation\nof (1) what my code does and (2) problems my code causes (failing to address\nthe problem) The specific problem I was working on was How to display a table\nthat counts the number of MAGs by phylum at each site AI was very helpful\nin all circumstances, especially in *Suggesting R code to solve problems*\nand *Explaining R code syntax and logic* Those were my two top uses of AI Of\ncourse, AI helped with all the other categories too, but for the most part, the\ntwo I just mentioned were the categories that I most often consulted AI for I\nfeel like the category *Data wrangling with MAG taxonomy + genome properties\n+ soil chemistry* can be included underneath either of these two, because it\nwas related to problem-solving The reason I put *Extremely helpful* for all\ncategories because of the ability to consult AI for really almost everything\nFor the two categories I mentioned above, these are the prompts I would ask\n“Here’s my code It seems to me that it works fine, but can you verify that it\nreally does?” Or, “What does this function do? What’s the difference between\nthis one and this one?” Or, “Here’s the problem I’m trying to solve, and here’s\nmy code so far Can you help me with my code (either troubleshoot this error\nI’m getting or show me whether this is the best way to approach the problem)”\nI took advantage of making the most out of AI by asking it to explain functions\nit used that I wasn’t familiar with I also always tried to understand AI logic\nby walking myself through each line, one by one The next two categories that\nI used AI for mostly in this course was *Brainstorming research questions with\nNEON metagenomic data* *‘Vibe code’: generating statistical, network, or other\nadvanced analyses* kind of follows or falls under this This is especially\nhelpful because I don’t have a research background and while I can ask simple\nquestions like “What phyla are enriched in these soil pH ranges?”, I still\nwould need to expand on this question My problem is that I’m not creative\n(maybe not curious enough?) with my questions This is why using AI to jumpstart\nbrainstorming is especially helpful, especially because it can quickly provide\nbackground information that might spark my curiosity\n\nWhat's wrong here? - “You are getting this error because joinID does not exist\nin the right table Check colnames() and rename before left_join()”\n\nAI fixing errors in my code\n\nProviding multiple approaches\n\nMost useful with chunks of code for statistical analysis and plots readibility\n\nNA\n\n\n\n\nExamples of unhelpful or incorrect AI responses - Open response\n\n\nR code\nwrapped_text &lt;- str_wrap(df_survey$could_you_give_short_examples_of_unhelpful_or_incorrect_ai_responses, width = 80, indent = 0, exdent = 0, whitespace_only = TRUE)\ncat(wrapped_text, sep = \"\\n\\n\")\n\n\nWhen it would tell me to use formulas that needed only numerical values but I\nhad N/A in my values It would tell me to use random names or column types that\ndidn't exist\n\nSometimes if I had an error it would keep giving me incorrect codes to the point\nwhere nothing worked and I did'nt know how to tell fix this issue\n\nWhen reading the soil mags + soil chem csv file to create Vibe code, AI would\neither reveal that the file is too big to be read or it would pretend to read\nit Either way, it would generate code that assumed variables and column names,\nwhich were often incorrect\n\nMany AI responses generate code that contain errors\n\nusing AI for therapy\n\nThe codes it would give me would have a lot of errors, but this was fine because\nyou learn a lot by seeing where those mistakes are and how to fix them yourself\n\nWhen you're trying to fix a path directory error or use code it's giving you and\nyou don't realize the proper names of the files are wrong and thats the secret\nreason why nothing is working\n\nIt takes a couple of tries for copilot to understand what is being asked which\ncan be frustrating\n\nWhen it came to working with GitHub or other external R coding, AI was not\nable to give very helpful tips and was just reiterating the same response in\ndifferent wording\n\nNA\n\nSometimes when prompting AI with my questions, I've learned that you have to\ncraft the question very specifically in order for them to give generally what\nyou're looking for The times that I have found AI giving unhelpful responses is\nwhen I don't prompt it correctly and sometimes it will give me an answer/code\nthat uses a library or code function that we have not learned or not part of the\nlesson we are working on\n\nWhen combining data frames I found that using AI wasn't as helpful as just\nreferring back to the module AI would overcomplicate things and sometimes I\nwould lose some of my data\n\nAI isnt the best at helping with MAG data or anything in terms of\ntroubleshooting for working directory\n\nAI responses that don't provide a code or give super complicated unrunnable\ncodes\n\nAI is very prompt-specific, so if a prompt does not reflect exactly what the\nperson wants, it takes a lot of back and forth between the AI and the person to\nget a good result\n\nsometimes it gives wrong names for column\n\nSometimes when I would get an error in my code I was ask AI what I did wrong and\nhow I can fix It and most of the time it did help me but sometimes it would act\nup and not really give me proper guidance in what needed to be done\n\nAI telling me to install and load new libraries when I don't even need them\n\nIt would occasionally give me a code that wouldn't run or it wouldn't explain\nwhy a certain code wasn't working\n\ngiving just example code with no explanation\n\nThere have been times where Copilot was wrong I would have to work with it back\nand forth, and say things like \"No, that is not correct The error: is coming\nup\", etc However, after I explained more, it would show me the correct code\n(most of the time)\n\nIn the beginning of the class, using AI was a challenge because I did not know\nthe proper formatting or syntax I would as AI to give me code to transform a\ndata set but not really understand what was being done An example of this was me\nusing the \"merge\" function instead \"left join\"\n\nIf I used AI to generate new code, it would often assume file/column names When\nI input my edited code (with corrected file and column names), it would solve\nproblems in the code but still using the file names it originally created Not a\nterrible issue, just annoying\n\nsometimes it would make the code too long & complicated when it could really be\nmore concise & it became really challenging when trying to use Al to help with\nuploading thing like soilChem and definitely NEONmag\n\nSometimes, AI would suggest code that included words/phrases that weren't in the\ndata set, so I struggled to know what I should enter instead\n\nI was trying to join two data sets but the IDs in the data sets had slightly\ndifferent suffixes The suggestion that AI was giving me wasn't working in\nremoving the suffix but it kept going around in the same circle Eventually, AI\nwas able to make the join work but I am not sure how\n\nI found that some of the unhelpful/incorrect Ai responses typically had to do\nwith sometimes I felt like it did not listen to my requests or it would suggest\ntroubleshooting techniques that would not work\n\nWhen AI would try to overcomplicate analysis (that would not run) and make me\ndownload a bunch of new libraries for one prompt By the end of lab 13 I have\nlibrary(ggfortify), library(vegan), library(plotly), library(respirometry),\nlibrary(ggplot2), and library(corrplot)\n\nSometimes it would output code that did not do what I wanted it to do; then\nI would have to refine it and tell it to redo it until it got it right Also,\nsometimes the graphs or figures it would make would be cramped together and not\nlegible\n\nAI made some graphs that were not accurate or did not have a good set up Also\ncertain commands it was not good at using\n\nFor some of the more complicated data analysis there will be some lines where\nyou get an error and try to use AI to solve it However it can't solve it (maybe\ndue to package/R version incompatibility) and you end up going down a rabbit\nhole and wasting time\n\nWould over-complicate simple exercises, adding unnecessary steps\n\nIn lab S3 Connecting a Github repo site with a new RStudio project It kept\ngiving me wrong code to link and due to that the Lab was not loading\n\nWhen AI will give me different variations of the same code and it will still\nnot work even after going back and forth with it for 30 minutes It happened\nto me multiple times that I asked my classmates for help Another bad short\nexamples will be AI using outdated syntax, complex code for simple tax and\nmisinterpreting data types\n\nsome wrong solution making everything complicated, ai love to Change environment\nfor some reasones\n\nNot an exact example but the answers that misinterpreted the prompts or didn’t\nhave the full data sets weren’t helpful\n\nAn unhelpful AI response might be overly vague, like ‘Just fix the code\nsomehow’, which gives no actionable guidance Another incorrect response could\nconfidently suggest a function or method that doesn’t exist, leaving the user\nconfused and misled\n\nFor me, examples would be when the code simply doesn’t work Sometimes it’s due\nto base R issues or packages and version incompatibility—things like that I\ndon’t know any technical terms to describe what the issues are more specifically\nI also don’t have a good understanding of how computers work One specific\nexample is when I wanted to do an analysis of MAG data using a heat map based\non soil moisture My input was, “Here’s my code so far where I got the average\nsoil temperature for each phyla How do I make a heat map from here?” AI gave\nme a solution where I had to convert my data frame to a matrix first The code\ndidn’t work from there due to some issues with creating the matrix I believe I\ncould have troubleshooted some more, as well as learn more about matrices on my\nown, but in that moment I really just wanted a quick visual and wasn’t willing\nto spend time to learn about matrices I think the only issue with AI feedback\nhere is that my code is nuanced to the data I’m working with and AI doesn’t\nget to have the full picture I admit I could have been less lazy here and try\ntroubleshooting it myself!\n\n“Here is an answer” with facts that are wrong or made up (hallucinating)\n\nAI going in the wrong direction about my code\n\nMisunderstanding the prompt, hallucinations\n\nHardest to solve problems If something did not work or wrong answer, needed to\nfigure it out mostly on my own, AI hardly could fix it\n\nNA\n\n\n\n\nSuggested changes for future offering - Open response\n\n\nR code\nwrapped_text &lt;- str_wrap(df_survey$suggested_changes_for_future_offerings, width = 80, indent = 0, exdent = 0, whitespace_only = TRUE)\ncat(wrapped_text, sep = \"\\n\\n\")\n\n\nMore help on github pushing and just using github as a whole\n\nI think AI helped my learning expirence I would just add more to debugging and\nvarifying AI like how to maybe do a monday class on this instead of a lab\n\nI definitely think that lab reports should require students to report the prompt\nthat was used to generate any code they used, preceding an exercise I think this\nis important data for this class and can be used to think of effective prompting\nstrategies One idea I had, is to use Monday discussions to do either class-wide\nor team/table debriefing in which we talk about what sort of code we are getting\nfrom AI I think a collaborative aspect in this class would be helpful for\nkeeping students on track together\n\nThe problems often felt formatted/designed in a way where AI use could easily\nsolve them without requiring critical thinking from the student I think problems\nthat focus more on having students determine what steps (or generated code\nchunks) would need to be taken could help the course encourage greater learning\n\nNA\n\nCovering more on ethics and AI use\n\nNA\n\nI think because there is a Monday and Wednesday portion, The Monday portion\nshould be spent explaining what the lab on Wednesday will be I found myself\nusing AI a lot for things that I felt could’ve been discussed in class\n\nSome assignments that focus more on the content of the course (ie assignments\nthat teach about NEON and soil chemistry/make up and how the research is\nadvancing) and where AI is not used\n\nNA\n\nI think when starting with AI sometimes it's easy to just copy and paste the\nquestion we're asked, but I think it would be helpful to start off with examples\nof how we can prompt AI to guide us in answering our problems\n\nNA\n\nn/a\n\nI would not say change anything but maybe more AI use towards the end of a\nspecific subject so students have time to actually learn about the topic (ex No\nAI use allowed monday(introduxe topic), AI use allowed in wednesday labs\n\nIncorporating learning R code without AI first would help give students a\nbaseline of what R code should look and function like without having to rely on\nAI too much\n\nmaybe not allow AI until a little further into the course and making the\nassignments not so full so that we could focus on learning rather than just\ngetting all the assignments and examples and then exercises done\n\nUsing AI and giving it some background before asking questions so it has the\nknowledge to help instead of pulling information from places that are irreverent\nto the person using it\n\nNothing right now\n\nThe class relied too heavily on AI I think it would have been helpful to have\nthe first couple of weeks without AI so we could learn the basics of R before\ndelving into more complicated analysis with AI Also I felt that I didn't learn\nhow to properly cite AI sources with respect to a coding assignment\n\nNA\n\nI would say there is not much to change, I think AI definitely brought my\nabilities of working with R to the next level The only thing I'd recommend is\ngoing over how to verify the AI outputs because sometimes I would get the wrong\ncode or have to install packages that we do not need for this class\n\nI would think it would be great if we could get to the MAG data earlier in the\nclass and learn more about different metagenomic analyses\n\nI think it was sometimes confusing to know exactly how to cite AI in the course,\nso more clarity on when/how it should be used in the earlier labs would be\nhelpful\n\nI know students will use Al to help them in this course in one way or another\nbut if there's a way to help improve learning I know many of us had no\nbackground use of coding at all and we kind of jumped in quickly so I would\ndepend on Al a lot to simply just get my work done\n\nNA\n\nAI skyrocketed my coding capabilities However, I don't feel very confident in\ncoding without AI This class would benefit from spending more time on ingraining\nR/coding basics before moving to advanced R coding and metagenomic analysis\nMaybe the first half of the course could be ingraining both the basics in code\nand metagenomic analysis and the second half could be the explosive, exciting,\nAI based coding and analysis Using AI was a world-opening experience for me in\ncoding\n\nN/A\n\nAsk people to discuss there code, or allow for people to do independent projects\nI really enjoyed the last lab that was open ended If we did the end of year\nproject I think that I would have been able to utilize all the skills that I\nlearned\n\nI would suggest more instruction for how to ask AI questions to get the results\nyou want Also, more help to verify if the results that the AI produced is\nactually what you want / answers the question how you wanted it answered Also,\nhow to make AI create more legible graphs where the labels or axes are not all\nclumped together so much\n\nMore help with learning how to write code on your own before working AI into the\ncourse I feel like maybe the first few classes should be without AI and trying\nto do things on your own and the bring AI in to help make more advanced code\n\nUsing some measure to prevent over reliance on AI to write all the code without\nunderstanding it Maybe having fewer exercises per lab, but requiring students to\nannotate their code and explain what was accomplished by the finished code\n\nNA\n\nMore activities in the beginning of the course\n\nI had hoped that the class would be more focused on the NEON Lab data and allow\nmore time to work with it We only engaged with the dataset for the last 2 weeks\nof classes And while the previous R sessions did build foundational skills\nthat definitely helped in analyzing the NEON Lab data, it would have been more\nbeneficial to introduce and work with the data earlier during the course Because\nfor me, doing so could have helped with better understanding the value of using\ncoding tools for biological data\n\nNA\n\nHave a class specifically for how to ask copilot for help with R coding,\nincluding how to write a helpful prompt and dissect the answers so that you are\nable to use them in your actual code\n\nNA\n\nI feel like *Debugging with AI* is the most common issue I experience when\nusing AI for code because what we are working on can be so nuanced Because of\nthe nuance, I’m not sure how feasible in-class support with AI debugging would\nbe (in the context of a whole class period), especially if everyone is working\non different analyses / project? I would say the same thing for *Verifying AI\noutput* I think AI is a useful tool used for assistance, but I also found that\nwhat we did in the course has been very valuable— namely doing the chapter\noutlines They can take quite a while to get through, but I agree that it’s\nimportant for us to know the basics of R ourselves For Advanced analyses,\nmaybe it would be helpful to have a brief overview in class of things like\nalpha-diversity and beta-diversity I learned about this before in other classes\n(and don’t remember what they are), so maybe formally going over it in class,\nif only briefly, would be good Maybe taking an additional lecture to explain\nmore of what MAGs are and why they’re useful in research, since I didn’t get it\nthe first time Eventually I came to understand what they are Also, personally\nI don’t know anything about analyzing data, so if we received some advice about\nthings to pay attention to (for example, make sure you take a larger sample\nsize Or any research principles we aren’t aware of? And what is ANOVA test, if\nwe happened to need to use it) from class, just briefly, that would help As for\nusing AI to help generate code for Advanced analyses, it helped so much Perhaps\none thing I think would be good for this course is to require or at least\nhighly encourage students to explain what their code chunk does, and put that\nexplanation in the Quarto document above the code chunk Or to put comments in\ncode in cases where the code looks suspiciously flawless We sometimes do these\nthings by putting the header “AI note” and “Code chunk”, but I wonder whether it\nwould be good for students to go further than minimally stating what prompt they\nasked AI Either way, I suppose a student’s ability to gain some useful knowledge\nis up to how disciplined they are able to be when they have powerful resources\nlike AI\n\nN/A\n\nNA\n\nNA\n\nI think it's like a calculator, I would not use it for anything else It works\nless well than google for research, but it works as expected when given complex\nprompts You may need to amen/check the equation yourself for sure, but it will\nbe able to give you a decent starting point/do the easy stuff faster than you\n\nNA\n\n\n\n\nFinal comments and suggestions - Open response\n\n\nR code\nwrapped_text &lt;- str_wrap(df_survey$final_comments_and_suggestions, width = 80, indent = 0, exdent = 0, whitespace_only = TRUE)\ncat(wrapped_text, sep = \"\\n\\n\")\n\n\nAi really did help and I feel like it is needed for advanced code based on the\nlevel of most students in this class, it just made it so coding felt robotic and\nI learned a little less\n\nI liked being able to use AI but I wish I knew what to do if I fall down a\nrabbit hole of incorrect codes with CoPilot\n\nYour suggestions of introducing the \"Vibe\" code mini labs earlier in the\nsemester is interesting to me I think it would be helpful in one way, as it sort\nof shows students our final goals for this class and can give ideas on what AI\ncan be used to achieve On the other hand, it might introduce advanced code too\nquickly and can make students feel lost or left behind\n\nNA\n\nNA\n\nVery well structured, I learned a lot\n\nNA\n\nNA\n\nNA\n\nNA\n\nI think the integration of AI is helpful in beginner courses as sometimes AI\nhas a good way to \"dumb\" things down almost to explain why the code produces the\noutput that it does Sometimes having a conversation with AI helps the learner\nunderstand a lot more than just reading the textbook because sometimes we need\nmore examples than just the ones provided to see what the concept is trying to\nshow us\n\nNA\n\nhelped me feel like i could confidently code if asked to\n\nLoved this class, Dr Blanchard was super helpful and made coding seem fun even\nthough I have never liked to code Love AI, makes sense for coding courses do not\nget rid of it completely\n\nI enjoyed this class and performed analyses I never would have thought of\nwithout AI suggestions I would have liked to learn coding without AI first to\nestablish a foundation, but ultimately, using AI allowed me to complete a lot\nmore work and understand more functions in R code\n\nI would not have been able to complete this class without the assistance of AI\nand if you want to continue to have a large workload AI is 100% necessary and is\nhelpful It is more on the individual student themselves to actually make sure we\nlearn the content rather than just copy and pasting\n\nWith zero knowledge of coding when coming into this class, AI helped me learn\nthings along the way and made my life so much easier\n\nNo comments\n\nGood course to introduce someone to the use of AI in R, but would have been\nhelpful/interesting to learn some of the basics beforehand\n\nWithout the help of AI, I don't think I would've been able to complete this\ncourse at all, honestly Coding is just extremely hard for me, and using Copilot\nreally helped to break it down into manageable chunks to learn/use\n\nThis has been a great semester! This class was super interesting!\n\nThanks for a great semester!\n\nI feel like the effectiveness of AI in this course varies between students, so\nthis survey is very interesting! I could see AI inhibiting learning if students\nused it heavily in the beginning of the course when we focused on R basics,\nbecause then that work is just busy work I feel like I learned the most from the\nbeginning of the course just using AI as an occasional troubleshooting tool, but\ndepending on AI from the start would have been detrimental\n\nNA\n\nAI helped me understand and learn a lot more in this course than I could have\notherwise without it!\n\nSee suggested changes for future offerings\n\nThis was a very cool class!\n\nUsing on AI was appropriate in this course, but I don't think that it would be\neffective in every course I attached my mycology class analysis in the comments\nif you are interested in the analysis I did I presented the information to my\nclass :) I really enjoyed the skills that I learned in this class and will be\nadding to my resume! Thank you for an amazing semester\n\nI wish there was less of a need to use AI in this course, but once things\nstarted getting complicated it was very hard to do anything without the use of\nAI, especially considering most of us in the class had no experience with coding\nof any sort For example, I did not use AI for the first couple of labs, and that\nwas on purpose because I did not want to have to rely on AI for everything I did\nThe first couple of labs were manageable to do without the use of AI, and they\nwere just challenging enough where I was actually learning a lot of the basics\nof coding However, after a certain point, I felt I needed to use AI to get the\nwork done because the level of difficulty increased too much for me to figure\nout the coding on my own After that point, I relied on AI too much, and I wasn't\nlearning anymore\n\nI really did enjoy the course overall, I like the room and set up of working\nwith a group I think Dr Blanchard was helpful when I went to office hours or\nneeded help in class I wish we learned a few more basic coding steps because\nI think I relied on AI too much I still think it should be a big part of the\ncourse but just that a little more coding without it at the start\n\nI think AI has its place in the modern world but that place is not everywhere I\nunderstand the potential AI has for making coding more accessible for newcomers\nbut it should not eliminate the learning process A balance needs to be found\nbetween AI use and organic learning\n\nWithout AI for this course, I doubt I would have been able to do the labs on my\nown I think the course name should be changed to \"Coding with R\" or something of\na similar nature, because that is what the assignments were, going through the\ntextbook While it was beneficial to get some exposure to R, it was not what I\nexpected at all If I knew what it would be like beforehand, I would never have\ntaken this course The time spent trying to resolve issues with no luck from AI\nand other sources added way too many hours into completing assignments I don't\nenjoy using AI and try to avoid it, but unfortunately with the way the class was\nformatted, I felt that there was no way I could have figured out the issues I\nhad or solved the exercises without it\n\nAI really helped understanding the coding\n\nUsing AI made me more comfortable with coding, and I am actually applying what I\nlearned to one of my classes for a project\n\nNA\n\nNone\n\nNA\n\nUsing AI is extremely helpful in a coding class especially for students like\nme without much familiarity with code I wish more coding classes were offered\nmore frequently in the Biology Department R programming is such a useful skill,\neven if I’m a beginner, and I will certainly be using it to make visuals in the\nfuture I even saw my classmate from here using R to make graphs for project in\na different course Since this is a research-based course and we have limited\ntime this semester (the semester is very short, only 3-4 months), I loved\nthat we gain hands-on, first-hand experience of working with real MAG data and\ndoing analyses with it Doing the chapter outlines from the textbook was very\nhelpful in providing us with foundational knowledge about code syntax and using\ndplyr and ggplot for visualization and analysis I like learning foundational\nprinciples of coding from a concrete source like a textbook, compared to\nself-teaching myself through google At the same time, I still think having\nresources like internet and AI are such a big help because they introduce me\nto so much I would’t have explored (like other functions, more example uses,\ngraph aesthetics, research ideas) And I’m sure that self-teaching is still\nan important part of learning to code and research, too (though I don’t like\nthinking of the idea so much) I found that AI saved so much time when it came to\nproblem-solving, troubleshooting errors, and me memorizing functions That can be\na pain when working on coding assignments, which I’ve experienced before I think\nit’s important to responsibly use AI as a tool to help with my understanding,\nbut not just use it to get the assignment done That’s why I don’t just use AI to\nask for code suggestions, but also for explanation for every function use I also\ntake a look at how the code was structured to understand the problem-solving\nlogic I think that goes back to how we emphasized vibe coding— you should be\nable to know how to approach the problem, even if you can’t write the code just\nyet I have one comment about why students asked less questions to the instructor\nabout the actual code itself and problem-solving (although maybe there were\nstill questions about debugging?) I am sure the reason is obviously having the\nassistance of AI, which is a big help with coding But I think the underlying\nreason could be that most students hope to avoid asking a question and then\nbeing told to look into it themselves and check their resources they already\nhave I hear instructors say this so often, even in my other research-based\nclass I’m taking this semester Personally, this is a main reason I refrain from\nasking questions, because given enough time and (sometimes hard) effort on my\nown, I can answer my own question I just have to look for the answers myself on\ngoogle or AI Most of the questions I want to ask are ones that I could answer\ngiven time and effort on my own I feel like instructors may think I am lazy if\nI don’t try to first figure it out Also, another reason is that students are\nconfused and want to ask questions, but don’t know what exactly to ask or how\nto phrase it I’ve heard a classmate say this in another course, and I feel the\nsame way sometimes It’s harder when there are technical terms involved because\nI don’t know how to describe what I’m confused about Overall, I personally\ndon’t find either of these things to be huge issues in this course, as you were\nalways willing to help us and were a very encouraging instructor I never felt\npressured in a negative way Given our resources, it made sense for me to try to\nat least try to problem solve myself, which often resulted in me figuring out\nan answer…thanks to my resources, like AI! Overall, I’m not sure what changes I\nwould suggest for this course, especially given our short semester The important\nthing was that I learned useful programming skills that I know I will apply in\nthe future Thank you Dr Blanchard!\n\nThank you :)\n\nAI removes the need to spend hours learning coding and allows me to focus on\nmore important things\n\nNA\n\nI genuinely think the point of AI is like computers and calculators, make\ncomplex work easier I just think we should not confuse a glorified calculator\nwith a friend, God's word, or an actual intelligent person (Cogito, ergo sum)\nEven if it was an intelligent person, I would check their words and answers if\nI suspected it sounded too good to be true (that's probably because by nature,\nI also have a tendency to say things with a lot of confidence even if they are\nnot true—simply because I believe them) If someone came to me with the total\nconfidence that there are three types of T cells, I would know better and tell\nthem \"that's not true\" If they told me \"a new type of IgG was discovered in\na paper by Matt et al,\" I'd have a tendency to say \"yes? Can you show me?\" If\nI was told \"Did you know there are over 300 types of turtles?\" I may have a\nsmall doubt, but usually say \"oh nice!\" and accept it as a doubtful, but actual\nfact (and mess up and repeat it with total confidence to other people as a\ntruth) Asking any question to AI comes with the understanding that it is only a\ncomputer and just does math But even if it could think and was human, wouldn't\nwe also double-check their statement and thoughts? Or correct them, or ask for\nproofs?\n\nNA\n\n\n\n\nTable - Overall effectiveness / Likelihood to recommend AI\n\n\nR code\ncols &lt;- c(\n  \"Overall effectiveness of AI in this course\",\n  \"Likelihood to recommend AI use in similar courses\"\n)\n\nres &lt;- df |&gt;\n  select(any_of(cols)) |&gt;\n  pivot_longer(everything(), names_to = \"column\", values_to = \"value\") |&gt;\n  mutate(value = as.numeric(value)) |&gt;                # convert to numeric\n  group_by(column) |&gt;\n  summarise(\n    mean = mean(value, na.rm = TRUE),\n    sd   = sd(value, na.rm = TRUE),\n    n    = sum(!is.na(value)),\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(mean = round(mean, 2), sd = round(sd, 2))\n\nkable(res)\n\n\n\n\n\ncolumn\nmean\nsd\nn\n\n\n\n\nLikelihood to recommend AI use in similar courses\n7.86\n1.98\n43\n\n\nOverall effectiveness of AI in this course\n8.60\n1.22\n43"
  },
  {
    "objectID": "labs/survey_results.html",
    "href": "labs/survey_results.html",
    "title": "Class Survey Results",
    "section": "",
    "text": "R code\nlibrary(tidyverse)"
  },
  {
    "objectID": "methods/NEON_metagenomes.html",
    "href": "methods/NEON_metagenomes.html",
    "title": "NEON Metagenome Samples Summary",
    "section": "",
    "text": "R code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(viridis)\nlibrary(janitor)\n\n\nSave file to have version used\n\n\nR code\nNEON_metaG_prelim &lt;- read_csv(\"../data/NEON_metadata/GOLD_Study_ID_Gs0166454_NEON_2023-4x.csv\") |&gt; \nclean_names() \n\n\n\n\nR code\nNEON_metaG_prelim &lt;- NEON_metaG_prelim |&gt; \n\n# Add a new column community corresponding to different communities names in the project_name\n\n  mutate(community = case_when(\n    str_detect(project_name, \"Freshwater sediment microbial communities\") ~ \"Freshwater sediment microbial communitie\",\n    str_detect(project_name, \"Freshwater biofilm microbial communities\") ~ \"Freshwater biofilm microbial communities\",\n    str_detect(project_name, \"Freshwater microbial communities\") ~ \"Freshwater microbial communities\",\n    str_detect(project_name, \"Soil microbial communities\") ~ \"Soil microbial communities\",\n    TRUE ~ NA_character_\n  )) |&gt; \n  \n  # Create a column type that is either Freshwater or Soil\n  mutate(type = case_when(\n    str_detect(project_name, \"Freshwater sediment microbial communities\") ~ \"Freshwater\",\n    str_detect(project_name, \"Freshwater biofilm microbial communities\") ~ \"Freshwater\",\n    str_detect(project_name, \"Freshwater microbial communities\") ~ \"Freshwater\",\n    str_detect(project_name, \"Soil microbial communities\") ~ \"Soil\",\n    TRUE ~ NA_character_\n  )) |&gt; \n  \n  # Get rid of the communities strings\n  mutate_at(\"project_name\", str_replace, \"Freshwater sediment microbial communities from \", \"\") |&gt; \n  mutate_at(\"project_name\", str_replace, \"Freshwater biofilm microbial communities from\", \"\") |&gt; \n  mutate_at(\"project_name\", str_replace, \"Freshwater microbial communities from \", \"\") |&gt; \n  mutate_at(\"project_name\", str_replace, \"Soil microbial communities from \", \"\") |&gt; \n\n  # separate site from sample name \n  separate(project_name, c(\"site\",\"sample_name\"), \" - \") |&gt;   \n  \n  # Deal with these unknown fields in the sample name by creating a new column and removing them from the sample name\n  mutate(sample_unknown = case_when(\n    str_detect(sample_name, \".SS.\") ~ \"SS\",\n    str_detect(sample_name, \".C0.\") ~ \"C0\",\n    str_detect(sample_name, \".C1.\") ~ \"C1\",\n    str_detect(sample_name, \".C2.\") ~ \"C2\",\n    TRUE ~ NA_character_\n  )) |&gt; \n\n# These fields are all associated with \"Freshwater microbial communities from...   \n# SS - near stream sensor\n# C0 - non-stratified lake/river surface near buoy\n# C1 - stratified lake surface/epilimnion near buoy\n# C2 - stratified lake hypolimnion near buoy\n# EPIPSAMMON  - biofilm on sand/silt\n# EPILITHON - biofilm on rocks/cobbles\n# EPIPHYTON - biofilm that grows on the stems and leaves of aquatic plants\n  \n# These fields are all associated with \"Freshwater biofilm microbial communities from\n# EPILITHON - biofilm on rocks/cobbles\n  \n# These fields are all associated with \"Freshwater sediment microbial communities from \n# EPIPSAMMON - biofilm on sand/silt\n  \n  mutate_at(\"sample_name\", str_replace, \".SS\", \"\") |&gt; \n  mutate_at(\"sample_name\", str_replace, \".C0\", \"\") |&gt; \n  mutate_at(\"sample_name\", str_replace, \".C1\", \"\") |&gt; \n  mutate_at(\"sample_name\", str_replace, \".C2\", \"\") |&gt; \n  \n\n  # Get rid of the the common strings at the end of sample names\n  mutate_at(\"sample_name\", str_replace, \"-GEN-DNA1\", \"\") |&gt;  \n  mutate_at(\"sample_name\", str_replace, \"-COMP-DNA1\", \"\") |&gt;  \n  mutate_at(\"sample_name\", str_replace, \"-COMP-DNA2\", \"\") |&gt;  \n  mutate_at(\"sample_name\", str_replace, \".DNA-DNA1\", \"\") |&gt;\n  mutate_at(\"sample_name\", str_replace, \"_v2\", \"\") |&gt;\n  mutate_at(\"sample_name\", str_replace, \" \\\\(version 2\\\\)\", \"\") |&gt;\n  mutate_at(\"sample_name\", str_replace, \" \\\\(version 3\\\\)\", \"\") |&gt; \n  \n# Some TOOK sites have an extra field. Remove it\n# TOOK.OT. \n\n  mutate_at(\"sample_name\", str_replace, \".OT\", \"\") |&gt; \n  mutate_at(\"sample_name\", str_replace, \".IF\", \"\")\n\n\n\nThis block is for separating out the freshwater sample names\n\n\nR code\nNEON_metaG_freshwater &lt;- NEON_metaG_prelim |&gt;\n  filter(type == \"Freshwater\") |&gt;\n  # separate the Sample Name into Site ID and plot info\n  separate(sample_name, c(\"site_ID\",\"date\", \"layer\", \"subplot\"), \"\\\\.\", remove = FALSE) |&gt;\n  mutate(quadrant = NA_character_)\n\n\n\n\nThis block is for separating out the soil sample names\n\n\nR code\nNEON_metaG_soil &lt;- NEON_metaG_prelim |&gt;\n  filter(type == \"Soil\") |&gt;\n  # separate the Sample Name into Site ID and plot info\n  separate(sample_name, c(\"site_ID\",\"subplot.layer.date\"), \"_\", remove = FALSE,) |&gt; \n  # some sample names have 3 fields while others have a fourth field for the quadrant. This code create a field for the quadrant when present and adds na for samples from combined cores.\n  extract(\n    subplot.layer.date,\n    into = c(\"subplot\", \"layer\", \"quadrant\", \"date\"),\n    regex = \"^([^-]+)-([^-]+)(?:-([^-]+))?-([^-]+)$\",\n    remove = FALSE\n  ) |&gt;\n  mutate(quadrant = na_if(quadrant, \"\")) |&gt;\n  select(-subplot.layer.date)\n\n\n\n\nThis combines the soil and freshwater data frames\n\n\nR code\nNEON_metaG &lt;- bind_rows(NEON_metaG_freshwater, NEON_metaG_soil) |&gt;\n  # This changes the format of the data column\n  mutate(date = ymd(date)) |&gt; \n  mutate(year = year(date))\n\n\n\n\nMake a table of the soiil samples each year by site\n\n\nR code\nNEON_metaG_soil_counts &lt;- NEON_metaG |&gt;\n  filter(type == \"Soil\") |&gt; \n  group_by(type, site_ID, year) |&gt; \n  summarize(count = n())\n\n\n\n\nR code\ndatatable(NEON_metaG_soil_counts)\n\n\n\n\n\n\n\n\nMake a table of the freshwaqter samples each year by site\n\n\nR code\nNEON_metaG_water_counts &lt;- NEON_metaG |&gt;\n  filter(type == \"Freshwater\") |&gt; \n  group_by(type, site_ID) |&gt; \n  summarize(count = n())\n\n\n\n\nR code\ndatatable(NEON_metaG_water_counts)"
  }
]